{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scyan documentation","text":"<p>Scyan stands for Single-cell Cytometry Annotation Network. Based on biological knowledge prior, it provides a fast cell population annotation without requiring any training label. Scyan is an interpretable model that also corrects batch-effect and can be used for debarcoding, cell sampling, and population discovery.</p>"},{"location":"#overview","title":"Overview","text":"<p>Scyan is a Bayesian probabilistic model composed of a deep invertible neural network called a normalizing flow (the function \\(f_{\\phi}\\)). It maps a latent distribution of cell expressions into the empirical distribution of cell expressions. This cell distribution is a mixture of gaussian-like distributions representing the sum of a cell-specific and a population-specific term. Also, interpretability and batch effect correction are based on the model latent space \u2014 more details in the article's Methods section.</p> <p> </p>"},{"location":"#technical-description","title":"Technical description","text":"<p>Scyan is a Python library based on:</p> <ul> <li>AnnData, a data library that works nicely with single-cell data</li> <li>Pytorch, a deep learning framework</li> <li>Pytorch Lightning, for model training</li> </ul> <p>Optionally, it also supports:</p> <ul> <li>Hydra, for project configuration</li> <li>Weight &amp; Biases, for model monitoring</li> </ul>"},{"location":"advice/","title":"Usage advices","text":""},{"location":"advice/#general-advices","title":"General advices","text":"<ul> <li>Make sure your data only contains the population of interest. E.g., if you are interested into the annotation of immune cells, then consider providing only the live cells that are CD45+. If needed, Scyan can be run to first extract the cells of interest before to annotate the different populations.</li> <li>Don't provide overlapping populations in the table. For instance, if you have CD16- NK and CD16+ NK, you don't need to write knowledge about NK cells. Note that, we would actually indirectly annotate NK cells, because they are the union of CD16- NK cells and CD16+ NK cells. You could also provide a hierarchical description of the populations.</li> <li>Before to run batch effect correction, check if you really have a significant batch effect (else, you could simply run Scyan without batch-effect correction). You can check that by plotting the batch observation on the umap (see scyan.plot.umap) and see if the different batches overlap or not.</li> </ul>"},{"location":"advice/#advice-for-the-creation-of-the-table","title":"Advice for the creation of the table","text":"<p>Some table examples can be found here.</p> <p>Important</p> <p>The design of the knowledge table is essential for the annotations. The literature can help its creation, but we also provide some advice to enhance the table.</p> <ul> <li>Check that all markers are working as intended. A marker that didn't work during the cytometer acquisition should be removed from the knowledge table, as it can mislead the model.</li> <li>It is better to provide no knowledge than false information; therefore, the user should feel comfortable using \"Not Applicable\" for a marker when unsure (depending on your table, 70% of NA can be acceptable). Besides, if needed, population discovery can be used to go deeper into this marker afterwards.</li> <li>When possible, provide at least one negative population and one positive population for each marker. For instance, if you provide only \"NA\" values and \"1\", then the model has no reference to understanding what is negative and what is positive (even though it should not cause any severe issue in most cases).</li> </ul> <p>Note</p> <p>However, you can create a column full of NA for a marker that you want to appear in the latent space. It will not be helpful for the annotations, but it can help the population discovery. You can also remove the marker from the knowledge table if you consider it unimportant for the annotation.</p> <ul> <li>Sometimes, you will have a marker that is expressed by most of the population \\(A\\) cells, but you know a few cells from the same population may not express this marker. In that case, we still advise adding the marker on the table, and Scyan will be able to catch these exceptions, as they look like population \\(A\\) cells for most of the other markers of the panel. Consider the image below: let's say this is the manual gating that you did traditionally. Even though some cells of \\(A\\) are <code>Marker1+</code>, you should still write that the population \\(A\\) is <code>Marker1-</code>, to make a clear distinction with the population \\(B\\). Also, if you are specifically interested in the cells from \\(A\\) that are <code>Marker1+</code>, you can still create another population \\(A_2\\) and write that \\(A_2\\) is <code>Marker1+</code> while \\(A\\) is <code>Marker1-</code>.</li> </ul> <p> </p> <ul> <li>Note that the model interprets NA values as \"any expression is possible\". Thus, a population described with extensive use of NA values (e.g., above 90% of markers, with no discriminative marker provided) can be over-predicted. This is a normal behaviour since few constraints are applied to this population.</li> <li>We enable the usage of intermediate expressions such as \"mid\" and \"low\" in the table. For that, choose a value between -1 and 1: for instance, 0 for mid, or -0.5 for low. Yet, we advise using it only to differentiate two similar populations. Overusing these intermediate expressions in the table will challenge you to create the table properly while not improving the results.</li> </ul>"},{"location":"advice/#what-should-i-do-if-scyan-seems-wrong","title":"What should I do if Scyan seems wrong?","text":"<ul> <li>First thing to do is to check your table again. You may have made a typo that could confuse the model. Typically, if you have written <code>Marker+</code> for a population that is <code>Marker-</code> (or the opposite), it can perturb the prediction toward this population and toward other populations.</li> <li>Try providing <code>prior_std=0.35</code> to the <code>scyan.Scyan</code> model. Maybe this parameter was too low. Advises to better choose the parameters can be found here.</li> <li>If one population annotation seems not consistent, or if a group of cells has not been predicted, you can try to target it with <code>scyan.tools.PolygonGatingUMAP</code>. Then, use <code>scyan.plot.pop_expressions(model, True, key=\"scyan_selected\")</code> to explore the expressions of this population. Another interesting graph is <code>scyan.plot.probs_per_marker(model, True, key=\"scyan_selected\")</code>: look for markers that show up dark on the heatmap, it may guide you to find some errors in the knowledge table. Combine it with a UMAP plot or with a scatter plot to make sure it seems correct, and then read some literature again / update your table.</li> <li>One reason for not predicting a population may be an unbalanced knowledge quantity between two related populations. For instance, having 10 values inside the table for <code>CD4 T CM</code> cells versus 5 values for <code>CD4 T EM</code> cells will probably make the model predict very few <code>CD4 T CM</code> cells. Indeed, <code>CD4 T CM</code> has many constraints compared to <code>CD4 T EM</code>, which becomes the \"easy prediction\" (indeed, very few constraints are applied to this population). In that case, read the advice related to the scatter plot above again.</li> </ul> <p>Example about how Scyan handles NA</p> <p>If a population \\(A\\) is labeled CD25+, then a cell that is CD25+ provides more confidence to the model towards this population than for a population \\(B\\) for which CD25 is labeled \"NA\". Yet, if a cell is CD25-, then it will provide more confidence to the population \\(B\\) than for the population \\(A\\). Indeed, CD25- is strong evidence that the cell is not \\(A\\), while it does not penalizes the population \\(B\\) too much.</p> <p>If you still can't make it work</p> <p>You can create an issue on Github or ask for help (quentin.blampey@gmail.com)</p>"},{"location":"cite/","title":"Cite us","text":"<p>Our paper is published in Briefings in Bioinformatics and is available here. <pre><code>@article{10.1093/bib/bbad260,\n    author = {Blampey, Quentin and Bercovici, Nad\u00e8ge and Dutertre, Charles-Antoine and Pic, Isabelle and Ribeiro, Joana Mourato and Andr\u00e9, Fabrice and Courn\u00e8de, Paul-Henry},\n    title = \"{A biology-driven deep generative model for cell-type annotation in cytometry}\",\n    journal = {Briefings in Bioinformatics},\n    pages = {bbad260},\n    year = {2023},\n    month = {07},\n    issn = {1477-4054},\n    doi = {10.1093/bib/bbad260},\n    url = {https://doi.org/10.1093/bib/bbad260},\n    eprint = {https://academic.oup.com/bib/advance-article-pdf/doi/10.1093/bib/bbad260/50973199/bbad260.pdf},\n}\n</code></pre></p> <p>This library has been developed by Quentin Blampey, PhD student in Biomathematics / Deep Learning. The following institutions funded this work:</p> <ul> <li>Lab of Mathematics and Computer Science (MICS), CentraleSup\u00e9lec (Engineering School, Paris-Saclay University).</li> <li>PRISM center, Gustave Roussy Institute (Cancer campus, Paris-Saclay University).</li> </ul>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#installation","title":"Installation","text":"<p>Scyan can be installed on every OS with <code>pip</code> or <code>poetry</code>.</p> <p>On macOS / Linux, <code>python&gt;=3.8,&lt;3.11</code> is required, while <code>python&gt;=3.8,&lt;3.10</code> is required on Windows. The preferred Python version is <code>3.9</code>.</p> <p>Advice (optional)</p> <p>We advise creating a new environment via a package manager (except if you use Poetry, which will automatically create the environment).</p> <p>For instance, you can create a new <code>conda</code> environment:</p> <pre><code>conda create --name scyan python=3.9\nconda activate scyan\n</code></pre> <p>Choose one of the following, depending on your needs (it should take at most a few minutes):</p> From PyPILocal install (pip)Local install (pip, dev mode)Poetry (dev mode) <pre><code>pip install scyan\n</code></pre> <pre><code>git clone https://github.com/MICS-Lab/scyan.git\ncd scyan\n\npip install .\n</code></pre> <pre><code>git clone https://github.com/MICS-Lab/scyan.git\ncd scyan\n\npip install -e '.[dev,hydra,discovery]'\n</code></pre> <pre><code>git clone https://github.com/MICS-Lab/scyan.git\ncd scyan\n\npoetry install -E 'dev hydra discovery'\n</code></pre>"},{"location":"getting_started/#usage","title":"Usage","text":""},{"location":"getting_started/#minimal-example","title":"Minimal example","text":"<pre><code>import scyan\n\nadata, table = scyan.data.load(\"aml\") # Automatic loading\n\nmodel = scyan.Scyan(adata, table)\nmodel.fit()\nmodel.predict()\n</code></pre> <p>This code should run in approximately 40 seconds (once the dataset is loaded).</p>"},{"location":"getting_started/#inputs-details","title":"Inputs details","text":"<ul> <li><code>adata</code> is an AnnData object, whose variables (<code>adata.var</code>) corresponds to markers, and observations (<code>adata.obs</code>) to cells. <code>adata.X</code> is a matrix of size (\\(N\\) cells, \\(M\\) markers) representing cell-marker expressions after being preprocessed (asinh or logicle) and standardized.</li> <li><code>table</code> is a pandas DataFrame with \\(P\\) rows (one per population) and \\(M\\) columns (one per marker). Each value represents the knowledge about the expected expression, i.e. <code>-1</code> for negative expression, <code>1</code> for positive expression, or <code>NA</code> if we don't know. It can also be any float value such as <code>0</code> or <code>-0.5</code> for mid and low expressions, respectively (use it only when necessary).</li> </ul> <p>Help to create the <code>adata</code> object and the <code>table</code></p> <p>Read the preprocessing tutorial if you have an FCS file and want explanations to initialize <code>Scyan</code>. You can also look at existing tables.</p> <p>Check</p> <p>Make sure every marker from the table (i.e. columns names of the DataFrame) is inside the data, i.e. in <code>adata.var_names</code>.</p>"},{"location":"getting_started/#resources-to-guide-you","title":"Resources to guide you","text":"<ul> <li>Read the tutorials (e.g. how to prepare your data or usage example with interpretability).</li> <li>Read our advice to design the knowledge table.</li> <li>Read the API to know more about what you can do (e.g. scyan.Scyan).</li> <li>Save and load your own dataset.</li> <li>How to choose the model parameters if you don't want to use the default ones.</li> </ul>"},{"location":"math/","title":"Math background","text":"<p>Here we briefly describe the maths behind Scyan. For more details, refer to the method section of our article.</p>"},{"location":"math/#notations","title":"Notations","text":"Symbol Belongs to Description \\(P\\) \\(\\mathbb{N}^*\\) Number of populations \\(N\\) \\(\\mathbb{N}^*\\) Number of cells \\(M\\) \\(\\mathbb{N}^*\\) Number of markers \\(M_{c}\\) \\(\\mathbb{N}^*\\) Number of covariates per cell \\(\\pmb{\\pi} = (\\pi_z)_{1 \\leq z \\leq P}\\) \\(]0, 1[^P\\) Population size ratios (i.e., \\(\\sum_z \\pi_z = 1\\)). This parameter is learned during training. \\(\\pmb{\\rho}\\) \\((\\mathbb{R} \\cup \\{NA\\})^{P \\times M}\\) Knowledge table. For a population \\(z\\) and a marker \\(m\\), the value \\(\\rho_{z,m}\\) describes the expected expression of \\(m\\) by population \\(z\\). Typical values are: \\(-1\\) for negative, \\(1\\) for positive, NA when not known. \\(\\pmb{x_i}\\) \\(\\mathbb{R}^M\\) Each \\(\\pmb{x_i}\\) is the vector of preprocessed marker expression of the cell \\(i \\in [1\\cdots N]\\) \\(\\pmb{c_i}\\) \\(\\mathbb{R}^{M_c}\\) Each \\(\\pmb{c_i}\\) is the vector of covariates associated to the cell \\(i \\in [1\\cdots N]\\)"},{"location":"math/#generative-process","title":"Generative process","text":"<p>We assume that the cell expressions come from the generative process below, where \\(f_{\\pmb{\\phi}}\\) is the normalizing flow detailed in the next section. We define the following random variables: \\(Z\\) the random population, \\(\\pmb{E}\\) the population-specific latent marker expressions, \\(\\pmb{H}\\) a cell-specific term, \\(\\pmb{U}\\) the latent expressions, and \\(\\pmb{X}\\) the actual (preprocessed) marker expressions.</p> \\[     Z \\sim Categorical(\\pmb{\\pi}) \\] \\[\\pmb{E} \\; | \\; Z = (e_m)_{1 \\leq m \\leq M} \\mbox{, where }         \\left\\{             \\begin{array}{ll}                 e_m = \\rho_{Z,m} &amp; \\mbox{if }\\rho_{Z,m} \\neq \\mbox{NA} \\\\                 e_m \\sim \\mathcal{U}([-1, 1]) &amp; \\mbox{otherwise,}             \\end{array}         \\right. \\] \\[     \\pmb{H} \\sim \\mathcal{N}(\\pmb{0}, \\sigma \\mathbb{\\pmb{I_M}}) \\] \\[     \\pmb{U} = \\pmb{E} + \\pmb{H} \\] \\[     \\pmb{X} = f_{\\pmb{\\phi}}^{-1}(\\pmb{U}) \\] <p>The latent distribution \\(\\pmb{U}\\) is defined on \\(\\mathbb{R}^M\\), i.e. the same space as the original marker expressions. Each dimension of the latent space corresponds to one marker. These latent marker expressions are meant to be free of batch effect or any non-biological factor.</p> <p>We can summarize this generative process with the figure below. \\(\\pmb{U}\\) corresponds to the mixture distribution in red (right), while \\(\\pmb{X}\\) corresponds to the distribution of cells marker expressions (left). The normalizing flow \\(f_{\\pmb{\\phi}}\\) is the mapping between these two distributions.</p> <p> </p> <p>Interpretation</p> <p>In this latent space, all marker expressions share the same scale (close to \\([-1, 1]\\), but not necessarily inside). For instance, a latent expression close to 1 is a positive expression, close to -1 is negative, while close to 0 would be a mid expression.</p>"},{"location":"math/#normalizing-flow","title":"Normalizing flow","text":"<p>The normalizing flow is a stack of multiple coupling layers: \\(f_{\\pmb{\\phi}} := f^{(L)} \\circ f^{(L-1)} \\circ \\dots \\circ f^{(1)}\\) with \\(L\\) the number of coupling layers. Each coupling layer \\(f^{(i)}: (\\pmb{x}, \\pmb{c}) \\mapsto \\pmb{y}\\) splits both \\(\\pmb{x}\\) and \\(\\pmb{y}\\) into two components \\((\\pmb{x^{(1)}}, \\pmb{x^{(2)}}), (\\pmb{y^{(1)}}, \\pmb{y^{(2)}})\\) on which distinct transformations are applied (the split is different for each layer). We propose below an extension of the traditional coupling layer to integrate covariates \\(\\pmb{c}\\):</p> \\[     \\begin{cases}       \\pmb{y^{(1)}} = \\pmb{x^{(1)}}\\\\       \\pmb{y^{(2)}} = \\pmb{x^{(2)}} \\odot exp\\Big(s([\\pmb{x^{(1)}}; \\pmb{c}])\\Big) + t([\\pmb{x^{(1)}}; \\pmb{c}]).     \\end{cases}   \\] <p>On the equation above, \\(s\\) and \\(t\\) are Multi-Layer-Perceptrons (MLPs). The coupling layer architecture is illustrated below:</p> <p> </p> <p>Overall, the normalizing flow has some interesting properties that help preserve the biological variability as much as possible:</p> <ul> <li>The coupling layers preserve the order relation for two different expression values.</li> <li>The loss penalizes huge space distortion (the log determinant term in the equation below).</li> </ul> <p>Note</p> <p>Each coupling layer is invertible, so is \\(f_{\\pmb{\\phi}}\\). Also, the log determinant of the jacobian of each layer is easy to compute, which is crucial to compute the loss function below.</p>"},{"location":"math/#loss","title":"Loss","text":"<p>We optimize the Kullback\u2013Leibler (KL) divergence defined below by stochastic gradient descent. More details leading to this expression and how to compute it can be found in the article methods section.</p> \\[     \\mathcal{L}_{KL}(\\pmb{\\theta}) = - \\sum_{1 \\leq i \\leq N} \\bigg[ log \\: \\Big( p_U(f_{\\pmb{\\phi}}(\\pmb{x_i}, \\pmb{c_i}); \\pmb{\\pi}) \\Big) + log \\;  \\Big| det \\frac{\\partial f_{\\pmb{\\phi}}(\\pmb{x_i}, \\pmb{c_i})}{\\partial \\pmb{x}^T} \\Big| \\bigg]. \\] <p>We optimize the loss on mini-batches of cells using the Adam optimizer.</p> <p>Note</p> <p>In the above equation, \\(p_U(f_{\\pmb{\\phi}}(\\pmb{x_i}, \\pmb{c_i}); \\pmb{\\pi}) = \\sum_{z=1}^P \\pi_z \\cdot p_{U \\mid Z = z}(f_{\\pmb{\\phi}}(\\pmb{x_i}, \\pmb{c_i}))\\), which is not computationally tractable because the presence of NA in \\(\\pmb{\\rho}\\) leads to the summation of a uniform and a normal random variable. We approximate the density of the sum of the two random variables by a piecewise density function.</p>"},{"location":"math/#annotating-cell-types","title":"Annotating cell-types","text":"<p>Once finished training, the annotation process \\(\\mathcal{A}_{\\pmb{\\theta}}\\) consists in choosing the most likely population according to the data using Bayes's rule. So, for a cell \\(\\pmb{x}\\) with covariates \\(\\pmb{c}\\), we have:</p> \\[     \\mathcal{A}_{\\pmb{\\theta}}(\\pmb{x}, \\pmb{c}) = argmax_{1 \\leq z \\leq P} \\; \\; \\pi_z \\cdot p_{U \\mid Z = z}(f_{\\pmb{\\phi}}(\\pmb{x}, \\pmb{c})). \\] <p>We also define a log threshold \\(t_{min}\\) to decide whether or not to label a cell, i.e., we don't label a cell if \\(max_{1 \\leq z \\leq P} \\; \\; p_{U \\mid Z = z}(f_{\\pmb{\\phi}}(\\pmb{x}, \\pmb{c})) \\leq e^{t_{min}}\\).</p>"},{"location":"math/#correcting-batch-effect","title":"Correcting batch effect","text":"<p>When the batches are provided into the covariates, the normalizing flow will naturally learn to align the latent representations of the multiple different batches. After model training, we can choose a batch as the reference and its corresponding covariates \\(\\pmb{c}_{ref}\\). Then, for a cell \\(\\pmb{x}\\) with covariates \\(\\pmb{c} \\neq \\pmb{c_{ref}}\\), its batch-effect corrected expressions are, \\(\\tilde{\\pmb{x}} = f_{\\pmb{\\phi}}^{-1}\\Big(f_{\\pmb{\\phi}}(\\pmb{x}, \\pmb{c}), \\pmb{c_{ref}}\\Big)\\).</p> <p>Note</p> <p>In this manner, we get expressions \\(\\tilde{\\pmb{x}}\\) as if \\(\\pmb{x}\\) were cell expressions from the reference batch. Applying \\(f_{\\pmb{\\phi}}\\) removes the batch effect, and applying \\(f_{\\pmb{\\phi}}^{-1}\\) recreates expressions that look like expressions of the reference batch.</p>"},{"location":"advanced/data/","title":"Add your own dataset","text":"<p>Existing datasets and versions can be listed via <code>scyan.data.list()</code>. By default, only public datasets are available, but you can add some if you follow the next steps.</p>"},{"location":"advanced/data/#1-prepare-your-python-objects","title":"1. Prepare your Python objects","text":"<p>You must prepare your <code>cytometry data</code> and create your <code>knowledge table</code> as described in the preprocessing tutorial. You can also read our advice to create the knowledge table (a great table leads to better predictions!).</p> <p>Info</p> <p>If needed, you have an example of an <code>adata</code> object and a knowledge <code>table</code> if you run:</p> <pre><code>adata, table = scyan.data.load(\"aml\")\n</code></pre>"},{"location":"advanced/data/#2-save-your-dataset","title":"2. Save your dataset","text":"<p>Now that you have created an <code>adata</code> object and a <code>table</code>, you can simply save them (for more details, see scyan.data.add):</p> <pre><code>scyan.data.add(\"&lt;your-project-name&gt;\", adata, table)\n</code></pre>"},{"location":"advanced/data/#3-load-your-dataset","title":"3. Load your dataset","text":"<p>Congrats, you can now load your dataset (for more details, see scyan.data.load):</p> <pre><code>adata, table = scyan.data.load(\"&lt;your-project-name&gt;\")\n</code></pre>"},{"location":"advanced/hydra_wandb/","title":"Configuration and monitoring","text":"<p>If needed, you can use Hydra to manage your configuration. It allows to run the scripts or run hyperparameter optimization easily. You can also monitor your jobs with Weight &amp; Biases</p> <p>For that, clone the repository and install of the project in development mode (see Getting Started) to install the hydra extras. Then, you have to follow the step listed below.</p>"},{"location":"advanced/hydra_wandb/#create-a-new-project-configuration","title":"Create a new project configuration","text":"<p>Create a new project at <code>config/project/&lt;your-project-name&gt;.yaml</code>, where <code>&lt;your-project-name&gt;</code> is the one you used to create your dataset. In this file, add <code>name: &lt;your-project-name&gt;</code>.</p> <p>Add optionally:</p> <ul> <li><code>version</code> or <code>table</code> if you don't want to use your dataset's default table or anndata files.</li> <li><code>batch_key</code> (and eventually <code>batch_ref</code>) if you want to correct the batch effect.</li> <li>You can add some <code>continuous_covariates</code> and <code>categorical_covariates</code> (as a list of items).</li> <li><code>wandb_project_name</code>, the name of your Weight and Biases project for model monitoring. It will log all the metrics over the epochs and save different figures online.</li> </ul>"},{"location":"advanced/hydra_wandb/#other-configuration","title":"Other configuration","text":"<p>Update <code>config/config.yaml</code> for some basic configuration. The most important config variables are:</p> <ul> <li><code>n_run</code>: the number of runs per trial (we advise choosing at least 5 to smooth the results at each trial).</li> <li><code>save_predictions</code> can be set to <code>True</code> to save scyan predictions at each run. They will be saved in the log directory created by Hydra.</li> <li><code>wandb.mode = online</code> if you want to use Weight and Biases.</li> </ul> <p>Tips</p> <p>Every config parameter can be updated directly via the command line (see Hydra docs). Thus, you don't have to change the parameters in the config files directly for every run.</p>"},{"location":"advanced/hydra_wandb/#optional-hyperparameter-optimization-configuration","title":"(Optional) Hyperparameter optimization configuration","text":"<p>Update <code>config/sweeper/optuna.yaml</code> to select the parameters you want to optimize and their ranges. In particular, choose the number of trials you desire or update it via the command line.</p> <p>Check</p> <p>Now that you have configured your project, you can run the scripts (see running scripts) by providing the argument <code>project=&lt;your-project-name&gt;</code>.</p>"},{"location":"advanced/parameters/","title":"Hyperparameters","text":"<p>Note</p> <p>If not done yet, you may be interested in reading our advice to improve your knowledge table. The default <code>Scyan</code> parameters should work well for most cases, so you should first check your knowledge table.</p>"},{"location":"advanced/parameters/#main-parameters","title":"Main parameters","text":"<p>We provide some help to choose scyan.Scyan initialization parameters. We listed below the most important ones.</p> <ul> <li><code>prior_std</code> is probably one of the most important parameters. Its default value should work for most of the usage, but it can be changed if needed. A low <code>prior_std</code> (e.g., <code>0.2</code>) will help better separate the populations, but it may be too stringent, and some small populations may disappear. In contrast, a high <code>prior_std</code> (e.g., <code>0.35</code>) increases the chances of having a large diversity of populations, but their separation may be less clear. We recommend to start with a medium value such as <code>0.25</code> or <code>0.3</code> and reducing it afterwards if it already captures all the populations.</li> <li>Reducing the <code>temperature</code> can help better capture small populations (e.g., <code>0.25</code>).</li> <li><code>batch_ref</code> is the reference batch we use to align distributions. By default, we use the batch where we have the most cells, but you can choose your own reference. For that, please choose a batch that is representative of the diversity of populations you want to annotate; it can help the batch effect correction.</li> <li>To improve batch effect correction, we recommend to let the model run longer. This can be done by changing the parameters of the <code>model.fit()</code> method: for instance, one can increase the <code>patience</code> (e.g., 6) and/or decrease the <code>min_delta</code> (e.g., 0.5).</li> <li><code>continuous_covariates</code> and <code>categorical_covariates</code> can be provided to the model if you have some. For instance, if you changed one antibody, you can add a categorical covariate telling which samples have been measured with which antibody. Any covariate may help the model annotations and batch effect correction.</li> </ul>"},{"location":"advanced/parameters/#hyperparameter-search","title":"Hyperparameter search","text":"<p>If you want to automate the choice of the hyperparameters, you can also run a hyperparameter optimization with Hydra. See how to configure your project and run Hydra.</p>"},{"location":"advanced/scripts/","title":"Running scripts","text":"<p>Caution</p> <p>You can run <code>Scyan</code> without using these scripts. Use it only if you want to use Hydra and/or Weight and Biases. Before continuing, ensure that you have configured your project.</p>"},{"location":"advanced/scripts/#usage-examples","title":"Usage examples","text":"<p>At the root of the repository, run the following command lines. Note that <code>&lt;project-name&gt;</code> can be <code>aml</code>, <code>bmmc</code> or your own project (the one you have configured).</p> <pre><code># One run with the default config\npython -m scripts.run project=&lt;project-name&gt; n_run=1\n\n# Running hyperparameter optimization (-m option)\npython -m scripts.run -m project=&lt;project-name&gt;\n\n# Using the debug trainer for a quick debugging\npython -m scripts.run project=&lt;project-name&gt; trainer=debug\n\n# Use the GPU trainer and enable wandb and save umap after training\npython -m scripts.run project=&lt;project-name&gt; trainer=gpu wandb.mode=online wandb.save_umap=true\n\n# Change the model parameters\npython -m scripts.run project=&lt;project-name&gt; model.temperature=0.5 model.prior_std=0.3\n</code></pre>"},{"location":"advanced/scripts/#reproduce-the-article-results","title":"Reproduce the article results","text":"<p>The hyperparameters were obtained by (unsupervised) hyperparameter optimization (see Hydra configuration).</p> <pre><code># Testing on BMMC\npython -m scripts.run project=bmmc model.hidden_size=32 model.n_hidden_layers=8 model.n_layers=8 model.prior_std=0.35 model.temperature=2.5 model.modulo_temp=2 callbacks.early_stopping.patience=2 callbacks.early_stopping.min_delta=2\n\n# Testing on AML\npython -m scripts.run project=aml model.hidden_size=32 model.n_hidden_layers=7 model.n_layers=7 model.prior_std=0.2 model.temperature=0.75 model.modulo_temp=3 model.lr=0.0003 model.batch_size=8192 callbacks.early_stopping.patience=3 callbacks.early_stopping.min_delta=0.5\n</code></pre>"},{"location":"api/coupling_layer/","title":"scyan.module.CouplingLayer","text":"<p>             Bases: <code>LightningModule</code></p> <p>One single coupling layer module.</p> <p>Attributes:</p> Name Type Description <code>sfun</code> <code>Module</code> <p><code>s</code> Multi-Layer-Perceptron.</p> <code>tfun</code> <code>Module</code> <p><code>t</code> Multi-Layer-Perceptron.</p> Source code in <code>scyan/module/coupling_layer.py</code> <pre><code>class CouplingLayer(pl.LightningModule):\n    \"\"\"One single coupling layer module.\n\n    Attributes:\n        sfun (nn.Module): `s` Multi-Layer-Perceptron.\n        tfun (nn.Module): `t` Multi-Layer-Perceptron.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        output_size: int,\n        n_hidden_layers: int,\n        mask: Tensor,\n    ):\n        \"\"\"\n        Args:\n            input_size: Input size, i.e. number of markers + covariates\n            hidden_size: MLP (`s` and `t`) hidden size.\n            output_size: Output size, i.e. number of markers.\n            n_hidden_layers: Number of hidden layers for the MLP (`s` and `t`).\n            mask: Mask used to separate $x$ into $(x^{(1)}, x^{(2)})$\n        \"\"\"\n        super().__init__()\n        self.sfun = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(inplace=True),\n            *self._hidden_layers(hidden_size, n_hidden_layers),\n            nn.Linear(hidden_size, output_size),\n            nn.Tanh(),\n        )\n        self.tfun = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(inplace=True),\n            *self._hidden_layers(hidden_size, n_hidden_layers),\n            nn.Linear(hidden_size, output_size),\n        )\n        self.register_buffer(\"mask\", mask)\n\n    def _hidden_layers(self, hidden_size: int, n_hidden_layers: int) -&gt; List[nn.Module]:\n        return [\n            module\n            for _ in range(n_hidden_layers)\n            for module in [nn.Linear(hidden_size, hidden_size), nn.ReLU(inplace=True)]\n        ]\n\n    def forward(\n        self, inputs: Tuple[Tensor, Tensor, Optional[Tensor]]\n    ) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        \"\"\"Coupling layer forward function.\n\n        Args:\n            inputs: cell-marker expressions, covariates, lod_det_jacobian sum\n\n        Returns:\n            outputs, covariates, lod_det_jacobian sum\n        \"\"\"\n        x, covariates, ldj_sum = inputs\n\n        x_m = x * self.mask\n        st_input = torch.cat([x_m, 100 * covariates], dim=1)\n\n        s_out = self.sfun(st_input)\n        t_out = self.tfun(st_input)\n\n        y = x_m + (1 - self.mask) * (x * torch.exp(s_out) + t_out)\n        ldj = (s_out * (1 - self.mask)).sum(dim=1)\n        ldj_sum = ldj_sum + ldj if ldj_sum is not None else ldj\n\n        return y, covariates, ldj_sum\n\n    def inverse(self, y: Tensor, covariates: Tensor) -&gt; Tensor:\n        \"\"\"Go through the coupling layer in reverse direction.\n\n        Args:\n            y: Inputs tensor or size $(B, M)$.\n            covariates: Covariates tensor of size $(B, M_c)$.\n\n        Returns:\n            Outputs tensor.\n        \"\"\"\n        y_m = y * self.mask\n        st_input = torch.cat([y_m, 100 * covariates], dim=1)\n\n        return y_m + (1 - self.mask) * (\n            y * (1 - self.mask) - self.tfun(st_input)\n        ) * torch.exp(-self.sfun(st_input))\n</code></pre>"},{"location":"api/coupling_layer/#scyan.module.CouplingLayer.__init__","title":"<code>__init__(input_size, hidden_size, output_size, n_hidden_layers, mask)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>Input size, i.e. number of markers + covariates</p> required <code>hidden_size</code> <code>int</code> <p>MLP (<code>s</code> and <code>t</code>) hidden size.</p> required <code>output_size</code> <code>int</code> <p>Output size, i.e. number of markers.</p> required <code>n_hidden_layers</code> <code>int</code> <p>Number of hidden layers for the MLP (<code>s</code> and <code>t</code>).</p> required <code>mask</code> <code>Tensor</code> <p>Mask used to separate \\(x\\) into \\((x^{(1)}, x^{(2)})\\)</p> required Source code in <code>scyan/module/coupling_layer.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    hidden_size: int,\n    output_size: int,\n    n_hidden_layers: int,\n    mask: Tensor,\n):\n    \"\"\"\n    Args:\n        input_size: Input size, i.e. number of markers + covariates\n        hidden_size: MLP (`s` and `t`) hidden size.\n        output_size: Output size, i.e. number of markers.\n        n_hidden_layers: Number of hidden layers for the MLP (`s` and `t`).\n        mask: Mask used to separate $x$ into $(x^{(1)}, x^{(2)})$\n    \"\"\"\n    super().__init__()\n    self.sfun = nn.Sequential(\n        nn.Linear(input_size, hidden_size),\n        nn.ReLU(inplace=True),\n        *self._hidden_layers(hidden_size, n_hidden_layers),\n        nn.Linear(hidden_size, output_size),\n        nn.Tanh(),\n    )\n    self.tfun = nn.Sequential(\n        nn.Linear(input_size, hidden_size),\n        nn.ReLU(inplace=True),\n        *self._hidden_layers(hidden_size, n_hidden_layers),\n        nn.Linear(hidden_size, output_size),\n    )\n    self.register_buffer(\"mask\", mask)\n</code></pre>"},{"location":"api/coupling_layer/#scyan.module.CouplingLayer.forward","title":"<code>forward(inputs)</code>","text":"<p>Coupling layer forward function.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tuple[Tensor, Tensor, Optional[Tensor]]</code> <p>cell-marker expressions, covariates, lod_det_jacobian sum</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>outputs, covariates, lod_det_jacobian sum</p> Source code in <code>scyan/module/coupling_layer.py</code> <pre><code>def forward(\n    self, inputs: Tuple[Tensor, Tensor, Optional[Tensor]]\n) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Coupling layer forward function.\n\n    Args:\n        inputs: cell-marker expressions, covariates, lod_det_jacobian sum\n\n    Returns:\n        outputs, covariates, lod_det_jacobian sum\n    \"\"\"\n    x, covariates, ldj_sum = inputs\n\n    x_m = x * self.mask\n    st_input = torch.cat([x_m, 100 * covariates], dim=1)\n\n    s_out = self.sfun(st_input)\n    t_out = self.tfun(st_input)\n\n    y = x_m + (1 - self.mask) * (x * torch.exp(s_out) + t_out)\n    ldj = (s_out * (1 - self.mask)).sum(dim=1)\n    ldj_sum = ldj_sum + ldj if ldj_sum is not None else ldj\n\n    return y, covariates, ldj_sum\n</code></pre>"},{"location":"api/coupling_layer/#scyan.module.CouplingLayer.inverse","title":"<code>inverse(y, covariates)</code>","text":"<p>Go through the coupling layer in reverse direction.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Tensor</code> <p>Inputs tensor or size \\((B, M)\\).</p> required <code>covariates</code> <code>Tensor</code> <p>Covariates tensor of size \\((B, M_c)\\).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Outputs tensor.</p> Source code in <code>scyan/module/coupling_layer.py</code> <pre><code>def inverse(self, y: Tensor, covariates: Tensor) -&gt; Tensor:\n    \"\"\"Go through the coupling layer in reverse direction.\n\n    Args:\n        y: Inputs tensor or size $(B, M)$.\n        covariates: Covariates tensor of size $(B, M_c)$.\n\n    Returns:\n        Outputs tensor.\n    \"\"\"\n    y_m = y * self.mask\n    st_input = torch.cat([y_m, 100 * covariates], dim=1)\n\n    return y_m + (1 - self.mask) * (\n        y * (1 - self.mask) - self.tfun(st_input)\n    ) * torch.exp(-self.sfun(st_input))\n</code></pre>"},{"location":"api/datasets/","title":"Loading/adding datasets","text":""},{"location":"api/datasets/#scyan.data.list","title":"<code>scyan.data.list(dataset_name=None)</code>","text":"<p>Show existing datasets and their different versions/table names.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>Optional[str]</code> <p>Optional dataset name. If provided, only display the version names of the provided <code>dataset_name</code>, otherwise list all existing datasets.</p> <code>None</code> Source code in <code>scyan/data/datasets.py</code> <pre><code>def _list(dataset_name: Optional[str] = None) -&gt; None:\n    \"\"\"Show existing datasets and their different versions/table names.\n\n    Args:\n        dataset_name: Optional dataset name. If provided, only display the version names of the provided `dataset_name`, otherwise list all existing datasets.\n    \"\"\"\n    data_path = get_data_path()\n\n    if dataset_name is not None:\n        dataset_path = data_path / dataset_name\n        log.info(f\"Listing versions inside {dataset_path}:\")\n        list_path(dataset_path)\n        return\n\n    log.info(f\"List of existing datasets inside {data_path}:\")\n    dataset_paths = set(data_path.iterdir())\n\n    for public_dataset in [\"poised\", \"aml\", \"bmmc\", \"debarcoding\"]:\n        dataset_path = Path(data_path / public_dataset)\n        dataset_path.mkdir(parents=True, exist_ok=True)\n        dataset_paths.add(dataset_path)\n\n    for dataset_path in dataset_paths:\n        if dataset_path.is_dir():\n            list_path(dataset_path)\n</code></pre>"},{"location":"api/datasets/#scyan.data.load","title":"<code>scyan.data.load(dataset_name, version='default', table='default', reducer=None)</code>","text":"<p>Load a dataset, i.e. its <code>AnnData</code> object and its knowledge table. Public datasets available are <code>\"poised\"</code>, <code>\"aml\"</code>, <code>\"bmmc\"</code>, and <code>\"debarcoding\"</code>; note that, if the dataset was not loaded yet, it is automatically downloaded (requires internet connection). Existing dataset names and versions/tables can be listed using scyan.data.list.</p> <p>Note</p> <p>If you want to load your own dataset, you first have to create it.</p> <p>Note</p> <p>The data is saved by default inside <code>&lt;home_path&gt;/.scyan_data</code>. Optionally, if <code>scyan</code> repository was cloned, you can create <code>&lt;scyan_repository_path&gt;/data</code> and use it instead of the default data folder.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset. Either one of your dataset, or one public dataset among <code>\"poised\"</code>, <code>\"aml\"</code>, <code>\"bmmc\"</code>, and <code>\"debarcoding\"</code>.</p> required <code>version</code> <code>Optional[str]</code> <p>Name of the <code>anndata</code> file (.h5ad) that should be loaded. The available versions can be listed with <code>scyan.data.list()</code>. If <code>None</code>, don't return an <code>adata</code> object.</p> <code>'default'</code> <code>table</code> <code>Optional[str]</code> <p>Name of the knowledge table that should be loaded. If <code>None</code>, don't return the <code>table</code> dataframe.</p> <code>'default'</code> <code>reducer</code> <code>Optional[str]</code> <p>Name of the umap reducer that should be loaded. If <code>None</code>, don't return the <code>UMAP</code> reducer.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[AnnData, DataFrame]</code> <p>Tuple containing the requested data, i.e. by default a tuple <code>(adata, table)</code> is returned (the adata instance and the knowledge table). But, for instance, if <code>version is None</code> and <code>reducer</code> is provided, then it returns a tuple <code>(table, reducer)</code>.</p> Source code in <code>scyan/data/datasets.py</code> <pre><code>def load(\n    dataset_name: str,\n    version: Optional[str] = \"default\",\n    table: Optional[str] = \"default\",\n    reducer: Optional[str] = None,\n) -&gt; Tuple[AnnData, pd.DataFrame]:\n    \"\"\"Load a dataset, i.e. its `AnnData` object and its knowledge table. Public datasets available are `\"poised\"`, `\"aml\"`, `\"bmmc\"`, and `\"debarcoding\"`; note that, if the dataset was not loaded yet, it is automatically downloaded (requires internet connection). Existing dataset names and versions/tables can be listed using [scyan.data.list][].\n\n    !!! note\n        If you want to load your own dataset, you first have to [create it](../../advanced/data).\n    !!! note\n        The data is saved by default inside `&lt;home_path&gt;/.scyan_data`. Optionally, if `scyan` repository was cloned, you can create `&lt;scyan_repository_path&gt;/data` and use it instead of the default data folder.\n\n    Args:\n        dataset_name: Name of the dataset. Either one of your dataset, or one public dataset among `\"poised\"`, `\"aml\"`, `\"bmmc\"`, and `\"debarcoding\"`.\n        version: Name of the `anndata` file (.h5ad) that should be loaded. The available versions can be listed with `scyan.data.list()`. If `None`, don't return an `adata` object.\n        table: Name of the knowledge table that should be loaded. If `None`, don't return the `table` dataframe.\n        reducer: Name of the umap reducer that should be loaded. If `None`, don't return the `UMAP` reducer.\n\n    Returns:\n        Tuple containing the requested data, i.e. by default a tuple `(adata, table)` is returned (the adata instance and the knowledge table). But, for instance, if `version is None` and `reducer` is provided, then it returns a tuple `(table, reducer)`.\n    \"\"\"\n    assert any(\n        arg is not None for arg in [version, table, reducer]\n    ), \"Provide at least one argument that is not `None` among 'version', 'table', and 'reducer'.\"\n\n    data_path = get_data_path()\n\n    dataset_path = data_path / dataset_name\n    dataset_path.mkdir(parents=True, exist_ok=True)\n\n    return tuple(\n        get_local_file(dataset_path, dataset_name, arg, ext)\n        for arg, ext in [(version, \"h5ad\"), (table, \"csv\"), (reducer, \"umap\")]\n        if arg is not None\n    )\n</code></pre>"},{"location":"api/datasets/#scyan.data.add","title":"<code>scyan.data.add(dataset_name, *objects, filename='default', overwrite=False)</code>","text":"<p>Add an object to a dataset (or create it if not existing). Objects can be <code>AnnData</code> objects, a knowledge-table (i.e. a <code>pd.DataFrame</code>), or a <code>UMAP</code> reducer. The provided filenames are the one you use when loading data with scyan.data.load.</p> <p>Note</p> <p>You will be able to load this dataset with scyan.data.load as long as you added at least a knowledge-table and a <code>adata</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset in which the object will be saved.</p> required <code>*objects</code> <code>List[Union[AnnData, DataFrame, UMAP]]</code> <p>Object(s) to save.</p> <code>()</code> <code>filename</code> <code>Union[str, List[str]]</code> <p>Name(s) without extension of the file(s) to create. The default value (<code>\"default\"</code>) is the filename loaded by default with <code>scyan.data.load</code>. If a list is provided, it should have the length of the number of objects provided and should have the same order. If a string, then use the same name for all objects (but different extensions).</p> <code>'default'</code> <code>overwrite</code> <code>bool</code> <p>If <code>True</code>, it will overwrite files that already exist.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object type is not one of the three required.</p> Source code in <code>scyan/data/datasets.py</code> <pre><code>def add(\n    dataset_name: str,\n    *objects: List[Union[AnnData, pd.DataFrame, umap.UMAP]],\n    filename: Union[str, List[str]] = \"default\",\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Add an object to a dataset (or create it if not existing). Objects can be `AnnData` objects, a knowledge-table (i.e. a `pd.DataFrame`), or a `UMAP` reducer. The provided filenames are the one you use when loading data with [scyan.data.load][].\n\n    !!! note\n        You will be able to load this dataset with [scyan.data.load][] as long as you added at least a knowledge-table and a `adata` object.\n\n    Args:\n        dataset_name: Name of the dataset in which the object will be saved.\n        *objects: Object(s) to save.\n        filename: Name(s) without extension of the file(s) to create. The default value (`\"default\"`) is the filename loaded by default with `scyan.data.load`. If a list is provided, it should have the length of the number of objects provided and should have the same order. If a string, then use the same name for all objects (but different extensions).\n        overwrite: If `True`, it will overwrite files that already exist.\n\n    Raises:\n        ValueError: If the object type is not one of the three required.\n    \"\"\"\n    data_path = get_data_path()\n\n    dataset_path = data_path / dataset_name\n    if not dataset_path.is_dir():\n        log.info(f\"Creating new dataset folder at {dataset_path}\")\n        dataset_path.mkdir(parents=True)\n\n    filenames = [filename] * len(objects) if isinstance(filename, str) else filename\n\n    for obj, filename in zip(objects, filenames):\n        if isinstance(obj, AnnData):\n            path = dataset_path / f\"{filename}.h5ad\"\n            _check_can_write(path, overwrite)\n            obj.write_h5ad(path)\n        elif isinstance(obj, pd.DataFrame):\n            path = dataset_path / f\"{filename}.csv\"\n            _check_can_write(path, overwrite)\n            obj.to_csv(path)\n        elif isinstance(obj, umap.UMAP):\n            path = dataset_path / f\"{filename}.umap\"\n            _check_can_write(path, overwrite)\n            import joblib\n\n            joblib.dump(obj, path)\n        else:\n            raise ValueError(\n                f\"Can't save object of type {type(obj)}. It must be an AnnData object, a DataFrame or a UMAP.\"\n            )\n\n        log.info(f\"Created file {path}\")\n</code></pre>"},{"location":"api/datasets/#scyan.data.remove","title":"<code>scyan.data.remove(dataset_name, version=None, table=None, reducer=None)</code>","text":"<p>Remove file(s) from a dataset folder.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset. Use <code>scyan.data.list()</code> to see the possible values.</p> required <code>version</code> <code>Optional[str]</code> <p>Name of the <code>.h5ad</code> file to remove (don't provide the extension).</p> <code>None</code> <code>table</code> <code>Optional[str]</code> <p>Name of the <code>.csv</code> file to remove (don't provide the extension).</p> <code>None</code> <code>reducer</code> <code>Optional[str]</code> <p>Name of the <code>.umap</code> file to remove (don't provide the extension).</p> <code>None</code> Source code in <code>scyan/data/datasets.py</code> <pre><code>def remove(\n    dataset_name: str,\n    version: Optional[str] = None,\n    table: Optional[str] = None,\n    reducer: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Remove file(s) from a dataset folder.\n\n    Args:\n        dataset_name: Name of the dataset. Use `scyan.data.list()` to see the possible values.\n        version: Name of the `.h5ad` file to remove (don't provide the extension).\n        table: Name of the `.csv` file to remove (don't provide the extension).\n        reducer: Name of the `.umap` file to remove (don't provide the extension).\n    \"\"\"\n    data_path = get_data_path()\n    dataset_path = data_path / dataset_name\n\n    for arg, ext in [(version, \"h5ad\"), (table, \"csv\"), (reducer, \"umap\")]:\n        if arg is not None:\n            filepath = dataset_path / f\"{arg}.{ext}\"\n            filepath.unlink()\n            log.info(f\"Successfully removed {filepath}\")\n</code></pre>"},{"location":"api/io/","title":"IO (read/write)","text":""},{"location":"api/io/#scyan.read_fcs","title":"<code>scyan.read_fcs(path, marker_regex='^cd|^hla|epcam|^ccr', exclude_markers=None, channel_suffix='S')</code>","text":"<p>Read a FCS file and return an <code>AnnData</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the FCS file that has to be read.</p> required <code>marker_regex</code> <code>str</code> <p>Regex used to find which columns correspond to markers. By default, it targets strings that starts with <code>CD</code>, <code>HLA</code>, <code>CCR</code>, or <code>EPCAM</code>. You can add names to the regex by adding the lowercase marker name after a new <code>|</code> in the string</p> <code>'^cd|^hla|epcam|^ccr'</code> <code>exclude_markers</code> <code>Optional[List[str]]</code> <p>Optional list of channel names that has to be considered as an observation (i.e., inside <code>adata.obs</code>), among the ones that were automatically classified as markers (i.e., inside <code>adata.var_names</code>).</p> <code>None</code> <code>channel_suffix</code> <code>Optional[str]</code> <p>Suffix for the channel naming convention, i.e. <code>\"S\"</code> for \"PnS\", or <code>\"N\"</code> for \"PnN\". If <code>None</code>, keep the raw names.</p> <code>'S'</code> <p>Returns:</p> Type Description <code>AnnData</code> <p><code>AnnData</code> object containing the FCS data.</p> Source code in <code>scyan/_io.py</code> <pre><code>def read_fcs(\n    path: str,\n    marker_regex: str = \"^cd|^hla|epcam|^ccr\",\n    exclude_markers: Optional[List[str]] = None,\n    channel_suffix: Optional[str] = \"S\",\n) -&gt; AnnData:\n    \"\"\"Read a FCS file and return an `AnnData` object.\n\n    Args:\n        path: Path to the FCS file that has to be read.\n        marker_regex: Regex used to find which columns correspond to markers. By default, it targets strings that starts with `CD`, `HLA`, `CCR`, or `EPCAM`. You can add names to the regex by adding the lowercase marker name after a new `|` in the string\n        exclude_markers: Optional list of channel names that has to be considered as an observation (i.e., inside `adata.obs`), among the ones that were automatically classified as markers (i.e., inside `adata.var_names`).\n        channel_suffix: Suffix for the channel naming convention, i.e. `\"S\"` for \"PnS\", or `\"N\"` for \"PnN\". If `None`, keep the raw names.\n\n    Returns:\n        `AnnData` object containing the FCS data.\n    \"\"\"\n    meta, data = fcsparser.parse(path)\n\n    names = pd.Series(\n        [meta.get(f\"$P{i + 1}{channel_suffix}\") for i in range(data.shape[1])]\n    )\n    fallback_names = pd.Series([meta[f\"$P{i + 1}N\"] for i in range(data.shape[1])])\n    data.columns = np.where(names.isna() | names.duplicated(False), fallback_names, names)\n\n    exclude_markers = _check_exlude_markers(data, exclude_markers)\n    is_marker = data.columns.str.lower().str.contains(marker_regex) &amp; ~np.isin(\n        data.columns, exclude_markers\n    )\n\n    adata = AnnData(\n        X=data.loc[:, is_marker].values.astype(np.float32),\n        var=pd.DataFrame(index=data.columns[is_marker]),\n        obs=data.loc[:, ~is_marker],\n    )\n\n    if \"$SPILLOVER\" in meta:\n        df_spillover = _read_spillover_matrix(meta[\"$SPILLOVER\"])\n        fallback_var_names = fallback_names[is_marker].values\n        is_in = np.isin(fallback_var_names, df_spillover.index)\n        if is_in.all():\n            adata.varp[\"spillover_matrix\"] = df_spillover.loc[\n                fallback_var_names, fallback_var_names\n            ]\n        else:\n            log.warn(\n                f\"Missing var names inside spillover matrix: {fallback_var_names[~is_in]}. The spillover matrix will be saved in adata.uns instead of adata.varp\"\n            )\n            adata.uns[\"spillover_matrix\"] = df_spillover\n\n    return adata\n</code></pre>"},{"location":"api/io/#scyan.read_csv","title":"<code>scyan.read_csv(path, marker_regex='^cd|^hla|epcam|^ccr', exclude_markers=None, **pandas_kwargs)</code>","text":"<p>Read a CSV file and return an <code>AnnData</code> object.</p> <p>Note</p> <p>It tries to infer which columns are markers by checking which columns contain one of these: CD, HLA, CCR, EPCAM, CADM, SIGLEC. Though, if it didn't select the right markers, you can help it by providing <code>extra_marker_names</code> or <code>remove_marker_names</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the CSV file that has to be read.</p> required <code>marker_regex</code> <code>str</code> <p>Regex used to find which columns correspond to markers. By default, it targets strings that starts with <code>CD</code>, <code>HLA</code>, <code>CCR</code>, or <code>EPCAM</code>. You can add names to the regex by adding the lowercase marker name after a new <code>|</code> in the string</p> <code>'^cd|^hla|epcam|^ccr'</code> <code>exclude_markers</code> <code>Optional[List[str]]</code> <p>Optional list of channel names that has to be considered as an observation (i.e., inside <code>adata.obs</code>), among the ones that were automatically classified as markers (i.e., inside <code>adata.var_names</code>).</p> <code>None</code> <code>**pandas_kwargs</code> <code>int</code> <p>Optional kwargs for <code>pandas.read_csv(...)</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AnnData</code> <p><code>AnnData</code> object containing the CSV data.</p> Source code in <code>scyan/_io.py</code> <pre><code>def read_csv(\n    path: str,\n    marker_regex: str = \"^cd|^hla|epcam|^ccr\",\n    exclude_markers: Optional[List[str]] = None,\n    **pandas_kwargs: int,\n) -&gt; AnnData:\n    \"\"\"Read a CSV file and return an `AnnData` object.\n\n    !!! note\n        It tries to infer which columns are markers by checking which columns contain one of these: CD, HLA, CCR, EPCAM, CADM, SIGLEC. Though, if it didn't select the right markers, you can help it by providing `extra_marker_names` or `remove_marker_names`.\n\n    Args:\n        path: Path to the CSV file that has to be read.\n        marker_regex: Regex used to find which columns correspond to markers. By default, it targets strings that starts with `CD`, `HLA`, `CCR`, or `EPCAM`. You can add names to the regex by adding the lowercase marker name after a new `|` in the string\n        exclude_markers: Optional list of channel names that has to be considered as an observation (i.e., inside `adata.obs`), among the ones that were automatically classified as markers (i.e., inside `adata.var_names`).\n        **pandas_kwargs: Optional kwargs for `pandas.read_csv(...)`.\n\n    Returns:\n        `AnnData` object containing the CSV data.\n    \"\"\"\n    df = pd.read_csv(path, **pandas_kwargs)\n\n    exclude_markers = _check_exlude_markers(df, exclude_markers)\n    is_marker = df.columns.str.lower().str.contains(marker_regex) &amp; ~np.isin(\n        df.columns, exclude_markers\n    )\n    return AnnData(df.loc[:, is_marker], obs=df.loc[:, ~is_marker], dtype=np.float32)\n</code></pre>"},{"location":"api/io/#scyan.write_fcs","title":"<code>scyan.write_fcs(adata, path, layer=None, columns_to_numeric=None, **fcswrite_kwargs)</code>","text":"<p>Based on a <code>AnnData</code> object, it writes a FCS file that contains (i) all the markers intensities, (ii) every numeric column of <code>adata.obs</code>, and (iii) all <code>adata.obsm</code> variables.</p> <p>Note</p> <p>As the FCS format doesn't support strings, some observations will not be kept in the FCS file.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p><code>AnnData</code> object to save.</p> required <code>path</code> <code>str</code> <p>Path to write the file.</p> required <code>layer</code> <code>Optional[str]</code> <p>Name of the <code>adata</code> layer from which intensities will be extracted. If <code>None</code>, uses <code>adata.X</code>.</p> <code>None</code> <code>columns_to_numeric</code> <code>Optional[List]</code> <p>List of non-numerical column names from <code>adata.obs</code> that should be kept, by transforming them into integers. Note that you don't need to list the numerical columns, that are written inside the FCS by default.</p> <code>None</code> <code>**fcswrite_kwargs</code> <code>int</code> <p>Optional kwargs provided to <code>fcswrite.write_fcs</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[None, Dict]</code> <p>If <code>columns_to_numeric</code> is <code>None</code>, returns nothing. Else, return a dict whose keys are the observation column names being transformed, and the values are ordered lists of the label encoded classes. E.g., <code>{\"batch\": [\"b1\", \"b2\"]}</code> means that the batch <code>\"b1\"</code> was encoded by 0, and <code>\"b2\"</code> by 1.</p> Source code in <code>scyan/_io.py</code> <pre><code>def write_fcs(\n    adata: AnnData,\n    path: str,\n    layer: Optional[str] = None,\n    columns_to_numeric: Optional[List] = None,\n    **fcswrite_kwargs: int,\n) -&gt; Union[None, Dict]:\n    \"\"\"Based on a `AnnData` object, it writes a FCS file that contains (i) all the markers intensities, (ii) every numeric column of `adata.obs`, and (iii) all `adata.obsm` variables.\n\n    !!! note\n        As the FCS format doesn't support strings, some observations will not be kept in the FCS file.\n\n    Args:\n        adata: `AnnData` object to save.\n        path: Path to write the file.\n        layer: Name of the `adata` layer from which intensities will be extracted. If `None`, uses `adata.X`.\n        columns_to_numeric: List of **non-numerical** column names from `adata.obs` that should be kept, by transforming them into integers. Note that you don't need to list the numerical columns, that are written inside the FCS by default.\n        **fcswrite_kwargs: Optional kwargs provided to `fcswrite.write_fcs`.\n\n    Returns:\n        If `columns_to_numeric` is `None`, returns nothing. Else, return a dict whose keys are the observation column names being transformed, and the values are ordered lists of the label encoded classes. E.g., `{\"batch\": [\"b1\", \"b2\"]}` means that the batch `\"b1\"` was encoded by 0, and `\"b2\"` by 1.\n    \"\"\"\n    df = _to_df(adata, layer)\n    dict_classes = {}\n    columns_removed = []\n\n    for column in df.columns:\n        if df[column].dtype == \"bool\":\n            df[column] = df[column].astype(int).values\n            continue\n        if is_numeric_dtype(df[column].dtype):\n            continue\n        try:\n            df[column] = pd.to_numeric(df[column].values)\n            continue\n        except:\n            from sklearn.preprocessing import LabelEncoder\n\n            if columns_to_numeric is not None and column in columns_to_numeric:\n                le = LabelEncoder()\n                df[column] = le.fit_transform(df[column].values)\n                dict_classes[column] = list(le.classes_)\n            else:\n                del df[column]\n                columns_removed.append(column)\n\n    log.info(f\"Found {len(df.columns)} features: {', '.join(df.columns)}.\")\n    if columns_removed:\n        log.warning(\n            f\"FCS does not support strings, so the following columns where removed: {', '.join(columns_removed)}.\\nIf you want to keep these str observations, use the 'columns_to_numeric' argument to encod them.\"\n        )\n\n    fcswrite.write_fcs(str(path), list(df.columns), df.values, **fcswrite_kwargs)\n\n    if columns_to_numeric is not None:\n        return dict_classes\n</code></pre>"},{"location":"api/io/#scyan.write_csv","title":"<code>scyan.write_csv(adata, path, layer=None)</code>","text":"<p>Based on a <code>AnnData</code> object, it writes a CSV file that contains (i) all the markers intensities, (ii) every numeric column of <code>adata.obs</code>, and (iii) all <code>adata.obsm</code> variables.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p><code>AnnData</code> object to save.</p> required <code>path</code> <code>str</code> <p>Path to write the file.</p> required <code>layer</code> <code>Optional[str]</code> <p>Name of the <code>adata</code> layer from which intensities will be extracted. If <code>None</code>, uses <code>adata.X</code>.</p> <code>None</code> Source code in <code>scyan/_io.py</code> <pre><code>def write_csv(\n    adata: AnnData,\n    path: str,\n    layer: Optional[str] = None,\n) -&gt; Union[None, Dict]:\n    \"\"\"Based on a `AnnData` object, it writes a CSV file that contains (i) all the markers intensities, (ii) every numeric column of `adata.obs`, and (iii) all `adata.obsm` variables.\n\n    Args:\n        adata: `AnnData` object to save.\n        path: Path to write the file.\n        layer: Name of the `adata` layer from which intensities will be extracted. If `None`, uses `adata.X`.\n    \"\"\"\n    _to_df(adata, layer).to_csv(path)\n</code></pre>"},{"location":"api/model/","title":"scyan.Scyan","text":"<pre><code>### Usage example\nimport scyan\n\nadata, table = scyan.data.load(\"aml\")\n\nmodel = scyan.Scyan(adata, table)\nmodel.fit()\nmodel.predict()\n</code></pre> <p>Notations</p> <p>\\(N\\) denotes the number of cells, \\(P\\) the number of populations, \\(M\\) the number of markers, and \\(B\\) the size of a mini-batch (not the number of biological batches). You can find other definitions in the article.</p> <p>             Bases: <code>LightningModule</code></p> <p>Scyan, a.k.a Single-cell Cytometry Annotation Network. It is a wrapper to the ScyanModule that contains the core logic (the loss implementation, the forward function, ...). While ScyanModule works on tensors, this class works directly on AnnData objects. To read more about the initialization arguments, read init().</p> <p>Attributes:</p> Name Type Description <code>adata</code> <code>AnnData</code> <p>The provided <code>adata</code>.</p> <code>table</code> <code>Dataframe</code> <p>The knowledge table of \\(P\\) populations x \\(M\\) markers.</p> <code>n_pops</code> <code>int</code> <p>Number of populations considered, i.e. \\(P\\)</p> <code>hparams</code> <code>object</code> <p>Model hyperparameters</p> <code>module</code> <code>ScyanModule</code> <p>A ScyanModule object</p> Source code in <code>scyan/model.py</code> <pre><code>class Scyan(pl.LightningModule):\n    \"\"\"\n    Scyan, a.k.a Single-cell Cytometry Annotation Network.\n    It is a wrapper to the ScyanModule that contains the core logic (the loss implementation, the forward function, ...).\n    While ScyanModule works on tensors, this class works directly on AnnData objects.\n    To read more about the initialization arguments, read [__init__()][scyan.model.Scyan.__init__].\n\n    Attributes:\n        adata (AnnData): The provided `adata`.\n        table (pd.Dataframe): The knowledge table of $P$ populations x $M$ markers.\n        n_pops (int): Number of populations considered, i.e. $P$\n        hparams (object): Model hyperparameters\n        module (ScyanModule): A [ScyanModule][scyan.module.ScyanModule] object\n    \"\"\"\n\n    def __init__(\n        self,\n        adata: AnnData,\n        table: pd.DataFrame,\n        continuous_covariates: Optional[List[str]] = None,\n        categorical_covariates: Optional[List[str]] = None,\n        continuum_markers: Optional[List[str]] = None,\n        hidden_size: int = 16,\n        n_hidden_layers: int = 6,\n        n_layers: int = 7,\n        prior_std: float = 0.3,\n        warm_up: Optional[tuple[float, int]] = (0.35, 4),\n        lr: float = 5e-4,\n        batch_size: int = 8_192,\n        temperature: float = 0.5,\n        modulo_temp: int = 3,\n        max_samples: Optional[int] = 200_000,\n        batch_key: Optional[str] = None,\n    ):\n        \"\"\"\n        Args:\n            adata: `AnnData` object containing the FCS data of $N$ cells. **Warning**: it has to be preprocessed (e.g. `asinh` or `logicle`) and scaled (see https://mics-lab.github.io/scyan/tutorials/preprocessing/).\n            table: Dataframe of shape $(P, M)$ representing the biological knowledge about markers and populations. The columns names corresponds to marker that must be in `adata.var_names`.\n            continuous_covariates: Optional list of keys in `adata.obs` that refers to continuous variables to use during the training.\n            categorical_covariates: Optional list of keys in `adata.obs` that refers to categorical variables to use during the training.\n            continuum_markers: Optional list of markers from the table whose expression is a continuum (for instance, it is often the case for PD1/PDL1). We advise to use it carefully, and keep values of -1 and 1 in the table.\n            hidden_size: Hidden size of the MLP (`s`, `t`).\n            n_hidden_layers: Number of hidden layers in the MLP.\n            n_layers: Number of coupling layers.\n            prior_std: Standard deviation $\\sigma$ of the cell-specific random variable $H$.\n            warm_up: If not `None`, sets the model prior standard deviation to `max(warm_up[0], prior_std)` during the first `warm_up[1]` epochs.\n            lr: Model learning rate.\n            batch_size: Model batch size.\n            temperature: Temperature to favor small populations.\n            modulo_temp: At which frequency temperature has to be applied.\n            max_samples: Maximum number of samples per epoch.\n            batch_key: Key in `adata.obs` referring to the cell batch variable.\n        \"\"\"\n        super().__init__()\n        self.adata, self.table, self.continuum_markers = utils._validate_inputs(\n            adata, table, continuum_markers\n        )\n        self.continuous_covariates = utils._default_list(continuous_covariates)\n        self.categorical_covariates = utils._default_list(categorical_covariates)\n        self.n_pops = len(self.table)\n\n        self._is_fitted = False\n        self._num_workers = 0\n\n        self.save_hyperparameters(\n            ignore=[\n                \"adata\",\n                \"table\",\n                \"continuous_covariates\",\n                \"categorical_covariates\",\n                \"continuum_markers\",\n            ]\n        )\n\n        self._prepare_data()\n\n        self.module = ScyanModule(\n            torch.tensor(table.values, dtype=torch.float32),\n            self.covariates.shape[1],\n            torch.tensor(np.isin(self.var_names, self.continuum_markers)),\n            hidden_size,\n            n_hidden_layers,\n            n_layers,\n            max(warm_up[0], prior_std) if warm_up is not None else prior_std,\n            temperature,\n        )\n\n        log.info(f\"Initialized {self}\")\n\n    def __repr__(self) -&gt; str:\n        covs = self.continuous_covariates + self.categorical_covariates\n        return f\"Scyan model with N={self.adata.n_obs} cells, P={self.n_pops} populations and M={len(self.var_names)} markers.\\n   \u251c\u2500\u2500 {'No covariate provided' if not len(covs) else 'Covariates: ' + ', '.join(covs)}\\n   \u251c\u2500\u2500 {'No continuum-marker provided' if not len(self.continuum_markers) else 'Continuum markers: ' + ', '.join(self.continuum_markers)}\\n   \u2514\u2500\u2500 Batch correction mode: {self._corr_mode}\"\n\n    @property\n    def _corr_mode(self):\n        return self.hparams.batch_key is not None\n\n    @property\n    def pop_names(self) -&gt; pd.Index:\n        \"\"\"Name of the populations considered in the knowledge table\"\"\"\n        return self.table.index.get_level_values(0)\n\n    @property\n    def var_names(self) -&gt; pd.Index:\n        \"\"\"Name of the markers considered in the knowledge table\"\"\"\n        return self.table.columns\n\n    @property\n    def level_names(self):\n        \"\"\"All population hierarchical level names, if existing.\"\"\"\n        if not isinstance(self.table.index, pd.MultiIndex):\n            log.warning(\n                \"The provided knowledge table has no population hierarchical level. See: https://mics-lab.github.io/scyan/tutorials/usage/#working-with-hierarchical-populations\"\n            )\n            return []\n\n        return list(self.table.index.names[1:])\n\n    def pops(\n        self,\n        level: Union[str, int, None] = None,\n        parent_of: Optional[str] = None,\n        children_of: Optional[str] = None,\n    ) -&gt; Union[List, str]:\n        \"\"\"Get the name of the populations that match a given contraint (only available if a hierarchical populations are provided, see [this tutorial](../../tutorials/usage/#working-with-hierarchical-populations)). If `level` is provided, returns all populations at this level. If `parent_of`, returns the parent of the given pop. If `children_of`, returns the children of the given pop.\n\n        !!! note\n            If you want to get the names of the leaves populations, you can simply use `model.pop_names`, which is equivalent to `model.pops(level=0)`.\n\n        Args:\n            level: If `str`, level name. If `int`, level index (0 corresponds to leaves).\n            parent_of: name of the population of which we want to get the parent in the tree.\n            children_of: name of the population of which we want to get the children populations in the tree.\n\n        Returns:\n            List of all populations that match the contraint, or one name if `parent_of` is not `None`.\n        \"\"\"\n\n        assert (\n            self.level_names\n        ), \"The provided knowledge table has no population hierarchical level. See the doc.\"\n\n        assert (\n            sum(arg is not None for arg in [level, parent_of, children_of]) == 1\n        ), \"One and exactly one argument has to be provided. Choose one among 'level', 'parent_of', and 'children_of'.\"\n\n        if level is not None:\n            assert (\n                isinstance(level, int) or level in self.level_names\n            ), f\"Level has to be one of [{', '.join(self.level_names)}]. Found {level}.\"\n\n            return list(set(self.table.index.get_level_values(level)))\n\n        name = parent_of or children_of\n        index = utils._get_pop_index(name, self.table)\n        where = self.table.index.get_level_values(index) == name\n\n        if children_of is not None:\n            if index == 0:\n                return []\n            return list(set(self.table.index.get_level_values(index - 1)[where]))\n\n        assert (\n            index &lt; self.table.index.nlevels - 1\n        ), \"Can not get parent of highest level population.\"\n\n        return self.table.index.get_level_values(index + 1)[where][0]\n\n    def _prepare_data(self) -&gt; None:\n        \"\"\"Initialize the data and the covariates\"\"\"\n        x, covariates = _prepare_data(\n            self.adata,\n            self.var_names,\n            self.hparams.batch_key,\n            self.categorical_covariates,\n            self.continuous_covariates,\n        )\n\n        self.register_buffer(\"x\", x)\n        self.register_buffer(\"covariates\", covariates)\n\n        self._n_samples = (\n            min(self.hparams.max_samples or self.adata.n_obs, self.adata.n_obs)\n            // self._batch_size\n            * self._batch_size\n        )\n\n    @_requires_fit\n    def forward(self, indices: Optional[np.ndarray] = None) -&gt; Tensor:\n        \"\"\"Model forward function (not used during training, see `training_step`instead).\n\n        !!! note\n            The core logic and the functions used for training are implemented in [ScyanModule][scyan.module.ScyanModule] (or see [scyan.Scyan.training_step][]).\n\n        Args:\n            indices: Indices of the cells to forward. By default, use all cells.\n\n        Returns:\n            Latent representation of the considered cells.\n        \"\"\"\n        if indices is None:\n            indices = np.arange(self.adata.n_obs)\n\n        x = self.x[indices]\n        cov = self.covariates[indices]\n\n        return self.dataset_apply(lambda *batch: self.module(*batch)[0], (x, cov))\n\n    def _repeat_ref_covariates(self, batch_ref: str, k: Optional[int] = None) -&gt; Tensor:\n        \"\"\"Repeat the covariates from the reference batch along axis 0.\n\n        Args:\n            k: Number of repetitions. By default, the number of cells $N$.\n\n        Returns:\n            A tensor of covariates of shape $(k, M_c)$\n        \"\"\"\n        n_repetitions = self.adata.n_obs if k is None else k\n\n        ref_covariate = self.covariates[\n            self.adata.obs[self.hparams.batch_key] == batch_ref\n        ][0]\n        return ref_covariate.repeat((n_repetitions, 1))\n\n    @torch.no_grad()\n    @_requires_fit\n    def sample(\n        self,\n        n_samples: int,\n        covariates_sample: Optional[Tensor] = None,\n        pop: Union[str, List[str], int, Tensor, None] = None,\n        return_z: bool = False,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Sampling cells by sampling from the prior distribution and going into the normalizing flow.\n\n        Args:\n            n_samples: Number of cells to sample.\n            covariates_sample: Optional tensor of covariates. If not provided: if the model was trained for batch correction then the reference covariates are repeated, else we sample from all the covariates.\n            pop: Optional population to sample from (by default, sample from all populations). If `str`, then a population name. If `int`, a population index. If `List[str]`, a list of population names. If `Tensor`, a tensor of population indices.\n            return_z: Whether to return the population `Tensor` (i.e., a tensor of population indices, whose order corresponds to `model.pop_names`).\n\n        Returns:\n            Sampled cells expressions and, if `return_z`, the populations associated to these cells.\n        \"\"\"\n        z = utils._process_pop_sample(self, pop)\n\n        if covariates_sample is None:\n            if self.hparams.batch_key is None:\n                indices = random.sample(range(self.adata.n_obs), n_samples)\n                covariates_sample = self.covariates[indices]\n            else:\n                covariates_sample = self._repeat_ref_covariates(n_samples)\n\n        return self.module.sample(n_samples, covariates_sample, z=z, return_z=return_z)\n\n    @torch.no_grad()\n    @_requires_fit\n    @utils._corr_mode_required\n    def batch_effect_correction(self, batch_ref: Optional[str] = None) -&gt; Tensor:\n        \"\"\"Correct batch effect by going into the latent space, setting the reference covariate to all cells, and then reversing the flow.\n\n        !!! info\n            To have a better batch effect correction, we advise to run [refine_fit()][scyan.Scyan.refine_fit] first.\n\n        !!! warning\n            As we standardised data for training, the resulting tensor is standardised too. You can save the tensor as a numpy layer of `adata` and use [scyan.preprocess.unscale][] to unscale it.\n\n        Args:\n            batch_ref: Name of the batch that will be considered as the reference. By default, it chooses the batch with the highest number of cells.\n\n        Returns:\n            The corrected marker expressions in the original space (a Tensor of shape $N$ cells x $M$ markers).\n        \"\"\"\n        batch_ref = utils._check_batch_arg(self.adata, self.hparams.batch_key, batch_ref)\n\n        u = self()\n        ref_covariates = self._repeat_ref_covariates(batch_ref)\n\n        return self.dataset_apply(self.module.inverse, (u, ref_covariates))\n\n    def training_step(self, data, _):\n        \"\"\"PyTorch lightning `training_step` implementation (i.e. returning the loss). See [ScyanModule][scyan.module.ScyanModule] for more details.\"\"\"\n        use_temp = self.current_epoch % self.hparams.modulo_temp &gt; 0\n        loss = self.module.kl(*data, use_temp)\n\n        self.log(\"loss\", loss, on_epoch=True, on_step=True)\n\n        return loss\n\n    def on_train_epoch_end(self):\n        if (\n            self.hparams.warm_up is not None\n            and self.current_epoch == self.hparams.warm_up[1] - 1\n        ):\n            log.info(\"Ended warm up epochs\")\n            self.module.prior.prior_std = self.hparams.prior_std\n\n    @_requires_fit\n    @torch.no_grad()\n    def predict(\n        self,\n        key_added: Optional[str] = \"scyan_pop\",\n        add_levels: bool = True,\n        log_prob_th: float = -50,\n    ) -&gt; pd.Series:\n        \"\"\"Model population predictions, i.e. one population is assigned for each cell. Predictions are saved in `adata.obs.scyan_pop` by default.\n\n        !!! note\n            Some cells may not be annotated, if their log probability is lower than `log_prob_th` for all populations. Then, the predicted label will be `np.nan`.\n\n        Args:\n            key_added: Column name used to save the predictions in `adata.obs`. If `None`, then the predictions will not be saved.\n            add_levels: If `True`, and if [hierarchical population names](../../tutorials/usage/#working-with-hierarchical-populations) were provided, then it also saves the prediction for every population level.\n            log_prob_th: If the log-probability of the most probable population for one cell is below this threshold, this cell will not be annotated (`np.nan`).\n\n        Returns:\n            Population predictions (pandas `Series` of length $N$ cells).\n        \"\"\"\n        df = self.predict_proba()\n        self.adata.obs[\"scyan_log_probs\"] = df[\"max_log_prob_u\"].values\n\n        populations = df.iloc[:, : self.n_pops].idxmax(axis=1).astype(\"category\")\n        populations[df[\"max_log_prob_u\"] &lt; log_prob_th] = np.nan\n\n        if key_added is not None:\n            self.adata.obs[key_added] = pd.Categorical(\n                populations, categories=self.pop_names\n            )\n            if add_levels and isinstance(self.table.index, pd.MultiIndex):\n                utils._add_level_predictions(self, key_added)\n\n        missing_pops = self.n_pops - len(populations.cat.categories)\n        if missing_pops:\n            log.warning(\n                f\"{missing_pops} population(s) were not predicted. It may be due to:\\n  - Errors in the knowledge table (see https://mics-lab.github.io/scyan/advice/#advice-for-the-creation-of-the-table)\\n  - The model hyperparameters choice (see https://mics-lab.github.io/scyan/advanced/parameters/)\\n  - Or maybe these populations are really absent from this dataset.\"\n            )\n\n        return populations\n\n    @_requires_fit\n    @torch.no_grad()\n    def predict_proba(self) -&gt; pd.DataFrame:\n        \"\"\"Soft predictions (i.e. an array of probability per population) for each cell.\n\n        Returns:\n            Dataframe of shape `(N, P)` with probabilities for each population.\n        \"\"\"\n        log_probs = self.dataset_apply(\n            lambda *data: self.module.compute_probabilities(*data)[0]\n        )\n        probs = torch.softmax(log_probs, dim=1)\n\n        df = pd.DataFrame(probs.numpy(force=True), columns=self.pop_names)\n\n        max_log_probs = log_probs.max(1)\n        df[\"max_log_prob\"] = max_log_probs.values.numpy(force=True)\n\n        log_pi = self.module.log_pi[max_log_probs.indices].numpy(force=True)\n        df[\"max_log_prob_u\"] = df[\"max_log_prob\"] - log_pi\n\n        return df\n\n    def configure_optimizers(self):\n        \"\"\"PyTorch lightning `configure_optimizers` implementation\"\"\"\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n\n    def save(self, path: str) -&gt; None:\n        \"\"\"Saves the Scyan model `state_dict` at the provided path.\n\n        Args:\n            path: Path where the parameters will be saved. For instance, `'scyan_state_dict.pt'`.\n        \"\"\"\n        torch.save(self.state_dict(), path)\n\n    def load(self, path: str) -&gt; None:\n        \"\"\"Loads the Scyan model that was saved at the provided path. Note that the model has to be initialized with the same arguments.\n\n        !!! example\n            ```python\n            &gt;&gt;&gt; model = scyan.Scyan(adata, table) # initialize the model\n            &gt;&gt;&gt; model.load('scyan_state_dict.pt')\n            ```\n\n        Args:\n            path: Path where the parameters were saved, i.e. the argument of `model.save(path)`.\n        \"\"\"\n        self.load_state_dict(torch.load(path))\n        self._is_fitted = True\n\n    @property\n    def _batch_size(self):\n        return min(self.hparams.batch_size, self.adata.n_obs)\n\n    def train_dataloader(self):\n        \"\"\"PyTorch lightning `train_dataloader` implementation\"\"\"\n        return DataLoader(\n            TensorDataset(self.x, self.covariates),\n            batch_size=self._batch_size,\n            sampler=RandomSampler(self.adata.n_obs, self._n_samples),\n            num_workers=self._num_workers,\n        )\n\n    def predict_dataloader(self):\n        \"\"\"PyTorch lightning `predict_dataloader` implementation\"\"\"\n        return DataLoader(\n            TensorDataset(self.x, self.covariates),\n            batch_size=self._batch_size,\n            num_workers=self._num_workers,\n        )\n\n    def dataset_apply(self, func: Callable, data: Tuple[Tensor] = None) -&gt; Tensor:\n        \"\"\"Apply a function on a dataset using a PyTorch DataLoader and with a progress bar display. It concatenates the results along the first axis.\n\n        Args:\n            func: Function to be applied. It takes a batch, and returns a Tensor.\n            data: Optional tuple of tensors to load from (we create a TensorDataset). By default, uses the main dataset.\n\n        Returns:\n            Tensor of concatenated results.\n        \"\"\"\n        if importlib.util.find_spec(\"ipywidgets\") is not None:\n            from tqdm.autonotebook import tqdm\n        else:\n            from tqdm import tqdm\n\n        if data is None:\n            loader = self.predict_dataloader()\n        else:\n            loader = DataLoader(\n                TensorDataset(*data),\n                batch_size=self._batch_size,\n                num_workers=self._num_workers,\n            )\n\n        return torch.cat(\n            [func(*batch) for batch in tqdm(loader, desc=\"DataLoader\")], dim=0\n        )\n\n    @torch.no_grad()\n    @utils._corr_mode_required\n    def refine_fit(\n        self,\n        patience: int = 10,\n        min_delta: float = 0.2,\n        key: str = \"scyan_pop\",\n        **fit_kwargs: int,\n    ):\n        \"\"\"Improve training (and also batch effect correction) by filling the NaN values in the table and continue fitting Scyan. Afterwards, you can correct batch effect with [batch_effect_correction()][scyan.Scyan.batch_effect_correction].\n\n        !!! info\n            Run this function only to improve batch effect correction (it is not designed to improve annotation).\n\n        Args:\n            patience: Number of epochs with no loss improvement before stopping training.\n            min_delta: min_delta parameters used for `EarlyStopping`. See Pytorch Lightning docs.\n            key: Column name used to save the predictions in `adata.obs`.\n        \"\"\"\n        assert (\n            key in self.adata.obs\n        ), f\"Column {key} not found in 'adata.obs'. Have you run 'model.predict()' first?\"\n\n        if self.module.prior.rho_mask.any():\n            log.info(\n                f\"Filling {self.module.prior.rho_mask.sum()} NA values in the table.\"\n            )\n            means = utils.grouped_mean(self, key)\n            self.module.prior.fill_rho(means)\n\n        self.fit(patience=patience, min_delta=min_delta, **fit_kwargs)\n\n    def fit(\n        self,\n        max_epochs: int = 100,\n        accelerator: str = \"cpu\",\n        min_delta: float = 1,\n        patience: int = 4,\n        num_workers: int = 0,\n        log_every_n_steps: int = 10,\n        callbacks: Optional[List[pl.Callback]] = None,\n        logger: Union[bool, \"pl.Logger\"] = False,\n        enable_checkpointing: bool = False,\n        trainer: Optional[pl.Trainer] = None,\n        **trainer_args: int,\n    ) -&gt; \"Scyan\":\n        \"\"\"Train the `Scyan` model. On interactive Python (e.g., Jupyter Notebooks), training can be interrupted at any time without crashing.\n\n        !!! note\n            The Pytorch Lightning training is used under the hood (see the corresponding API [here](https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-class-api))\n\n        !!! note\n            Depending on your machine, you may have a warning about some performance issues. You can simply set `num_workers` to the number indicated by the warning.\n\n        Args:\n            max_epochs: Maximum number of epochs.\n            accelerator: Accelerator used during training. See Pytorch Lightning docs.\n            min_delta: min_delta parameters used for `EarlyStopping`. See Pytorch Lightning docs.\n            patience: Number of epochs with no loss improvement before stopping training.\n            num_workers: Pytorch DataLoader `num_workers` argument, i.e. how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\n            log_every_n_steps: How often to log within steps (see Pytorch Lightning Trainer API).\n            callbacks: Optional list of Pytorch Lightning callbacks (see their Trainer API). They will be added to our EarlyStopping callback.\n            logger: Pytorch Lightning logger argument (see their Trainer API).\n            enable_checkpointing: If `True`, enables Pytorch Lightning checkpointing.\n            trainer: Optional Pytorch Lightning Trainer. **Warning**: it will replace the default Trainer, and every other argument will be unused.\n            **trainer_args: Optional kwargs to provide to the `pytorch_lightning.Trainer` initialization.\n\n        Returns:\n            The trained model itself.\n        \"\"\"\n        log.info(f\"Training scyan with the following hyperparameters:\\n{self.hparams}\\n\")\n\n        self._num_workers = num_workers\n\n        if trainer is None:\n            esc = EarlyStopping(\n                monitor=\"loss_epoch\",\n                min_delta=min_delta,\n                patience=patience,\n                check_on_train_epoch_end=True,\n            )\n\n            log_every_n_steps = min(log_every_n_steps, len(self.x) // self._batch_size)\n            trainer = pl.Trainer(\n                max_epochs=max_epochs,\n                accelerator=accelerator,\n                callbacks=[esc] + (callbacks or []),\n                log_every_n_steps=log_every_n_steps,\n                enable_checkpointing=enable_checkpointing,\n                logger=logger,\n                **trainer_args,\n            )\n\n        trainer.fit(self)\n\n        self._is_fitted = True\n        log.info(\"Successfully ended training.\")\n\n        return self\n</code></pre>"},{"location":"api/model/#scyan.Scyan.__init__","title":"<code>__init__(adata, table, continuous_covariates=None, categorical_covariates=None, continuum_markers=None, hidden_size=16, n_hidden_layers=6, n_layers=7, prior_std=0.3, warm_up=(0.35, 4), lr=0.0005, batch_size=8192, temperature=0.5, modulo_temp=3, max_samples=200000, batch_key=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p><code>AnnData</code> object containing the FCS data of \\(N\\) cells. Warning: it has to be preprocessed (e.g. <code>asinh</code> or <code>logicle</code>) and scaled (see https://mics-lab.github.io/scyan/tutorials/preprocessing/).</p> required <code>table</code> <code>DataFrame</code> <p>Dataframe of shape \\((P, M)\\) representing the biological knowledge about markers and populations. The columns names corresponds to marker that must be in <code>adata.var_names</code>.</p> required <code>continuous_covariates</code> <code>Optional[List[str]]</code> <p>Optional list of keys in <code>adata.obs</code> that refers to continuous variables to use during the training.</p> <code>None</code> <code>categorical_covariates</code> <code>Optional[List[str]]</code> <p>Optional list of keys in <code>adata.obs</code> that refers to categorical variables to use during the training.</p> <code>None</code> <code>continuum_markers</code> <code>Optional[List[str]]</code> <p>Optional list of markers from the table whose expression is a continuum (for instance, it is often the case for PD1/PDL1). We advise to use it carefully, and keep values of -1 and 1 in the table.</p> <code>None</code> <code>hidden_size</code> <code>int</code> <p>Hidden size of the MLP (<code>s</code>, <code>t</code>).</p> <code>16</code> <code>n_hidden_layers</code> <code>int</code> <p>Number of hidden layers in the MLP.</p> <code>6</code> <code>n_layers</code> <code>int</code> <p>Number of coupling layers.</p> <code>7</code> <code>prior_std</code> <code>float</code> <p>Standard deviation \\(\\sigma\\) of the cell-specific random variable \\(H\\).</p> <code>0.3</code> <code>warm_up</code> <code>Optional[tuple[float, int]]</code> <p>If not <code>None</code>, sets the model prior standard deviation to <code>max(warm_up[0], prior_std)</code> during the first <code>warm_up[1]</code> epochs.</p> <code>(0.35, 4)</code> <code>lr</code> <code>float</code> <p>Model learning rate.</p> <code>0.0005</code> <code>batch_size</code> <code>int</code> <p>Model batch size.</p> <code>8192</code> <code>temperature</code> <code>float</code> <p>Temperature to favor small populations.</p> <code>0.5</code> <code>modulo_temp</code> <code>int</code> <p>At which frequency temperature has to be applied.</p> <code>3</code> <code>max_samples</code> <code>Optional[int]</code> <p>Maximum number of samples per epoch.</p> <code>200000</code> <code>batch_key</code> <code>Optional[str]</code> <p>Key in <code>adata.obs</code> referring to the cell batch variable.</p> <code>None</code> Source code in <code>scyan/model.py</code> <pre><code>def __init__(\n    self,\n    adata: AnnData,\n    table: pd.DataFrame,\n    continuous_covariates: Optional[List[str]] = None,\n    categorical_covariates: Optional[List[str]] = None,\n    continuum_markers: Optional[List[str]] = None,\n    hidden_size: int = 16,\n    n_hidden_layers: int = 6,\n    n_layers: int = 7,\n    prior_std: float = 0.3,\n    warm_up: Optional[tuple[float, int]] = (0.35, 4),\n    lr: float = 5e-4,\n    batch_size: int = 8_192,\n    temperature: float = 0.5,\n    modulo_temp: int = 3,\n    max_samples: Optional[int] = 200_000,\n    batch_key: Optional[str] = None,\n):\n    \"\"\"\n    Args:\n        adata: `AnnData` object containing the FCS data of $N$ cells. **Warning**: it has to be preprocessed (e.g. `asinh` or `logicle`) and scaled (see https://mics-lab.github.io/scyan/tutorials/preprocessing/).\n        table: Dataframe of shape $(P, M)$ representing the biological knowledge about markers and populations. The columns names corresponds to marker that must be in `adata.var_names`.\n        continuous_covariates: Optional list of keys in `adata.obs` that refers to continuous variables to use during the training.\n        categorical_covariates: Optional list of keys in `adata.obs` that refers to categorical variables to use during the training.\n        continuum_markers: Optional list of markers from the table whose expression is a continuum (for instance, it is often the case for PD1/PDL1). We advise to use it carefully, and keep values of -1 and 1 in the table.\n        hidden_size: Hidden size of the MLP (`s`, `t`).\n        n_hidden_layers: Number of hidden layers in the MLP.\n        n_layers: Number of coupling layers.\n        prior_std: Standard deviation $\\sigma$ of the cell-specific random variable $H$.\n        warm_up: If not `None`, sets the model prior standard deviation to `max(warm_up[0], prior_std)` during the first `warm_up[1]` epochs.\n        lr: Model learning rate.\n        batch_size: Model batch size.\n        temperature: Temperature to favor small populations.\n        modulo_temp: At which frequency temperature has to be applied.\n        max_samples: Maximum number of samples per epoch.\n        batch_key: Key in `adata.obs` referring to the cell batch variable.\n    \"\"\"\n    super().__init__()\n    self.adata, self.table, self.continuum_markers = utils._validate_inputs(\n        adata, table, continuum_markers\n    )\n    self.continuous_covariates = utils._default_list(continuous_covariates)\n    self.categorical_covariates = utils._default_list(categorical_covariates)\n    self.n_pops = len(self.table)\n\n    self._is_fitted = False\n    self._num_workers = 0\n\n    self.save_hyperparameters(\n        ignore=[\n            \"adata\",\n            \"table\",\n            \"continuous_covariates\",\n            \"categorical_covariates\",\n            \"continuum_markers\",\n        ]\n    )\n\n    self._prepare_data()\n\n    self.module = ScyanModule(\n        torch.tensor(table.values, dtype=torch.float32),\n        self.covariates.shape[1],\n        torch.tensor(np.isin(self.var_names, self.continuum_markers)),\n        hidden_size,\n        n_hidden_layers,\n        n_layers,\n        max(warm_up[0], prior_std) if warm_up is not None else prior_std,\n        temperature,\n    )\n\n    log.info(f\"Initialized {self}\")\n</code></pre>"},{"location":"api/model/#scyan.Scyan.fit","title":"<code>fit(max_epochs=100, accelerator='cpu', min_delta=1, patience=4, num_workers=0, log_every_n_steps=10, callbacks=None, logger=False, enable_checkpointing=False, trainer=None, **trainer_args)</code>","text":"<p>Train the <code>Scyan</code> model. On interactive Python (e.g., Jupyter Notebooks), training can be interrupted at any time without crashing.</p> <p>Note</p> <p>The Pytorch Lightning training is used under the hood (see the corresponding API here)</p> <p>Note</p> <p>Depending on your machine, you may have a warning about some performance issues. You can simply set <code>num_workers</code> to the number indicated by the warning.</p> <p>Parameters:</p> Name Type Description Default <code>max_epochs</code> <code>int</code> <p>Maximum number of epochs.</p> <code>100</code> <code>accelerator</code> <code>str</code> <p>Accelerator used during training. See Pytorch Lightning docs.</p> <code>'cpu'</code> <code>min_delta</code> <code>float</code> <p>min_delta parameters used for <code>EarlyStopping</code>. See Pytorch Lightning docs.</p> <code>1</code> <code>patience</code> <code>int</code> <p>Number of epochs with no loss improvement before stopping training.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Pytorch DataLoader <code>num_workers</code> argument, i.e. how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.</p> <code>0</code> <code>log_every_n_steps</code> <code>int</code> <p>How often to log within steps (see Pytorch Lightning Trainer API).</p> <code>10</code> <code>callbacks</code> <code>Optional[List[Callback]]</code> <p>Optional list of Pytorch Lightning callbacks (see their Trainer API). They will be added to our EarlyStopping callback.</p> <code>None</code> <code>logger</code> <code>Union[bool, Logger]</code> <p>Pytorch Lightning logger argument (see their Trainer API).</p> <code>False</code> <code>enable_checkpointing</code> <code>bool</code> <p>If <code>True</code>, enables Pytorch Lightning checkpointing.</p> <code>False</code> <code>trainer</code> <code>Optional[Trainer]</code> <p>Optional Pytorch Lightning Trainer. Warning: it will replace the default Trainer, and every other argument will be unused.</p> <code>None</code> <code>**trainer_args</code> <code>int</code> <p>Optional kwargs to provide to the <code>pytorch_lightning.Trainer</code> initialization.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Scyan</code> <p>The trained model itself.</p> Source code in <code>scyan/model.py</code> <pre><code>def fit(\n    self,\n    max_epochs: int = 100,\n    accelerator: str = \"cpu\",\n    min_delta: float = 1,\n    patience: int = 4,\n    num_workers: int = 0,\n    log_every_n_steps: int = 10,\n    callbacks: Optional[List[pl.Callback]] = None,\n    logger: Union[bool, \"pl.Logger\"] = False,\n    enable_checkpointing: bool = False,\n    trainer: Optional[pl.Trainer] = None,\n    **trainer_args: int,\n) -&gt; \"Scyan\":\n    \"\"\"Train the `Scyan` model. On interactive Python (e.g., Jupyter Notebooks), training can be interrupted at any time without crashing.\n\n    !!! note\n        The Pytorch Lightning training is used under the hood (see the corresponding API [here](https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-class-api))\n\n    !!! note\n        Depending on your machine, you may have a warning about some performance issues. You can simply set `num_workers` to the number indicated by the warning.\n\n    Args:\n        max_epochs: Maximum number of epochs.\n        accelerator: Accelerator used during training. See Pytorch Lightning docs.\n        min_delta: min_delta parameters used for `EarlyStopping`. See Pytorch Lightning docs.\n        patience: Number of epochs with no loss improvement before stopping training.\n        num_workers: Pytorch DataLoader `num_workers` argument, i.e. how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\n        log_every_n_steps: How often to log within steps (see Pytorch Lightning Trainer API).\n        callbacks: Optional list of Pytorch Lightning callbacks (see their Trainer API). They will be added to our EarlyStopping callback.\n        logger: Pytorch Lightning logger argument (see their Trainer API).\n        enable_checkpointing: If `True`, enables Pytorch Lightning checkpointing.\n        trainer: Optional Pytorch Lightning Trainer. **Warning**: it will replace the default Trainer, and every other argument will be unused.\n        **trainer_args: Optional kwargs to provide to the `pytorch_lightning.Trainer` initialization.\n\n    Returns:\n        The trained model itself.\n    \"\"\"\n    log.info(f\"Training scyan with the following hyperparameters:\\n{self.hparams}\\n\")\n\n    self._num_workers = num_workers\n\n    if trainer is None:\n        esc = EarlyStopping(\n            monitor=\"loss_epoch\",\n            min_delta=min_delta,\n            patience=patience,\n            check_on_train_epoch_end=True,\n        )\n\n        log_every_n_steps = min(log_every_n_steps, len(self.x) // self._batch_size)\n        trainer = pl.Trainer(\n            max_epochs=max_epochs,\n            accelerator=accelerator,\n            callbacks=[esc] + (callbacks or []),\n            log_every_n_steps=log_every_n_steps,\n            enable_checkpointing=enable_checkpointing,\n            logger=logger,\n            **trainer_args,\n        )\n\n    trainer.fit(self)\n\n    self._is_fitted = True\n    log.info(\"Successfully ended training.\")\n\n    return self\n</code></pre>"},{"location":"api/model/#scyan.Scyan.predict","title":"<code>predict(key_added='scyan_pop', add_levels=True, log_prob_th=-50)</code>","text":"<p>Model population predictions, i.e. one population is assigned for each cell. Predictions are saved in <code>adata.obs.scyan_pop</code> by default.</p> <p>Note</p> <p>Some cells may not be annotated, if their log probability is lower than <code>log_prob_th</code> for all populations. Then, the predicted label will be <code>np.nan</code>.</p> <p>Parameters:</p> Name Type Description Default <code>key_added</code> <code>Optional[str]</code> <p>Column name used to save the predictions in <code>adata.obs</code>. If <code>None</code>, then the predictions will not be saved.</p> <code>'scyan_pop'</code> <code>add_levels</code> <code>bool</code> <p>If <code>True</code>, and if hierarchical population names were provided, then it also saves the prediction for every population level.</p> <code>True</code> <code>log_prob_th</code> <code>float</code> <p>If the log-probability of the most probable population for one cell is below this threshold, this cell will not be annotated (<code>np.nan</code>).</p> <code>-50</code> <p>Returns:</p> Type Description <code>Series</code> <p>Population predictions (pandas <code>Series</code> of length \\(N\\) cells).</p> Source code in <code>scyan/model.py</code> <pre><code>@_requires_fit\n@torch.no_grad()\ndef predict(\n    self,\n    key_added: Optional[str] = \"scyan_pop\",\n    add_levels: bool = True,\n    log_prob_th: float = -50,\n) -&gt; pd.Series:\n    \"\"\"Model population predictions, i.e. one population is assigned for each cell. Predictions are saved in `adata.obs.scyan_pop` by default.\n\n    !!! note\n        Some cells may not be annotated, if their log probability is lower than `log_prob_th` for all populations. Then, the predicted label will be `np.nan`.\n\n    Args:\n        key_added: Column name used to save the predictions in `adata.obs`. If `None`, then the predictions will not be saved.\n        add_levels: If `True`, and if [hierarchical population names](../../tutorials/usage/#working-with-hierarchical-populations) were provided, then it also saves the prediction for every population level.\n        log_prob_th: If the log-probability of the most probable population for one cell is below this threshold, this cell will not be annotated (`np.nan`).\n\n    Returns:\n        Population predictions (pandas `Series` of length $N$ cells).\n    \"\"\"\n    df = self.predict_proba()\n    self.adata.obs[\"scyan_log_probs\"] = df[\"max_log_prob_u\"].values\n\n    populations = df.iloc[:, : self.n_pops].idxmax(axis=1).astype(\"category\")\n    populations[df[\"max_log_prob_u\"] &lt; log_prob_th] = np.nan\n\n    if key_added is not None:\n        self.adata.obs[key_added] = pd.Categorical(\n            populations, categories=self.pop_names\n        )\n        if add_levels and isinstance(self.table.index, pd.MultiIndex):\n            utils._add_level_predictions(self, key_added)\n\n    missing_pops = self.n_pops - len(populations.cat.categories)\n    if missing_pops:\n        log.warning(\n            f\"{missing_pops} population(s) were not predicted. It may be due to:\\n  - Errors in the knowledge table (see https://mics-lab.github.io/scyan/advice/#advice-for-the-creation-of-the-table)\\n  - The model hyperparameters choice (see https://mics-lab.github.io/scyan/advanced/parameters/)\\n  - Or maybe these populations are really absent from this dataset.\"\n        )\n\n    return populations\n</code></pre>"},{"location":"api/model/#scyan.Scyan.predict_proba","title":"<code>predict_proba()</code>","text":"<p>Soft predictions (i.e. an array of probability per population) for each cell.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe of shape <code>(N, P)</code> with probabilities for each population.</p> Source code in <code>scyan/model.py</code> <pre><code>@_requires_fit\n@torch.no_grad()\ndef predict_proba(self) -&gt; pd.DataFrame:\n    \"\"\"Soft predictions (i.e. an array of probability per population) for each cell.\n\n    Returns:\n        Dataframe of shape `(N, P)` with probabilities for each population.\n    \"\"\"\n    log_probs = self.dataset_apply(\n        lambda *data: self.module.compute_probabilities(*data)[0]\n    )\n    probs = torch.softmax(log_probs, dim=1)\n\n    df = pd.DataFrame(probs.numpy(force=True), columns=self.pop_names)\n\n    max_log_probs = log_probs.max(1)\n    df[\"max_log_prob\"] = max_log_probs.values.numpy(force=True)\n\n    log_pi = self.module.log_pi[max_log_probs.indices].numpy(force=True)\n    df[\"max_log_prob_u\"] = df[\"max_log_prob\"] - log_pi\n\n    return df\n</code></pre>"},{"location":"api/model/#scyan.Scyan.refine_fit","title":"<code>refine_fit(patience=10, min_delta=0.2, key='scyan_pop', **fit_kwargs)</code>","text":"<p>Improve training (and also batch effect correction) by filling the NaN values in the table and continue fitting Scyan. Afterwards, you can correct batch effect with batch_effect_correction().</p> <p>Info</p> <p>Run this function only to improve batch effect correction (it is not designed to improve annotation).</p> <p>Parameters:</p> Name Type Description Default <code>patience</code> <code>int</code> <p>Number of epochs with no loss improvement before stopping training.</p> <code>10</code> <code>min_delta</code> <code>float</code> <p>min_delta parameters used for <code>EarlyStopping</code>. See Pytorch Lightning docs.</p> <code>0.2</code> <code>key</code> <code>str</code> <p>Column name used to save the predictions in <code>adata.obs</code>.</p> <code>'scyan_pop'</code> Source code in <code>scyan/model.py</code> <pre><code>@torch.no_grad()\n@utils._corr_mode_required\ndef refine_fit(\n    self,\n    patience: int = 10,\n    min_delta: float = 0.2,\n    key: str = \"scyan_pop\",\n    **fit_kwargs: int,\n):\n    \"\"\"Improve training (and also batch effect correction) by filling the NaN values in the table and continue fitting Scyan. Afterwards, you can correct batch effect with [batch_effect_correction()][scyan.Scyan.batch_effect_correction].\n\n    !!! info\n        Run this function only to improve batch effect correction (it is not designed to improve annotation).\n\n    Args:\n        patience: Number of epochs with no loss improvement before stopping training.\n        min_delta: min_delta parameters used for `EarlyStopping`. See Pytorch Lightning docs.\n        key: Column name used to save the predictions in `adata.obs`.\n    \"\"\"\n    assert (\n        key in self.adata.obs\n    ), f\"Column {key} not found in 'adata.obs'. Have you run 'model.predict()' first?\"\n\n    if self.module.prior.rho_mask.any():\n        log.info(\n            f\"Filling {self.module.prior.rho_mask.sum()} NA values in the table.\"\n        )\n        means = utils.grouped_mean(self, key)\n        self.module.prior.fill_rho(means)\n\n    self.fit(patience=patience, min_delta=min_delta, **fit_kwargs)\n</code></pre>"},{"location":"api/model/#scyan.Scyan.batch_effect_correction","title":"<code>batch_effect_correction(batch_ref=None)</code>","text":"<p>Correct batch effect by going into the latent space, setting the reference covariate to all cells, and then reversing the flow.</p> <p>Info</p> <p>To have a better batch effect correction, we advise to run refine_fit() first.</p> <p>Warning</p> <p>As we standardised data for training, the resulting tensor is standardised too. You can save the tensor as a numpy layer of <code>adata</code> and use scyan.preprocess.unscale to unscale it.</p> <p>Parameters:</p> Name Type Description Default <code>batch_ref</code> <code>Optional[str]</code> <p>Name of the batch that will be considered as the reference. By default, it chooses the batch with the highest number of cells.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The corrected marker expressions in the original space (a Tensor of shape \\(N\\) cells x \\(M\\) markers).</p> Source code in <code>scyan/model.py</code> <pre><code>@torch.no_grad()\n@_requires_fit\n@utils._corr_mode_required\ndef batch_effect_correction(self, batch_ref: Optional[str] = None) -&gt; Tensor:\n    \"\"\"Correct batch effect by going into the latent space, setting the reference covariate to all cells, and then reversing the flow.\n\n    !!! info\n        To have a better batch effect correction, we advise to run [refine_fit()][scyan.Scyan.refine_fit] first.\n\n    !!! warning\n        As we standardised data for training, the resulting tensor is standardised too. You can save the tensor as a numpy layer of `adata` and use [scyan.preprocess.unscale][] to unscale it.\n\n    Args:\n        batch_ref: Name of the batch that will be considered as the reference. By default, it chooses the batch with the highest number of cells.\n\n    Returns:\n        The corrected marker expressions in the original space (a Tensor of shape $N$ cells x $M$ markers).\n    \"\"\"\n    batch_ref = utils._check_batch_arg(self.adata, self.hparams.batch_key, batch_ref)\n\n    u = self()\n    ref_covariates = self._repeat_ref_covariates(batch_ref)\n\n    return self.dataset_apply(self.module.inverse, (u, ref_covariates))\n</code></pre>"},{"location":"api/model/#scyan.Scyan.sample","title":"<code>sample(n_samples, covariates_sample=None, pop=None, return_z=False)</code>","text":"<p>Sampling cells by sampling from the prior distribution and going into the normalizing flow.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of cells to sample.</p> required <code>covariates_sample</code> <code>Optional[Tensor]</code> <p>Optional tensor of covariates. If not provided: if the model was trained for batch correction then the reference covariates are repeated, else we sample from all the covariates.</p> <code>None</code> <code>pop</code> <code>Union[str, List[str], int, Tensor, None]</code> <p>Optional population to sample from (by default, sample from all populations). If <code>str</code>, then a population name. If <code>int</code>, a population index. If <code>List[str]</code>, a list of population names. If <code>Tensor</code>, a tensor of population indices.</p> <code>None</code> <code>return_z</code> <code>bool</code> <p>Whether to return the population <code>Tensor</code> (i.e., a tensor of population indices, whose order corresponds to <code>model.pop_names</code>).</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Sampled cells expressions and, if <code>return_z</code>, the populations associated to these cells.</p> Source code in <code>scyan/model.py</code> <pre><code>@torch.no_grad()\n@_requires_fit\ndef sample(\n    self,\n    n_samples: int,\n    covariates_sample: Optional[Tensor] = None,\n    pop: Union[str, List[str], int, Tensor, None] = None,\n    return_z: bool = False,\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Sampling cells by sampling from the prior distribution and going into the normalizing flow.\n\n    Args:\n        n_samples: Number of cells to sample.\n        covariates_sample: Optional tensor of covariates. If not provided: if the model was trained for batch correction then the reference covariates are repeated, else we sample from all the covariates.\n        pop: Optional population to sample from (by default, sample from all populations). If `str`, then a population name. If `int`, a population index. If `List[str]`, a list of population names. If `Tensor`, a tensor of population indices.\n        return_z: Whether to return the population `Tensor` (i.e., a tensor of population indices, whose order corresponds to `model.pop_names`).\n\n    Returns:\n        Sampled cells expressions and, if `return_z`, the populations associated to these cells.\n    \"\"\"\n    z = utils._process_pop_sample(self, pop)\n\n    if covariates_sample is None:\n        if self.hparams.batch_key is None:\n            indices = random.sample(range(self.adata.n_obs), n_samples)\n            covariates_sample = self.covariates[indices]\n        else:\n            covariates_sample = self._repeat_ref_covariates(n_samples)\n\n    return self.module.sample(n_samples, covariates_sample, z=z, return_z=return_z)\n</code></pre>"},{"location":"api/model/#scyan.Scyan.pop_names","title":"<code>pop_names: pd.Index</code>  <code>property</code>","text":"<p>Name of the populations considered in the knowledge table</p>"},{"location":"api/model/#scyan.Scyan.var_names","title":"<code>var_names: pd.Index</code>  <code>property</code>","text":"<p>Name of the markers considered in the knowledge table</p>"},{"location":"api/model/#scyan.Scyan.pops","title":"<code>pops(level=None, parent_of=None, children_of=None)</code>","text":"<p>Get the name of the populations that match a given contraint (only available if a hierarchical populations are provided, see this tutorial). If <code>level</code> is provided, returns all populations at this level. If <code>parent_of</code>, returns the parent of the given pop. If <code>children_of</code>, returns the children of the given pop.</p> <p>Note</p> <p>If you want to get the names of the leaves populations, you can simply use <code>model.pop_names</code>, which is equivalent to <code>model.pops(level=0)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>Union[str, int, None]</code> <p>If <code>str</code>, level name. If <code>int</code>, level index (0 corresponds to leaves).</p> <code>None</code> <code>parent_of</code> <code>Optional[str]</code> <p>name of the population of which we want to get the parent in the tree.</p> <code>None</code> <code>children_of</code> <code>Optional[str]</code> <p>name of the population of which we want to get the children populations in the tree.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[List, str]</code> <p>List of all populations that match the contraint, or one name if <code>parent_of</code> is not <code>None</code>.</p> Source code in <code>scyan/model.py</code> <pre><code>def pops(\n    self,\n    level: Union[str, int, None] = None,\n    parent_of: Optional[str] = None,\n    children_of: Optional[str] = None,\n) -&gt; Union[List, str]:\n    \"\"\"Get the name of the populations that match a given contraint (only available if a hierarchical populations are provided, see [this tutorial](../../tutorials/usage/#working-with-hierarchical-populations)). If `level` is provided, returns all populations at this level. If `parent_of`, returns the parent of the given pop. If `children_of`, returns the children of the given pop.\n\n    !!! note\n        If you want to get the names of the leaves populations, you can simply use `model.pop_names`, which is equivalent to `model.pops(level=0)`.\n\n    Args:\n        level: If `str`, level name. If `int`, level index (0 corresponds to leaves).\n        parent_of: name of the population of which we want to get the parent in the tree.\n        children_of: name of the population of which we want to get the children populations in the tree.\n\n    Returns:\n        List of all populations that match the contraint, or one name if `parent_of` is not `None`.\n    \"\"\"\n\n    assert (\n        self.level_names\n    ), \"The provided knowledge table has no population hierarchical level. See the doc.\"\n\n    assert (\n        sum(arg is not None for arg in [level, parent_of, children_of]) == 1\n    ), \"One and exactly one argument has to be provided. Choose one among 'level', 'parent_of', and 'children_of'.\"\n\n    if level is not None:\n        assert (\n            isinstance(level, int) or level in self.level_names\n        ), f\"Level has to be one of [{', '.join(self.level_names)}]. Found {level}.\"\n\n        return list(set(self.table.index.get_level_values(level)))\n\n    name = parent_of or children_of\n    index = utils._get_pop_index(name, self.table)\n    where = self.table.index.get_level_values(index) == name\n\n    if children_of is not None:\n        if index == 0:\n            return []\n        return list(set(self.table.index.get_level_values(index - 1)[where]))\n\n    assert (\n        index &lt; self.table.index.nlevels - 1\n    ), \"Can not get parent of highest level population.\"\n\n    return self.table.index.get_level_values(index + 1)[where][0]\n</code></pre>"},{"location":"api/model/#scyan.Scyan.level_names","title":"<code>level_names</code>  <code>property</code>","text":"<p>All population hierarchical level names, if existing.</p>"},{"location":"api/model/#scyan.Scyan.save","title":"<code>save(path)</code>","text":"<p>Saves the Scyan model <code>state_dict</code> at the provided path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where the parameters will be saved. For instance, <code>'scyan_state_dict.pt'</code>.</p> required Source code in <code>scyan/model.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"Saves the Scyan model `state_dict` at the provided path.\n\n    Args:\n        path: Path where the parameters will be saved. For instance, `'scyan_state_dict.pt'`.\n    \"\"\"\n    torch.save(self.state_dict(), path)\n</code></pre>"},{"location":"api/model/#scyan.Scyan.load","title":"<code>load(path)</code>","text":"<p>Loads the Scyan model that was saved at the provided path. Note that the model has to be initialized with the same arguments.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; model = scyan.Scyan(adata, table) # initialize the model\n&gt;&gt;&gt; model.load('scyan_state_dict.pt')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where the parameters were saved, i.e. the argument of <code>model.save(path)</code>.</p> required Source code in <code>scyan/model.py</code> <pre><code>def load(self, path: str) -&gt; None:\n    \"\"\"Loads the Scyan model that was saved at the provided path. Note that the model has to be initialized with the same arguments.\n\n    !!! example\n        ```python\n        &gt;&gt;&gt; model = scyan.Scyan(adata, table) # initialize the model\n        &gt;&gt;&gt; model.load('scyan_state_dict.pt')\n        ```\n\n    Args:\n        path: Path where the parameters were saved, i.e. the argument of `model.save(path)`.\n    \"\"\"\n    self.load_state_dict(torch.load(path))\n    self._is_fitted = True\n</code></pre>"},{"location":"api/module/","title":"scyan.module.ScyanModule","text":"<p>             Bases: <code>LightningModule</code></p> <p>Core logic contained inside the main class Scyan. Do not use this class directly.</p> <p>Attributes:</p> Name Type Description <code>real_nvp</code> <code>RealNVP</code> <p>The Normalizing Flow (a RealNVP object)</p> <code>prior</code> <code>PriorDistribution</code> <p>The prior \\(U\\) (a PriorDistribution object)</p> <code>pi_logit</code> <code>Tensor</code> <p>Logits used to learn the population weights</p> Source code in <code>scyan/module/scyan_module.py</code> <pre><code>class ScyanModule(pl.LightningModule):\n    \"\"\"Core logic contained inside the main class [Scyan][scyan.Scyan]. Do not use this class directly.\n\n    Attributes:\n        real_nvp (RealNVP): The Normalizing Flow (a [RealNVP][scyan.module.RealNVP] object)\n        prior (PriorDistribution): The prior $U$ (a [PriorDistribution][scyan.module.PriorDistribution] object)\n        pi_logit (Tensor): Logits used to learn the population weights\n    \"\"\"\n\n    pi_logit_ratio: float = 100  # To learn pi logits faster\n\n    def __init__(\n        self,\n        rho: Tensor,\n        n_covariates: int,\n        is_continuum_marker: Tensor,\n        hidden_size: int,\n        n_hidden_layers: int,\n        n_layers: int,\n        prior_std: float,\n        temperature: float,\n    ):\n        \"\"\"\n        Args:\n            rho: Tensor $\\rho$ representing the knowledge table.\n            n_covariates: Number of covariates $M_c$ considered.\n            hidden_size: MLP (`s` and `t`) hidden size.\n            n_hidden_layers: Number of hidden layers for the MLP (`s` and `t`).\n            n_layers: Number of coupling layers.\n            prior_std: Standard deviation $\\sigma$ of the cell-specific random variable $H$.\n            temperature: Temperature to favour small populations.\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters(ignore=[\"rho\", \"n_covariates\", \"is_continuum_marker\"])\n\n        self.n_pops, self.n_markers = rho.shape\n\n        self.pi_logit = nn.Parameter(torch.zeros(self.n_pops))\n\n        self.real_nvp = RealNVP(\n            self.n_markers + n_covariates,\n            self.hparams.hidden_size,\n            self.n_markers,\n            self.hparams.n_hidden_layers,\n            self.hparams.n_layers,\n        )\n\n        self.prior = PriorDistribution(\n            rho, is_continuum_marker, self.hparams.prior_std, self.n_markers\n        )\n\n    def forward(self, x: Tensor, covariates: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        \"\"\"Forward implementation, going through the complete flow $f_{\\phi}$.\n\n        Args:\n            x: Inputs of size $(B, M)$.\n            covariates: Covariates of size $(B, M_c)$\n\n        Returns:\n            Tuple of (outputs, covariates, lod_det_jacobian sum)\n        \"\"\"\n        return self.real_nvp(x, covariates)\n\n    @torch.no_grad()\n    def inverse(self, u: Tensor, covariates: Tensor) -&gt; Tensor:\n        \"\"\"Go through the flow in reverse direction, i.e. $f_{\\phi}^{-1}$.\n\n        Args:\n            u: Latent expressions of size $(B, M)$.\n            covariates: Covariates of size $(B, M_c)$\n\n        Returns:\n            Outputs of size $(B, M)$.\n        \"\"\"\n        return self.real_nvp.inverse(u, covariates)\n\n    @property\n    def prior_z(self) -&gt; distributions.Distribution:\n        \"\"\"Population prior, i.e. $Categorical(\\pi)$.\n\n        Returns:\n            Distribution of the population index.\n        \"\"\"\n        return distributions.Categorical(self.pi)\n\n    @property\n    def log_pi(self) -&gt; Tensor:\n        \"\"\"Log population weights $log \\; \\pi$.\"\"\"\n        return torch.log_softmax(self.pi_logit_ratio * self.pi_logit, dim=0)\n\n    @property\n    def pi(self) -&gt; Tensor:\n        \"\"\"Population weights $\\pi$\"\"\"\n        return torch.exp(self.log_pi)\n\n    def log_pi_temperature(self, T: float) -&gt; Tensor:\n        \"\"\"Compute the log weights with temperature $log \\; \\pi^{(-T)}$\n\n        Args:\n            T: Temperature.\n\n        Returns:\n            Log weights with temperature.\n        \"\"\"\n        return torch.log_softmax(self.pi_logit_ratio * self.pi_logit / T, dim=0).detach()\n\n    @torch.no_grad()\n    def sample(\n        self,\n        n_samples: int,\n        covariates: Tensor,\n        z: Union[int, Tensor, None] = None,\n        return_z: bool = False,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Sampling cell-marker expressions.\n\n        Args:\n            n_samples: Number of cells to sample.\n            covariates: Tensor of covariates.\n            z: Either one population index or a Tensor of population indices. If None, sampling from all populations.\n            return_z: Whether to return the population Tensor.\n\n        Returns:\n            Sampled cells expressions and, if `return_z`, the populations associated to these cells.\n        \"\"\"\n        if z is None:\n            z = self.prior_z.sample((n_samples,))\n        elif isinstance(z, int):\n            z = torch.full((n_samples,), z)\n        elif isinstance(z, torch.Tensor):\n            z = z.to(int)\n        else:\n            raise ValueError(\n                f\"z has to be 'None', an 'int' or a 'torch.Tensor'. Found type {type(z)}.\"\n            )\n\n        u = self.prior.sample(z)\n        x = self.inverse(u, covariates)\n\n        return (x, z) if return_z else x\n\n    def compute_probabilities(\n        self, x: Tensor, covariates: Tensor, use_temp: bool = False\n    ) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        \"\"\"Compute probabilities used in the loss function.\n\n        Args:\n            x: Inputs of size $(B, M)$.\n            covariates: Covariates of size $(B, M_c)$.\n\n        Returns:\n            Log probabilities of size $(B, P)$, the log det jacobian and the latent expressions of size $(B, M)$.\n        \"\"\"\n        u, _, ldj_sum = self(x, covariates)\n\n        log_pi = (\n            self.log_pi_temperature(-self.hparams.temperature)\n            if use_temp\n            else self.log_pi\n        )\n\n        log_probs = self.prior.log_prob(u) + log_pi  # size N x P\n\n        return log_probs, ldj_sum, u\n\n    def kl(\n        self,\n        x: Tensor,\n        covariates: Tensor,\n        use_temp: bool,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Compute the module loss for one mini-batch.\n\n        Args:\n            x: Inputs of size $(B, M)$.\n            covariates: Covariates of size $(B, M_c)$.\n            use_temp: Whether to consider temperature is the KL term.\n\n        Returns:\n            The KL loss term.\n        \"\"\"\n        log_probs, ldj_sum, _ = self.compute_probabilities(x, covariates, use_temp)\n\n        return -(torch.logsumexp(log_probs, dim=1) + ldj_sum).mean()\n</code></pre>"},{"location":"api/module/#scyan.module.ScyanModule.log_pi","title":"<code>log_pi: Tensor</code>  <code>property</code>","text":"<p>Log population weights \\(log \\; \\pi\\).</p>"},{"location":"api/module/#scyan.module.ScyanModule.pi","title":"<code>pi: Tensor</code>  <code>property</code>","text":"<p>Population weights \\(\\pi\\)</p>"},{"location":"api/module/#scyan.module.ScyanModule.prior_z","title":"<code>prior_z: distributions.Distribution</code>  <code>property</code>","text":"<p>Population prior, i.e. \\(Categorical(\\pi)\\).</p> <p>Returns:</p> Type Description <code>Distribution</code> <p>Distribution of the population index.</p>"},{"location":"api/module/#scyan.module.ScyanModule.__init__","title":"<code>__init__(rho, n_covariates, is_continuum_marker, hidden_size, n_hidden_layers, n_layers, prior_std, temperature)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>rho</code> <code>Tensor</code> <p>Tensor $ ho$ representing the knowledge table.</p> required <code>n_covariates</code> <code>int</code> <p>Number of covariates \\(M_c\\) considered.</p> required <code>hidden_size</code> <code>int</code> <p>MLP (<code>s</code> and <code>t</code>) hidden size.</p> required <code>n_hidden_layers</code> <code>int</code> <p>Number of hidden layers for the MLP (<code>s</code> and <code>t</code>).</p> required <code>n_layers</code> <code>int</code> <p>Number of coupling layers.</p> required <code>prior_std</code> <code>float</code> <p>Standard deviation \\(\\sigma\\) of the cell-specific random variable \\(H\\).</p> required <code>temperature</code> <code>float</code> <p>Temperature to favour small populations.</p> required Source code in <code>scyan/module/scyan_module.py</code> <pre><code>def __init__(\n    self,\n    rho: Tensor,\n    n_covariates: int,\n    is_continuum_marker: Tensor,\n    hidden_size: int,\n    n_hidden_layers: int,\n    n_layers: int,\n    prior_std: float,\n    temperature: float,\n):\n    \"\"\"\n    Args:\n        rho: Tensor $\\rho$ representing the knowledge table.\n        n_covariates: Number of covariates $M_c$ considered.\n        hidden_size: MLP (`s` and `t`) hidden size.\n        n_hidden_layers: Number of hidden layers for the MLP (`s` and `t`).\n        n_layers: Number of coupling layers.\n        prior_std: Standard deviation $\\sigma$ of the cell-specific random variable $H$.\n        temperature: Temperature to favour small populations.\n    \"\"\"\n    super().__init__()\n    self.save_hyperparameters(ignore=[\"rho\", \"n_covariates\", \"is_continuum_marker\"])\n\n    self.n_pops, self.n_markers = rho.shape\n\n    self.pi_logit = nn.Parameter(torch.zeros(self.n_pops))\n\n    self.real_nvp = RealNVP(\n        self.n_markers + n_covariates,\n        self.hparams.hidden_size,\n        self.n_markers,\n        self.hparams.n_hidden_layers,\n        self.hparams.n_layers,\n    )\n\n    self.prior = PriorDistribution(\n        rho, is_continuum_marker, self.hparams.prior_std, self.n_markers\n    )\n</code></pre>"},{"location":"api/module/#scyan.module.ScyanModule.compute_probabilities","title":"<code>compute_probabilities(x, covariates, use_temp=False)</code>","text":"<p>Compute probabilities used in the loss function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Inputs of size \\((B, M)\\).</p> required <code>covariates</code> <code>Tensor</code> <p>Covariates of size \\((B, M_c)\\).</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>Log probabilities of size \\((B, P)\\), the log det jacobian and the latent expressions of size \\((B, M)\\).</p> Source code in <code>scyan/module/scyan_module.py</code> <pre><code>def compute_probabilities(\n    self, x: Tensor, covariates: Tensor, use_temp: bool = False\n) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Compute probabilities used in the loss function.\n\n    Args:\n        x: Inputs of size $(B, M)$.\n        covariates: Covariates of size $(B, M_c)$.\n\n    Returns:\n        Log probabilities of size $(B, P)$, the log det jacobian and the latent expressions of size $(B, M)$.\n    \"\"\"\n    u, _, ldj_sum = self(x, covariates)\n\n    log_pi = (\n        self.log_pi_temperature(-self.hparams.temperature)\n        if use_temp\n        else self.log_pi\n    )\n\n    log_probs = self.prior.log_prob(u) + log_pi  # size N x P\n\n    return log_probs, ldj_sum, u\n</code></pre>"},{"location":"api/module/#scyan.module.ScyanModule.forward","title":"<code>forward(x, covariates)</code>","text":"<p>Forward implementation, going through the complete flow \\(f_{\\phi}\\).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Inputs of size \\((B, M)\\).</p> required <code>covariates</code> <code>Tensor</code> <p>Covariates of size \\((B, M_c)\\)</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>Tuple of (outputs, covariates, lod_det_jacobian sum)</p> Source code in <code>scyan/module/scyan_module.py</code> <pre><code>def forward(self, x: Tensor, covariates: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Forward implementation, going through the complete flow $f_{\\phi}$.\n\n    Args:\n        x: Inputs of size $(B, M)$.\n        covariates: Covariates of size $(B, M_c)$\n\n    Returns:\n        Tuple of (outputs, covariates, lod_det_jacobian sum)\n    \"\"\"\n    return self.real_nvp(x, covariates)\n</code></pre>"},{"location":"api/module/#scyan.module.ScyanModule.inverse","title":"<code>inverse(u, covariates)</code>","text":"<p>Go through the flow in reverse direction, i.e. \\(f_{\\phi}^{-1}\\).</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>Tensor</code> <p>Latent expressions of size \\((B, M)\\).</p> required <code>covariates</code> <code>Tensor</code> <p>Covariates of size \\((B, M_c)\\)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Outputs of size \\((B, M)\\).</p> Source code in <code>scyan/module/scyan_module.py</code> <pre><code>@torch.no_grad()\ndef inverse(self, u: Tensor, covariates: Tensor) -&gt; Tensor:\n    \"\"\"Go through the flow in reverse direction, i.e. $f_{\\phi}^{-1}$.\n\n    Args:\n        u: Latent expressions of size $(B, M)$.\n        covariates: Covariates of size $(B, M_c)$\n\n    Returns:\n        Outputs of size $(B, M)$.\n    \"\"\"\n    return self.real_nvp.inverse(u, covariates)\n</code></pre>"},{"location":"api/module/#scyan.module.ScyanModule.kl","title":"<code>kl(x, covariates, use_temp)</code>","text":"<p>Compute the module loss for one mini-batch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Inputs of size \\((B, M)\\).</p> required <code>covariates</code> <code>Tensor</code> <p>Covariates of size \\((B, M_c)\\).</p> required <code>use_temp</code> <code>bool</code> <p>Whether to consider temperature is the KL term.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>The KL loss term.</p> Source code in <code>scyan/module/scyan_module.py</code> <pre><code>def kl(\n    self,\n    x: Tensor,\n    covariates: Tensor,\n    use_temp: bool,\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Compute the module loss for one mini-batch.\n\n    Args:\n        x: Inputs of size $(B, M)$.\n        covariates: Covariates of size $(B, M_c)$.\n        use_temp: Whether to consider temperature is the KL term.\n\n    Returns:\n        The KL loss term.\n    \"\"\"\n    log_probs, ldj_sum, _ = self.compute_probabilities(x, covariates, use_temp)\n\n    return -(torch.logsumexp(log_probs, dim=1) + ldj_sum).mean()\n</code></pre>"},{"location":"api/module/#scyan.module.ScyanModule.log_pi_temperature","title":"<code>log_pi_temperature(T)</code>","text":"<p>Compute the log weights with temperature \\(log \\; \\pi^{(-T)}\\)</p> <p>Parameters:</p> Name Type Description Default <code>T</code> <code>float</code> <p>Temperature.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Log weights with temperature.</p> Source code in <code>scyan/module/scyan_module.py</code> <pre><code>def log_pi_temperature(self, T: float) -&gt; Tensor:\n    \"\"\"Compute the log weights with temperature $log \\; \\pi^{(-T)}$\n\n    Args:\n        T: Temperature.\n\n    Returns:\n        Log weights with temperature.\n    \"\"\"\n    return torch.log_softmax(self.pi_logit_ratio * self.pi_logit / T, dim=0).detach()\n</code></pre>"},{"location":"api/module/#scyan.module.ScyanModule.sample","title":"<code>sample(n_samples, covariates, z=None, return_z=False)</code>","text":"<p>Sampling cell-marker expressions.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of cells to sample.</p> required <code>covariates</code> <code>Tensor</code> <p>Tensor of covariates.</p> required <code>z</code> <code>Union[int, Tensor, None]</code> <p>Either one population index or a Tensor of population indices. If None, sampling from all populations.</p> <code>None</code> <code>return_z</code> <code>bool</code> <p>Whether to return the population Tensor.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Sampled cells expressions and, if <code>return_z</code>, the populations associated to these cells.</p> Source code in <code>scyan/module/scyan_module.py</code> <pre><code>@torch.no_grad()\ndef sample(\n    self,\n    n_samples: int,\n    covariates: Tensor,\n    z: Union[int, Tensor, None] = None,\n    return_z: bool = False,\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Sampling cell-marker expressions.\n\n    Args:\n        n_samples: Number of cells to sample.\n        covariates: Tensor of covariates.\n        z: Either one population index or a Tensor of population indices. If None, sampling from all populations.\n        return_z: Whether to return the population Tensor.\n\n    Returns:\n        Sampled cells expressions and, if `return_z`, the populations associated to these cells.\n    \"\"\"\n    if z is None:\n        z = self.prior_z.sample((n_samples,))\n    elif isinstance(z, int):\n        z = torch.full((n_samples,), z)\n    elif isinstance(z, torch.Tensor):\n        z = z.to(int)\n    else:\n        raise ValueError(\n            f\"z has to be 'None', an 'int' or a 'torch.Tensor'. Found type {type(z)}.\"\n        )\n\n    u = self.prior.sample(z)\n    x = self.inverse(u, covariates)\n\n    return (x, z) if return_z else x\n</code></pre>"},{"location":"api/plots/","title":"Plots","text":""},{"location":"api/plots/#scyan.plot.umap","title":"<code>scyan.plot.umap(adata, color=None, vmax='p95', vmin='p05', show=True, **scanpy_kwargs)</code>","text":"<p>Plot a UMAP using scanpy.</p> <p>Note</p> <p>If you trained your UMAP with scyan.tools.umap on a subset of cells, it will only display the desired subset of cells.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required <code>color</code> <code>Union[str, List[str]]</code> <p>Marker(s) or <code>obs</code> name(s) to color. It can be either just one string, or a list (it will plot one UMAP per element in the list).</p> <code>None</code> <code>vmax</code> <code>Union[str, float]</code> <p><code>scanpy.pl.umap</code> vmax argument.</p> <code>'p95'</code> <code>vmin</code> <code>Union[str, float]</code> <p><code>scanpy.pl.umap</code> vmin argument.</p> <code>'p05'</code> <code>show</code> <code>bool</code> <p>Whether or not to display the figure.</p> <code>True</code> <code>**scanpy_kwargs</code> <code>int</code> <p>Optional kwargs provided to <code>scanpy.pl.umap</code>.</p> <code>{}</code> Source code in <code>scyan/plot/dot.py</code> <pre><code>@plot_decorator(adata=True)\ndef umap(\n    adata: AnnData,\n    color: Union[str, List[str]] = None,\n    vmax: Union[str, float] = \"p95\",\n    vmin: Union[str, float] = \"p05\",\n    show: bool = True,\n    **scanpy_kwargs: int,\n):\n    \"\"\"Plot a UMAP using scanpy.\n\n    !!! note\n        If you trained your UMAP with [scyan.tools.umap][] on a subset of cells, it will only display the desired subset of cells.\n\n    Args:\n        adata: An `AnnData` object.\n        color: Marker(s) or `obs` name(s) to color. It can be either just one string, or a list (it will plot one UMAP per element in the list).\n        vmax: `scanpy.pl.umap` vmax argument.\n        vmin: `scanpy.pl.umap` vmin argument.\n        show: Whether or not to display the figure.\n        **scanpy_kwargs: Optional kwargs provided to `scanpy.pl.umap`.\n    \"\"\"\n    assert isinstance(\n        adata, AnnData\n    ), f\"umap first argument has to be an AnnData object. Received type {type(adata)}.\"\n\n    has_umap = _has_umap(adata)\n    if not has_umap.all():\n        adata = adata[has_umap]\n\n    if color is None:\n        return scanpy_pl_umap(adata, **scanpy_kwargs)\n\n    return scanpy_pl_umap(adata, color=color, vmax=vmax, vmin=vmin, **scanpy_kwargs)\n</code></pre>"},{"location":"api/plots/#scyan.plot.scatter","title":"<code>scyan.plot.scatter(adata, population, markers=None, n_markers=3, key='scyan_pop', max_obs=2000, s=1.0, show=True)</code>","text":"<p>Display marker expressions on 2D scatter plots with colors per population. One scatter plot is displayed for each pair of markers.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required <code>population</code> <code>Union[str, List[str], None]</code> <p>One population, or a list of population to be colored, or <code>None</code>. If not <code>None</code>, the population name(s) has to be in <code>adata.obs[key]</code>.</p> required <code>markers</code> <code>Optional[List[str]]</code> <p>List of markers to plot. If <code>None</code>, the list is chosen automatically.</p> <code>None</code> <code>n_markers</code> <code>Optional[int]</code> <p>Number of markers to choose automatically if <code>markers is None</code>.</p> <code>3</code> <code>key</code> <code>str</code> <p>Key to look for populations in <code>adata.obs</code>. By default, uses the model predictions.</p> <code>'scyan_pop'</code> <code>max_obs</code> <code>int</code> <p>Maximum number of cells per population to be displayed. If population is None, then this number is multiplied by 10.</p> <code>2000</code> <code>s</code> <code>float</code> <p>Dot marker size.</p> <code>1.0</code> <code>show</code> <code>bool</code> <p>Whether or not to display the figure.</p> <code>True</code> Source code in <code>scyan/plot/dot.py</code> <pre><code>@plot_decorator(adata=True)\n@check_population(return_list=True)\ndef scatter(\n    adata: AnnData,\n    population: Union[str, List[str], None],\n    markers: Optional[List[str]] = None,\n    n_markers: Optional[int] = 3,\n    key: str = \"scyan_pop\",\n    max_obs: int = 2000,\n    s: float = 1.0,\n    show: bool = True,\n) -&gt; None:\n    \"\"\"Display marker expressions on 2D scatter plots with colors per population.\n    One scatter plot is displayed for each pair of markers.\n\n    Args:\n        adata: An `AnnData` object.\n        population: One population, or a list of population to be colored, or `None`. If not `None`, the population name(s) has to be in `adata.obs[key]`.\n        markers: List of markers to plot. If `None`, the list is chosen automatically.\n        n_markers: Number of markers to choose automatically if `markers is None`.\n        key: Key to look for populations in `adata.obs`. By default, uses the model predictions.\n        max_obs: Maximum number of cells per population to be displayed. If population is None, then this number is multiplied by 10.\n        s: Dot marker size.\n        show: Whether or not to display the figure.\n    \"\"\"\n    markers = select_markers(adata, markers, n_markers, key, population)\n\n    if population is None:\n        indices = _get_subset_indices(adata.n_obs, max_obs * 10)\n        data = adata[indices, markers].to_df()\n        g = sns.PairGrid(data, corner=True)\n        g.map_offdiag(sns.scatterplot, s=s)\n        g.map_diag(sns.histplot)\n        g.add_legend()\n        return\n\n    data = adata[:, markers].to_df()\n    keys = adata.obs[key].astype(str)\n    data[\"Population\"] = np.where(~np.isin(keys, population), \"Others\", keys)\n\n    pops = list(population) + [\"Others\"]\n    if max_obs is not None:\n        groups = data.groupby(\"Population\").groups\n        data = data.loc[[i for pop in pops[::-1] for i in _subset(groups[pop], max_obs)]]\n\n    palette = get_palette_others(data, \"Population\")\n\n    g = sns.PairGrid(data, hue=\"Population\", corner=True, palette=palette, hue_order=pops)\n    g.map_offdiag(sns.scatterplot, s=s)\n    g.map_diag(sns.histplot)\n    g.add_legend()\n</code></pre>"},{"location":"api/plots/#scyan.plot.probs_per_marker","title":"<code>scyan.plot.probs_per_marker(model, population, key='scyan_pop', prob_name='Prob', vmin_threshold=-100, figsize=(10, 6), show=True)</code>","text":"<p>Interpretability tool: get a group of cells and plot a heatmap of marker probabilities for each population.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Scyan</code> <p>Scyan model.</p> required <code>population</code> <code>str</code> <p>Name of one population to interpret. To be valid, the population name has to be in <code>adata.obs[key]</code>.</p> required <code>key</code> <code>str</code> <p>Key to look for population in <code>adata.obs</code>. By default, uses the model predictions.</p> <code>'scyan_pop'</code> <code>prob_name</code> <code>str</code> <p>Name to display on the plot.</p> <code>'Prob'</code> <code>vmin_threshold</code> <code>int</code> <p>Minimum threshold for the heatmap colorbar.</p> <code>-100</code> <code>figsize</code> <code>Tuple[float]</code> <p>Pair <code>(width, height)</code> indicating the size of the figure.</p> <code>(10, 6)</code> <code>show</code> <code>bool</code> <p>Whether or not to display the figure.</p> <code>True</code> Source code in <code>scyan/plot/heatmap.py</code> <pre><code>@torch.no_grad()\n@plot_decorator()\n@check_population(one=True)\ndef probs_per_marker(\n    model: Scyan,\n    population: str,\n    key: str = \"scyan_pop\",\n    prob_name: str = \"Prob\",\n    vmin_threshold: int = -100,\n    figsize: Tuple[float] = (10, 6),\n    show: bool = True,\n):\n    \"\"\"Interpretability tool: get a group of cells and plot a heatmap of marker probabilities for each population.\n\n    Args:\n        model: Scyan model.\n        population: Name of one population to interpret. To be valid, the population name has to be in `adata.obs[key]`.\n        key: Key to look for population in `adata.obs`. By default, uses the model predictions.\n        prob_name: Name to display on the plot.\n        vmin_threshold: Minimum threshold for the heatmap colorbar.\n        figsize: Pair `(width, height)` indicating the size of the figure.\n        show: Whether or not to display the figure.\n    \"\"\"\n    u = model(model.adata.obs[key] == population)\n\n    log_probs = model.module.prior.log_prob_per_marker(u)\n    mean_log_probs = log_probs.mean(dim=0).numpy(force=True)\n\n    df_probs = pd.DataFrame(\n        mean_log_probs,\n        columns=model.var_names,\n        index=model.pop_names,\n    )\n    df_probs = df_probs.reindex(\n        df_probs.mean().sort_values(ascending=False).index, axis=1\n    )\n    means = df_probs.mean(axis=1)\n    means = means / means.min() * df_probs.values.min()\n    df_probs.insert(0, prob_name, means)\n    df_probs.insert(1, \" \", np.nan)\n    df_probs.sort_values(by=prob_name, inplace=True, ascending=False)\n\n    plt.figure(figsize=figsize)\n    sns.heatmap(df_probs, cmap=\"magma\", vmin=max(vmin_threshold, mean_log_probs.min()))\n    plt.title(\"Log probabilities per marker for each population\")\n</code></pre>"},{"location":"api/plots/#scyan.plot.pop_percentage","title":"<code>scyan.plot.pop_percentage(adata, groupby=None, key='scyan_pop', figsize=None, dendogram=False, show=True)</code>","text":"<p>Show populations percentages. Depending on <code>groupby</code>, this is either done globally, or as a stacked bar plot (one bar for each group).</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required <code>groupby</code> <code>Union[str, List[str], None]</code> <p>Key(s) of <code>adata.obs</code> used to create groups (e.g. the patient ID).</p> <code>None</code> <code>key</code> <code>str</code> <p>Key of <code>adata.obs</code> containing the population names (or the values) for which percentage will be displayed.</p> <code>'scyan_pop'</code> <code>figsize</code> <code>tuple[float, float]</code> <p>matplotlib figure size.</p> <code>None</code> <code>dendogram</code> <code>bool</code> <p>If True, the groups are sorted based on a dendogram clustering.</p> <code>False</code> <code>show</code> <code>bool</code> <p>Whether or not to display the figure.</p> <code>True</code> Source code in <code>scyan/plot/ratios.py</code> <pre><code>@plot_decorator(adata=True)\ndef pop_percentage(\n    adata: AnnData,\n    groupby: Union[str, List[str], None] = None,\n    key: str = \"scyan_pop\",\n    figsize: tuple[float, float] = None,\n    dendogram: bool = False,\n    show: bool = True,\n):\n    \"\"\"Show populations percentages. Depending on `groupby`, this is either done globally, or as a stacked bar plot (one bar for each group).\n\n    Args:\n        adata: An `AnnData` object.\n        groupby: Key(s) of `adata.obs` used to create groups (e.g. the patient ID).\n        key: Key of `adata.obs` containing the population names (or the values) for which percentage will be displayed.\n        figsize: matplotlib figure size.\n        dendogram: If True, the groups are sorted based on a dendogram clustering.\n        show: Whether or not to display the figure.\n    \"\"\"\n    if groupby is None:\n        adata.obs[key].value_counts(normalize=True).mul(100).plot.bar(figsize=figsize)\n    else:\n        df = adata.obs.groupby(groupby)[key].value_counts(normalize=True)\n        df = df.mul(100).unstack()\n\n        if dendogram:\n            Z = hierarchy.linkage(df.values, method=\"ward\")\n            dendrogram = hierarchy.dendrogram(Z, no_plot=True)\n            df = df.iloc[dendrogram[\"leaves\"]]\n\n        df.plot.bar(stacked=True, figsize=figsize)\n        plt.legend(\n            bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0, frameon=False\n        )\n\n    plt.ylabel(f\"{key} percentage\")\n    sns.despine(offset=10, trim=True)\n    plt.xticks(rotation=90)\n</code></pre>"},{"location":"api/plots/#scyan.plot.pop_dynamics","title":"<code>scyan.plot.pop_dynamics(adata, time_key, groupby=None, key='scyan_pop', among=None, n_cols=4, size_mul=None, figsize=None, show=True)</code>","text":"<p>Show populations percentages dynamics for different timepoints. Depending on <code>groupby</code>, this is either done globally, or for each group.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required <code>time_key</code> <code>str</code> <p>Key of <code>adata.obs</code> containing the timepoints. We recommend to use a categorical series (to use the right timepoint order).</p> required <code>groupby</code> <code>Union[str, List[str], None]</code> <p>Key(s) of <code>adata.obs</code> used to create groups (e.g. the patient ID).</p> <code>None</code> <code>key</code> <code>str</code> <p>Key of <code>adata.obs</code> containing the population names (or the values) for which dynamics will be displayed.</p> <code>'scyan_pop'</code> <code>among</code> <code>str</code> <p>Key of <code>adata.obs</code> containing the parent population name. See scyan.tools.cell_type_ratios.</p> <code>None</code> <code>n_cols</code> <code>int</code> <p>Number of figures per row.</p> <code>4</code> <code>size_mul</code> <code>Optional[float]</code> <p>Dot size multiplication factor. By default, it is computed using the population counts.</p> <code>None</code> <code>figsize</code> <code>tuple[float, float]</code> <p>matplotlib figure size.</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether or not to display the figure.</p> <code>True</code> Source code in <code>scyan/plot/ratios.py</code> <pre><code>@plot_decorator(adata=True)\ndef pop_dynamics(\n    adata: AnnData,\n    time_key: str,\n    groupby: Union[str, List[str], None] = None,\n    key: str = \"scyan_pop\",\n    among: str = None,\n    n_cols: int = 4,\n    size_mul: Optional[float] = None,\n    figsize: tuple[float, float] = None,\n    show: bool = True,\n):\n    \"\"\"Show populations percentages dynamics for different timepoints. Depending on `groupby`, this is either done globally, or for each group.\n\n    Args:\n        adata: An `AnnData` object.\n        time_key: Key of `adata.obs` containing the timepoints. We recommend to use a categorical series (to use the right timepoint order).\n        groupby: Key(s) of `adata.obs` used to create groups (e.g. the patient ID).\n        key: Key of `adata.obs` containing the population names (or the values) for which dynamics will be displayed.\n        among: Key of `adata.obs` containing the parent population name. See [scyan.tools.cell_type_ratios][].\n        n_cols: Number of figures per row.\n        size_mul: Dot size multiplication factor. By default, it is computed using the population counts.\n        figsize: matplotlib figure size.\n        show: Whether or not to display the figure.\n    \"\"\"\n    if not adata.obs[time_key].dtype.name == \"category\":\n        log.info(f\"Converting adata.obs['{time_key}'] to categorical\")\n        adata.obs[time_key] = adata.obs[time_key].astype(\"category\")\n\n    if groupby is None:\n        groupby = [time_key]\n    else:\n        groupby = ([groupby] if isinstance(groupby, str) else groupby) + [time_key]\n\n    df = cell_type_ratios(adata, groupby=groupby, key=key, normalize=\"%\", among=among)\n    df.index = df.index.set_levels(df.index.levels[-1].codes, level=-1)\n    df_log_count = np.log(\n        1 + cell_type_ratios(adata, groupby=groupby, key=key, normalize=False)\n    )\n\n    n_pops = df.shape[1]\n    n_rows = ceil(n_pops / n_cols)\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize or (12, 3 * n_rows))\n\n    if size_mul is None:\n        size_mul = 40 / df_log_count.mean().mean()\n\n    axes = axes.flatten()\n\n    if len(groupby) == 1:\n        for i, pop in enumerate(df.columns):\n            axes[i].plot(df.index, df.iloc[:, i])\n            axes[i].scatter(df.index, df.iloc[:, i], s=df_log_count.iloc[:, i] * size_mul)\n    else:\n        drop_levels = list(range(len(groupby) - 1))\n\n        for group, group_df in df.groupby(level=drop_levels):\n            label = \" \".join(map(str, group)) if isinstance(group, tuple) else group\n            group_df = group_df.droplevel(drop_levels)\n            group_df_log_count = df_log_count.loc[group]\n\n            for i, pop in enumerate(group_df.columns):\n                axes[i].plot(group_df.index, group_df.iloc[:, i], label=label)\n                axes[i].scatter(\n                    group_df.index,\n                    group_df.iloc[:, i],\n                    s=(group_df_log_count.iloc[:, i] * size_mul).clip(0, 100),\n                )\n\n        fig.legend(\n            *axes[0].get_legend_handles_labels(),\n            bbox_to_anchor=(1.04, 0.55),\n            loc=\"lower left\",\n            borderaxespad=0,\n            frameon=False,\n        )\n\n    timepoints = adata.obs[time_key].cat.categories\n    for i, pop in enumerate(df.columns):\n        axes[i].set_ylabel(pop)\n        axes[i].set_xlabel(time_key)\n        axes[i].set_xticks(range(len(timepoints)), timepoints)\n\n    sizes = [1, 10, 25, 40, 60]\n    legend_markers = [\n        Line2D([0], [0], linewidth=0, marker=\"o\", markersize=np.sqrt(s)) for s in sizes\n    ]\n    legend2 = fig.legend(\n        legend_markers,\n        [f\" {ceil(np.exp(s / size_mul) - 1):,} cells\" for s in sizes],\n        bbox_to_anchor=(1.04, 0.45),\n        loc=\"upper left\",\n        borderaxespad=0,\n        frameon=False,\n    )\n    fig.add_artist(legend2)\n\n    for ax in axes[n_pops:]:\n        ax.set_axis_off()\n\n    sns.despine(offset=10, trim=True)\n    plt.tight_layout()\n</code></pre>"},{"location":"api/plots/#scyan.plot.pop_expressions","title":"<code>scyan.plot.pop_expressions(model, population, key='scyan_pop', max_value=1.5, num_pieces=100, radius=0.05, figsize=(2, 6), show=True)</code>","text":"<p>Plot latent cell expressions for one population. Contrary to <code>scyan.plot.pops_expressions</code>, in displays expressions on a vertical bar, from <code>Neg</code> to <code>Pos</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Scyan</code> <p>Scyan model.</p> required <code>population</code> <code>str</code> <p>Name of one population to interpret. To be valid, the population name has to be in <code>adata.obs[key]</code>.</p> required <code>key</code> <code>str</code> <p>Key to look for populations in <code>adata.obs</code>. By default, uses the model predictions.</p> <code>'scyan_pop'</code> <code>max_value</code> <code>float</code> <p>Maximum absolute latent value.</p> <code>1.5</code> <code>num_pieces</code> <code>int</code> <p>Number of pieces to display the colorbar.</p> <code>100</code> <code>radius</code> <code>float</code> <p>Radius used to chunk the colorbar. Increase this value if multiple names overlap.</p> <code>0.05</code> <code>figsize</code> <code>Tuple[float]</code> <p>Pair <code>(width, height)</code> indicating the size of the figure.</p> <code>(2, 6)</code> <code>show</code> <code>bool</code> <p>Whether or not to display the figure.</p> <code>True</code> Source code in <code>scyan/plot/expressions.py</code> <pre><code>@torch.no_grad()\n@plot_decorator()\n@check_population(one=True)\ndef pop_expressions(\n    model: Scyan,\n    population: str,\n    key: str = \"scyan_pop\",\n    max_value: float = 1.5,\n    num_pieces: int = 100,\n    radius: float = 0.05,\n    figsize: Tuple[float] = (2, 6),\n    show: bool = True,\n):\n    \"\"\"Plot latent cell expressions for one population. Contrary to `scyan.plot.pops_expressions`, in displays expressions on a vertical bar, from `Neg` to `Pos`.\n\n    Args:\n        model: Scyan model.\n        population: Name of one population to interpret. To be valid, the population name has to be in `adata.obs[key]`.\n        key: Key to look for populations in `adata.obs`. By default, uses the model predictions.\n        max_value: Maximum absolute latent value.\n        num_pieces: Number of pieces to display the colorbar.\n        radius: Radius used to chunk the colorbar. Increase this value if multiple names overlap.\n        figsize: Pair `(width, height)` indicating the size of the figure.\n        show: Whether or not to display the figure.\n    \"\"\"\n    condition = model.adata.obs[key] == population\n    u_mean = model(condition).mean(dim=0)\n    values = u_mean.numpy(force=True).clip(-max_value, max_value)\n\n    y = np.linspace(-max_value, max_value, num_pieces + 1)\n    cmap = plt.get_cmap(\"RdBu\")\n    y_cmap = norm.pdf(np.abs(y) - 1, scale=model.hparams.prior_std)\n    y_cmap = y_cmap - y_cmap.min()\n    y_cmap = 0.5 - np.sign(y) * (y_cmap / y_cmap.max() / 2)\n    colors = cmap(y_cmap).clip(0, 0.8)\n\n    plt.figure(figsize=figsize, dpi=100)\n    plt.vlines(np.zeros(num_pieces), y[:-1], y[1:], colors=colors, linewidth=5)\n    plt.annotate(\"Pos\", (-0.7, 1), fontsize=15)\n    plt.annotate(\"Neg\", (-0.7, -1), fontsize=15)\n\n    for v in np.arange(-max_value, max_value, 2 * radius):\n        labels = [\n            label\n            for value, label in zip(values, model.var_names)\n            if abs(v - value) &lt; radius\n        ]\n        if labels:\n            plt.plot([0, 0.1], [v, v], \"k\")\n            plt.annotate(\", \".join(labels), (0.2, v - 0.03))\n\n    plt.xlim([-1, 1])\n    plt.axis(\"off\")\n</code></pre>"},{"location":"api/plots/#scyan.plot.pops_expressions","title":"<code>scyan.plot.pops_expressions(model, latent=True, key='scyan_pop', n_cells=200000, vmax=1.2, vmin=-1.2, cmap=None, figsize=(10, 6), show=True)</code>","text":"<p>Heatmap that shows (latent or standardized) cell expressions for all populations.</p> <p>Note</p> <p>If using the latent space, it will only show the marker you provided to Scyan. Else, it shows every marker of the panel.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Scyan</code> <p>Scyan model.</p> required <code>latent</code> <code>bool</code> <p>If <code>True</code>, displays Scyan's latent expressions, else just the standardized expressions.</p> <code>True</code> <code>key</code> <code>str</code> <p>Key to look for populations in <code>adata.obs</code>. By default, uses the model predictions.</p> <code>'scyan_pop'</code> <code>n_cells</code> <code>Optional[int]</code> <p>Number of cells to be considered for the heatmap (to accelerate it when \\(N\\) is very high). If <code>None</code>, consider all cells.</p> <code>200000</code> <code>vmax</code> <code>float</code> <p>Maximum value on the heatmap.</p> <code>1.2</code> <code>vmax</code> <code>float</code> <p>Minimum value on the heatmap.</p> <code>1.2</code> <code>cmap</code> <code>Optional[str]</code> <p>Colormap name. By default, uses <code>\"coolwarm\"</code> if <code>latent</code>, else <code>\"viridis\"</code>.</p> <code>None</code> <code>figsize</code> <code>Tuple[float]</code> <p>Pair <code>(width, height)</code> indicating the size of the figure.</p> <code>(10, 6)</code> <code>show</code> <code>bool</code> <p>Whether or not to display the figure.</p> <code>True</code> Source code in <code>scyan/plot/expressions.py</code> <pre><code>@torch.no_grad()\n@plot_decorator()\ndef pops_expressions(\n    model: Scyan,\n    latent: bool = True,\n    key: str = \"scyan_pop\",\n    n_cells: Optional[int] = 200_000,\n    vmax: float = 1.2,\n    vmin: float = -1.2,\n    cmap: Optional[str] = None,\n    figsize: Tuple[float] = (10, 6),\n    show: bool = True,\n):\n    \"\"\"Heatmap that shows (latent or standardized) cell expressions for all populations.\n\n    !!! note\n        If using the latent space, it will only show the marker you provided to Scyan. Else, it shows every marker of the panel.\n\n    Args:\n        model: Scyan model.\n        latent: If `True`, displays Scyan's latent expressions, else just the standardized expressions.\n        key: Key to look for populations in `adata.obs`. By default, uses the model predictions.\n        n_cells: Number of cells to be considered for the heatmap (to accelerate it when $N$ is very high). If `None`, consider all cells.\n        vmax: Maximum value on the heatmap.\n        vmax: Minimum value on the heatmap.\n        cmap: Colormap name. By default, uses `\"coolwarm\"` if `latent`, else `\"viridis\"`.\n        figsize: Pair `(width, height)` indicating the size of the figure.\n        show: Whether or not to display the figure.\n    \"\"\"\n    not_na = ~model.adata.obs[key].isna()\n    indices = _get_subset_indices(not_na.sum(), n_cells)\n    indices = np.where(not_na)[0][indices]\n\n    x = model(indices).numpy(force=True) if latent else model.adata[indices].X\n    columns = model.var_names if latent else model.adata.var_names\n\n    df = pd.DataFrame(x, columns=columns, index=model.adata.obs.index[indices])\n    df[\"Population\"] = model.adata[indices].obs[key]\n\n    if cmap is None:\n        cmap = \"coolwarm\" if latent else \"viridis\"\n\n    plt.figure(figsize=figsize)\n    sns.heatmap(df.groupby(\"Population\").mean(), vmax=vmax, vmin=vmin, cmap=cmap)\n    plt.title(f\"{'Latent' if latent else 'Standardized'} expressions grouped by {key}\")\n</code></pre>"},{"location":"api/plots/#scyan.plot.kde","title":"<code>scyan.plot.kde(adata, population, markers=None, key='scyan_pop', n_markers=3, n_cells=100000, ncols=2, var_name='Marker', value_name='Expression', show=True)</code>","text":"<p>Plot Kernel-Density-Estimation for each provided population and for multiple markers.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required <code>population</code> <code>Union[str, List[str], None]</code> <p>One population, or a list of population to be analyzed, or <code>None</code>. If not <code>None</code>, the population name(s) has to be in <code>adata.obs[key]</code>.</p> required <code>markers</code> <code>Optional[List[str]]</code> <p>List of markers to plot. If <code>None</code>, the list is chosen automatically.</p> <code>None</code> <code>key</code> <code>str</code> <p>Key to look for populations in <code>adata.obs</code>. By default, uses the model predictions.</p> <code>'scyan_pop'</code> <code>n_markers</code> <code>Optional[int]</code> <p>Number of markers to choose automatically if <code>markers is None</code>.</p> <code>3</code> <code>n_cells</code> <code>Optional[int]</code> <p>Number of cells to be considered for the heatmap (to accelerate it when \\(N\\) is very high). If <code>None</code>, consider all cells.</p> <code>100000</code> <code>ncols</code> <code>int</code> <p>Number of figures per row.</p> <code>2</code> <code>var_name</code> <code>str</code> <p>Name displayed on the graphs.</p> <code>'Marker'</code> <code>value_name</code> <code>str</code> <p>Name displayed on the graphs.</p> <code>'Expression'</code> <code>show</code> <code>bool</code> <p>Whether or not to display the figure.</p> <code>True</code> Source code in <code>scyan/plot/density.py</code> <pre><code>@plot_decorator(adata=True)\n@check_population(return_list=True)\ndef kde(\n    adata: AnnData,\n    population: Union[str, List[str], None],\n    markers: Optional[List[str]] = None,\n    key: str = \"scyan_pop\",\n    n_markers: Optional[int] = 3,\n    n_cells: Optional[int] = 100_000,\n    ncols: int = 2,\n    var_name: str = \"Marker\",\n    value_name: str = \"Expression\",\n    show: bool = True,\n):\n    \"\"\"Plot Kernel-Density-Estimation for each provided population and for multiple markers.\n\n    Args:\n        adata: An `AnnData` object.\n        population: One population, or a list of population to be analyzed, or `None`. If not `None`, the population name(s) has to be in `adata.obs[key]`.\n        markers: List of markers to plot. If `None`, the list is chosen automatically.\n        key: Key to look for populations in `adata.obs`. By default, uses the model predictions.\n        n_markers: Number of markers to choose automatically if `markers is None`.\n        n_cells: Number of cells to be considered for the heatmap (to accelerate it when $N$ is very high). If `None`, consider all cells.\n        ncols: Number of figures per row.\n        var_name: Name displayed on the graphs.\n        value_name: Name displayed on the graphs.\n        show: Whether or not to display the figure.\n    \"\"\"\n    indices = _get_subset_indices(adata.n_obs, n_cells)\n    adata = adata[indices]\n\n    markers = select_markers(adata, markers, n_markers, key, population, 1)\n\n    df = adata.to_df()\n\n    if population is None:\n        df = pd.melt(\n            df,\n            value_vars=markers,\n            var_name=var_name,\n            value_name=value_name,\n        )\n\n        sns.displot(\n            df,\n            x=value_name,\n            col=var_name,\n            col_wrap=ncols,\n            kind=\"kde\",\n            common_norm=False,\n            facet_kws=dict(sharey=False),\n        )\n        return\n\n    keys = adata.obs[key]\n    df[key] = np.where(~np.isin(keys, population), \"Others\", keys)\n\n    df = pd.melt(\n        df,\n        id_vars=[key],\n        value_vars=markers,\n        var_name=var_name,\n        value_name=value_name,\n    )\n\n    sns.displot(\n        df,\n        x=value_name,\n        col=var_name,\n        hue=key,\n        col_wrap=ncols,\n        kind=\"kde\",\n        common_norm=False,\n        facet_kws=dict(sharey=False),\n        palette=get_palette_others(df, key),\n        hue_order=sorted(df[key].unique(), key=\"Others\".__eq__),\n    )\n</code></pre>"},{"location":"api/plots/#scyan.plot.log_prob_threshold","title":"<code>scyan.plot.log_prob_threshold(adata, show=True)</code>","text":"<p>Plot the number of cells annotated depending on the log probability threshold (below which cells are left non-classified). It can be helpful to determine the best threshold value, i.e. before a significative decrease in term of number of cells annotated.</p> <p>Note</p> <p>To use this function, you first need to fit a <code>scyan.Scyan</code> model and use the <code>model.predict()</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>The <code>AnnData</code> object used during the model training.</p> required <code>show</code> <code>bool</code> <p>Whether or not to display the figure.</p> <code>True</code> Source code in <code>scyan/plot/density.py</code> <pre><code>@plot_decorator(adata=True)\ndef log_prob_threshold(adata: AnnData, show: bool = True):\n    \"\"\"Plot the number of cells annotated depending on the log probability threshold (below which cells are left non-classified). It can be helpful to determine the best threshold value, i.e. before a significative decrease in term of number of cells annotated.\n\n    !!! note\n        To use this function, you first need to fit a `scyan.Scyan` model and use the `model.predict()` method.\n\n    Args:\n        adata: The `AnnData` object used during the model training.\n        show: Whether or not to display the figure.\n    \"\"\"\n    assert (\n        \"scyan_log_probs\" in adata.obs\n    ), f\"Cannot find 'scyan_log_probs' in adata.obs. Have you run model.predict()?\"\n\n    x = np.sort(adata.obs[\"scyan_log_probs\"])\n    y = 1 - np.arange(len(x)) / float(len(x))\n\n    plt.plot(x, y)\n    plt.xlim(-100, x.max())\n    sns.despine(offset=10, trim=True)\n    plt.ylabel(\"Ratio of predicted cells\")\n    plt.xlabel(\"Log density threshold\")\n</code></pre>"},{"location":"api/plots/#scyan.plot.pop_level","title":"<code>scyan.plot.pop_level(model, group_name, level_name='level', key='scyan_pop', **scanpy_kwargs)</code>","text":"<p>Plot all subpopulations of a group at a certain level on a UMAP (according to the populations levels provided in the knowledge table).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Scyan</code> <p>Scyan model.</p> required <code>group_name</code> <code>str</code> <p>The group to look at among the populations of the selected level.</p> required <code>level_name</code> <code>str</code> <p>Name of the column of the knowledge table containing the names of the grouped populations.</p> <code>'level'</code> <code>key</code> <code>str</code> <p>Key of <code>adata.obs</code> to access the model predictions.</p> <code>'scyan_pop'</code> Source code in <code>scyan/plot/dot.py</code> <pre><code>def pop_level(\n    model: Scyan,\n    group_name: str,\n    level_name: str = \"level\",\n    key: str = \"scyan_pop\",\n    **scanpy_kwargs: int,\n) -&gt; None:\n    \"\"\"Plot all subpopulations of a group at a certain level on a UMAP (according to the populations levels provided in the knowledge table).\n\n    Args:\n        model: Scyan model.\n        group_name: The group to look at among the populations of the selected level.\n        level_name: Name of the column of the knowledge table containing the names of the grouped populations.\n        key: Key of `adata.obs` to access the model predictions.\n    \"\"\"\n    adata = model.adata\n    table = model.table\n\n    assert isinstance(\n        table.index, pd.MultiIndex\n    ), \"To use this function, you need a MultiIndex DataFrame, see: https://mics-lab.github.io/scyan/tutorials/usage/#working-with-hierarchical-populations\"\n\n    level_names = table.index.names[1:]\n    assert (\n        level_name in level_names\n    ), f\"Level '{level_name}' unknown. Choose one of: {level_names}\"\n\n    base_pops = table.index.get_level_values(0)\n    group_pops = table.index.get_level_values(level_name)\n    assert (\n        group_name in group_pops\n    ), f\"Invalid group name '{group_name}'. It has to be one of: {', '.join(group_pops)}.\"\n\n    valid_populations = [\n        pop for pop, group in zip(base_pops, group_pops) if group == group_name\n    ]\n    key_name = f\"{key}_one_level\"\n    adata.obs[key_name] = pd.Categorical(\n        [pop if pop in valid_populations else np.nan for pop in adata.obs[key]]\n    )\n    umap(\n        adata,\n        color=key_name,\n        title=f\"Among {group_name}\",\n        na_in_legend=False,\n        **scanpy_kwargs,\n    )\n</code></pre>"},{"location":"api/plots/#scyan.plot.pops_hierarchy","title":"<code>scyan.plot.pops_hierarchy(model, figsize=(18, 5), show=True)</code>","text":"<p>Plot populations as a tree, where each level corresponds to more detailed populations. To run this function, your knowledge table need to contain at least one population 'level' (see this tutorial), and you need to install <code>graphviz</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Scyan</code> <p>Scyan model.</p> required <code>figsize</code> <code>tuple</code> <p>Matplotlib figure size.</p> <code>(18, 5)</code> <code>show</code> <code>bool</code> <p>Whether or not to display the figure.</p> <code>True</code> Source code in <code>scyan/plot/graph.py</code> <pre><code>@plot_decorator()\ndef pops_hierarchy(model: Scyan, figsize: tuple = (18, 5), show: bool = True) -&gt; None:\n    \"\"\"Plot populations as a tree, where each level corresponds to more detailed populations. To run this function, your knowledge table need to contain at least one population 'level' (see [this tutorial](../../tutorials/usage/#working-with-hierarchical-populations)), and you need to install `graphviz`.\n\n    Args:\n        model: Scyan model.\n        figsize: Matplotlib figure size.\n        show: Whether or not to display the figure.\n    \"\"\"\n    import networkx as nx\n    from networkx.drawing.nx_agraph import graphviz_layout\n\n    table = model.table\n\n    assert isinstance(\n        table.index, pd.MultiIndex\n    ), \"To plot population hierarchy, you need a MultiIndex DataFrame. See the documentation for more details.\"\n\n    root = \"All populations\"\n\n    G = nx.DiGraph()\n    G.add_node(root)\n\n    def add_nodes(table, indices, level, parent=root):\n        if level == -1:\n            return\n\n        index = table.index.get_level_values(level)\n        dict_indices = defaultdict(list)\n        for i in indices:\n            dict_indices[index[i]].append(i)\n\n        for name, indices in dict_indices.items():\n            if not name == parent:\n                G.add_node(name)\n                G.add_edge(parent, name)\n            add_nodes(table, indices, level - 1, name)\n\n    add_nodes(\n        table,\n        range(len(table)),\n        table.index.nlevels - 1,\n    )\n\n    plt.figure(figsize=figsize)\n    pos = graphviz_layout(G, prog=\"dot\")\n    nx.draw_networkx(G, pos, with_labels=False, arrows=False, node_size=0)\n    for node, (x, y) in pos.items():\n        plt.text(\n            x,\n            y,\n            node,\n            ha=\"center\",\n            va=\"center\",\n            rotation=90,\n            bbox=dict(facecolor=\"wheat\", edgecolor=\"black\", boxstyle=\"round,pad=0.5\"),\n        )\n\n    plt.grid(False)\n    plt.box(False)\n</code></pre>"},{"location":"api/preprocess/","title":"Preprocessing","text":""},{"location":"api/preprocess/#scyan.preprocess.auto_logicle_transform","title":"<code>scyan.preprocess.auto_logicle_transform(adata, q=0.05, m=4.5, quantile_clip=1e-05)</code>","text":"<p>Auto-logicle transformation implementation. We recommend it for flow cytometry or spectral flow cytometry data.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required <code>q</code> <code>float</code> <p>See logicle article. Defaults to 0.05.</p> <code>0.05</code> <code>m</code> <code>float</code> <p>See logicle article. Defaults to 4.5.</p> <code>4.5</code> Source code in <code>scyan/preprocess.py</code> <pre><code>def auto_logicle_transform(\n    adata: AnnData, q: float = 0.05, m: float = 4.5, quantile_clip: Optional[float] = 1e-5\n) -&gt; None:\n    \"\"\"[Auto-logicle transformation](https://pubmed.ncbi.nlm.nih.gov/16604519/) implementation.\n    We recommend it for flow cytometry or spectral flow cytometry data.\n\n    Args:\n        adata: An `AnnData` object.\n        q: See logicle article. Defaults to 0.05.\n        m: See logicle article. Defaults to 4.5.\n    \"\"\"\n    adata.uns[\"scyan_logicle\"] = {}\n    markers_failed = []\n\n    for marker in adata.var_names:\n        column = adata[:, marker].X.toarray().flatten()\n\n        w = 0\n        t = column.max()\n        negative_values = column[column &lt; 0]\n\n        if negative_values.size:\n            threshold = np.quantile(negative_values, 0.25) - 1.5 * scipy.stats.iqr(\n                negative_values\n            )\n            negative_values = negative_values[negative_values &gt;= threshold]\n\n            if negative_values.size:\n                r = 1e-8 + np.quantile(negative_values, q)\n                if 10**m * abs(r) &gt; t:\n                    w = (m - np.log10(t / abs(r))) / 2\n\n        if not w or w &gt; 2:\n            markers_failed.append(marker)\n            w, t = 1, 5e5\n\n        column = flowutils.transforms.logicle(column, None, t=t, m=m, w=w)\n        adata.uns[\"scyan_logicle\"][marker] = [t, m, w]\n\n        if quantile_clip is None:\n            adata[:, marker] = column\n        else:\n            adata[:, marker] = column.clip(np.quantile(column, quantile_clip))\n\n    if markers_failed:\n        log.warning(\n            f\"Auto logicle transformation failed for the following markers (logicle was used instead): {', '.join(markers_failed)}.\\nIt can happen when expressions are all positive or all negative.\"\n        )\n</code></pre>"},{"location":"api/preprocess/#scyan.preprocess.asinh_transform","title":"<code>scyan.preprocess.asinh_transform(adata, translation=0, cofactor=5)</code>","text":"<p>Asinh transformation for cell-expressions: \\(asinh((x - translation)/cofactor)\\).</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required <code>translation</code> <code>float</code> <p>Constant substracted to the marker expression before division by the cofactor.</p> <code>0</code> <code>cofactor</code> <code>float</code> <p>Scaling factor before computing the asinh.</p> <code>5</code> Source code in <code>scyan/preprocess.py</code> <pre><code>def asinh_transform(adata: AnnData, translation: float = 0, cofactor: float = 5) -&gt; None:\n    \"\"\"Asinh transformation for cell-expressions: $asinh((x - translation)/cofactor)$.\n\n    Args:\n        adata: An `AnnData` object.\n        translation: Constant substracted to the marker expression before division by the cofactor.\n        cofactor: Scaling factor before computing the asinh.\n    \"\"\"\n    adata.uns[\"scyan_asinh\"] = [translation, cofactor]\n    adata.X = np.arcsinh((adata.X - translation) / cofactor)\n</code></pre>"},{"location":"api/preprocess/#scyan.preprocess.inverse_transform","title":"<code>scyan.preprocess.inverse_transform(adata, obsm=None, obsm_names=None, transformation=None)</code>","text":"<p>Inverses the transformation function, i.e. either scyan.preprocess.auto_logicle_transform or scyan.preprocess.asinh_transform. It requires to have run have of these before.</p> <p>Note</p> <p>If you scaled your data, the complete inverse consists in running scyan.preprocess.unscale first, and then this function.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required <code>obsm</code> <code>Optional[str]</code> <p>Name of the anndata obsm to consider. If <code>None</code>, use <code>adata.X</code>.</p> <code>None</code> <code>obsm_names</code> <code>Optional[List[str]]</code> <p>Names of the ordered markers from obsm. It is required if obsm is not <code>None</code>, if there are less markers than in <code>adata.X</code>, and if the transformation to reverse is <code>logicle</code>. Usually, it corresponds to <code>model.var_names</code>.</p> <code>None</code> <code>transformation</code> <code>Optional[str]</code> <p>Name of the transformation to inverse: one of <code>['logicle', 'asinh', None]</code>. By default, it chooses automatically depending on which transformation was previously run.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Inverse transformed expressions array of shape \\((N, M)\\).</p> Source code in <code>scyan/preprocess.py</code> <pre><code>def inverse_transform(\n    adata: AnnData,\n    obsm: Optional[str] = None,\n    obsm_names: Optional[List[str]] = None,\n    transformation: Optional[str] = None,\n) -&gt; np.ndarray:\n    \"\"\"Inverses the transformation function, i.e. either [scyan.preprocess.auto_logicle_transform][] or [scyan.preprocess.asinh_transform][]. It requires to have run have of these before.\n\n    !!! note\n        If you scaled your data, the complete inverse consists in running [scyan.preprocess.unscale][] first, and then this function.\n\n    Args:\n        adata: An `AnnData` object.\n        obsm: Name of the anndata obsm to consider. If `None`, use `adata.X`.\n        obsm_names: Names of the ordered markers from obsm. It is required if obsm is not `None`, if there are less markers than in `adata.X`, and if the transformation to reverse is `logicle`. Usually, it corresponds to `model.var_names`.\n        transformation: Name of the transformation to inverse: one of `['logicle', 'asinh', None]`. By default, it chooses automatically depending on which transformation was previously run.\n\n    Returns:\n        Inverse transformed expressions array of shape $(N, M)$.\n    \"\"\"\n    if transformation is None:\n        transformation = \"asinh\" if \"scyan_asinh\" in adata.uns else None\n        transformation = \"logicle\" if \"scyan_logicle\" in adata.uns else transformation\n        if transformation is None:\n            raise ValueError(\n                \"No transformation to inverse: you need to run 'asinh_transform' or 'auto_logicle_transform' before to inverse it.\"\n            )\n\n    if transformation == \"logicle\":\n        log.info(\"Performing inverse logicle transform\")\n        assert (\n            \"scyan_logicle\" in adata.uns\n        ), \"You need to run 'auto_logicle_transform' before to inverse it.\"\n\n        if obsm is None:\n            obsm_names = adata.var_names\n        elif obsm_names is None:\n            assert (\n                adata.obsm[obsm].shape[1] != adata.n_vars\n            ), f\"When the number of var in adata.obsm['{obsm}'] is not `adata.n_vars`, use `obs_names`\"\n            obsm_names = adata.var_names\n\n        return np.stack(\n            [_logicle_inverse_one(adata, obsm, marker) for marker in obsm_names],\n            axis=1,\n        )\n\n    if transformation == \"asinh\":\n        log.info(\"Performing inverse asinh transform\")\n        assert (\n            \"scyan_asinh\" in adata.uns\n        ), \"You need to run 'asinh_transform' before to inverse it.\"\n\n        X = adata.X if obsm is None else adata.obsm[obsm]\n        translation, cofactor = adata.uns[\"scyan_asinh\"]\n\n        return np.sinh(X) * cofactor + translation\n\n    raise NameError(\n        f\"Parameter 'transformation' has to be 'logicle' or 'asinh'. Found {transformation}.\"\n    )\n</code></pre>"},{"location":"api/preprocess/#scyan.preprocess.scale","title":"<code>scyan.preprocess.scale(adata, max_value=10, center=None)</code>","text":"<p>Tranforms the data such as (i) <code>std=1</code>, and (ii) either <code>0</code> is sent to <code>-1</code> (for CyTOF data) or <code>means=0</code> (for flow or spectral flow data); except if <code>center</code> is set (which overwrites the default behavior).</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required <code>max_value</code> <code>float</code> <p>Clip to this value after scaling.</p> <code>10</code> <code>center</code> <code>Optional[bool]</code> <p>If <code>None</code>, data is only centered for spectral or flow cytometry data (recommended), else, it is centered or not according to the value given.</p> <code>None</code> Source code in <code>scyan/preprocess.py</code> <pre><code>def scale(adata: AnnData, max_value: float = 10, center: Optional[bool] = None) -&gt; None:\n    \"\"\"Tranforms the data such as (i) `std=1`, and (ii) either `0` is sent to `-1` (for CyTOF data) or `means=0` (for flow or spectral flow data); except if `center` is set (which overwrites the default behavior).\n\n    Args:\n        adata: An `AnnData` object.\n        max_value: Clip to this value after scaling.\n        center: If `None`, data is only centered for spectral or flow cytometry data (recommended), else, it is centered or not according to the value given.\n    \"\"\"\n    stds = adata.X.std(axis=0)\n    adata.uns[\"scyan_scaling_stds\"] = stds\n\n    if (center is False) or (center is None and \"scyan_asinh\" in adata.uns):\n        log.info(\n            \"Data will be standardised, and translated so that 0 goes to -1. This is advised only when using CyTOF data (if this is not your case, consider running 'auto_logicle_transform' instead of 'asinh_transform').\"\n        )\n        adata.X = (adata.X / stds - 1).clip(-max_value, max_value)\n    else:\n        log.info(\n            \"Data will be centered and standardised. This is advised only when using spectral/flow data (if this is not your case, consider running 'asinh_transform' instead of 'auto_logicle_transform').\"\n        )\n        means = adata.X.mean(axis=0)\n        adata.X = ((adata.X - means) / stds).clip(-max_value, max_value)\n        adata.uns[\"scyan_scaling_means\"] = means\n</code></pre>"},{"location":"api/preprocess/#scyan.preprocess.unscale","title":"<code>scyan.preprocess.unscale(adata, obsm=None, obsm_names=None)</code>","text":"<p>Reverse standardisation. It requires to have run scyan.preprocess.scale before.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required <code>obsm</code> <code>Optional[str]</code> <p>Name of the adata obsm to consider. If <code>None</code>, use <code>adata.X</code>.</p> <code>None</code> <code>obsm_names</code> <code>Optional[List[str]]</code> <p>Names of the ordered markers from obsm. It is required if obsm is not <code>None</code>, and if there are less markers than in <code>adata.X</code>. Usually, it corresponds to <code>model.var_names</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Unscaled numpy array of shape \\((N, M)\\).</p> Source code in <code>scyan/preprocess.py</code> <pre><code>def unscale(\n    adata: AnnData, obsm: Optional[str] = None, obsm_names: Optional[List[str]] = None\n) -&gt; np.ndarray:\n    \"\"\"Reverse standardisation. It requires to have run [scyan.preprocess.scale][] before.\n\n    Args:\n        adata: An `AnnData` object.\n        obsm: Name of the adata obsm to consider. If `None`, use `adata.X`.\n        obsm_names: Names of the ordered markers from obsm. It is required if obsm is not `None`, and if there are less markers than in `adata.X`. Usually, it corresponds to `model.var_names`.\n\n    Returns:\n        Unscaled numpy array of shape $(N, M)$.\n    \"\"\"\n    assert (\n        \"scyan_scaling_stds\" in adata.uns\n    ), \"It seems you haven't run 'scyan.preprocess.scale' before.\"\n\n    X = adata.X if obsm is None else adata.obsm[obsm]\n    stds = adata.uns[\"scyan_scaling_stds\"]\n\n    if obsm is not None and X.shape[1] != adata.n_vars:\n        assert (\n            obsm_names is not None\n        ), f\"Found {X.shape[1]} markers in adata.obsm['{obsm}'], but 'adata' has {adata.n_vars} vars. Please use the 'obsm_names' argument to provide the ordered names of the markers used in adata.obsm['{obsm}'].\"\n\n        indices = [adata.var_names.get_loc(marker) for marker in obsm_names]\n        stds = stds[indices]\n\n    if \"scyan_scaling_means\" in adata.uns:\n        return adata.uns[\"scyan_scaling_means\"] + stds * X\n\n    return (X + 1) * stds\n</code></pre>"},{"location":"api/preprocess/#scyan.preprocess.compensate","title":"<code>scyan.preprocess.compensate(adata, key_added=None)</code>","text":"<p>Use the spillover matrix in <code>adata.varp[\"spillover_matrix\"]</code> to correct spillover from <code>adata.X</code></p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object</p> required <code>key_added</code> <code>Optional[str]</code> <p>Optional key in <code>adata.layers</code> information is saved to. By default, saved in <code>adata.X</code></p> <code>None</code> Source code in <code>scyan/preprocess.py</code> <pre><code>def compensate(adata: AnnData, key_added: Optional[str] = None):\n    \"\"\"Use the spillover matrix in `adata.varp[\"spillover_matrix\"]` to correct spillover from `adata.X`\n\n    Args:\n        adata: An `AnnData` object\n        key_added: Optional key in `adata.layers` information is saved to. By default, saved in `adata.X`\n    \"\"\"\n    assert \"spillover_matrix\" in adata.varp, f\"No 'spillover_matrix' found in adata.varp\"\n\n    if any(\n        name in adata.uns\n        for name in [\"scyan_asinh\", \"scyan_logicle\", \"scyan_scaling_means\"]\n    ):\n        log.warn(\"It is recommended to apply spillover only on raw data (unprocessed)\")\n\n    S = adata.varp[\"spillover_matrix\"]\n    corrected = np.linalg.solve(S, adata.X.T).T\n\n    if key_added is None:\n        adata.X = corrected\n    else:\n        adata.layers[key_added] = corrected\n</code></pre>"},{"location":"api/prior/","title":"scyan.module.PriorDistribution","text":"<p>             Bases: <code>LightningModule</code></p> <p>Prior distribution \\(U\\)</p> Source code in <code>scyan/module/distribution.py</code> <pre><code>class PriorDistribution(pl.LightningModule):\n    \"\"\"Prior distribution $U$\"\"\"\n\n    def __init__(\n        self,\n        rho: Tensor,\n        is_continuum_marker: Tensor,\n        prior_std: float,\n        n_markers: int,\n    ):\n        \"\"\"\n        Args:\n            rho: Tensor $\\rho$ representing the knowledge table (size $P$ x $M$)\n            is_continuum_marker: tensor of size $M$ whose values tell if the marker is a continuum of expressions.\n            prior_std: Standard deviation $\\sigma$ for $H$.\n            n_markers: Number of markers in the table.\n        \"\"\"\n        super().__init__()\n        self.n_markers = n_markers\n        self.is_continuum_marker = is_continuum_marker\n\n        self.register_buffer(\"rho\", rho)\n        self.register_buffer(\"loc\", torch.zeros((n_markers)))\n        self.set_rho_mask()\n\n        self.prior_std = prior_std\n        self.uniform = distributions.Uniform(-1, 1)\n\n    @property\n    def prior_std(self):\n        return self._prior_std\n\n    @prior_std.setter\n    def prior_std(self, std: float) -&gt; None:\n        self._prior_std = std\n        cov = torch.eye((self.n_markers)) * std**2\n        self.register_buffer(\"cov\", cov.to(self.device))\n        self.normal = distributions.Normal(0, std)\n        self.compute_constant_terms()\n\n    def set_rho_mask(self) -&gt; None:\n        rho_mask = self.rho.isnan()\n        self.rho[rho_mask] = 0\n        self.register_buffer(\"rho_mask\", rho_mask)\n        self.compute_modes()\n\n    def fill_rho(self, means: torch.Tensor) -&gt; None:\n        self.rho[self.rho_mask] = means[self.rho_mask]\n        self.register_buffer(\"rho_mask\", torch.full_like(self.rho, False, dtype=bool))\n        self.compute_modes()\n        self.compute_constant_terms()\n\n    def compute_modes(self):\n        self.factor = torch.ones(self.n_markers, dtype=torch.float32)\n        self.factor[self.is_continuum_marker] = 5\n        self.factor = self.factor[None, None, :]\n\n        self.register_buffer(\"modes\", self.rho[None, ...] / self.factor)\n\n    @property\n    def prior_h(self) -&gt; distributions.Distribution:\n        \"\"\"The distribution of $H$\"\"\"\n        return distributions.MultivariateNormal(self.loc, self.cov)\n\n    def compute_constant_terms(self) -&gt; None:\n        self.uniform_law_radius = 1 - self.prior_std\n\n        _gamma = (\n            self.uniform_law_radius\n            / self.prior_std\n            * torch.sqrt(2 / torch.tensor(torch.pi))\n        )\n        self.gamma = 1 / (1 + _gamma)\n\n        na_constant_term = self.rho_mask.sum(dim=1) * torch.log(self.gamma)\n        self.register_buffer(\"na_constant_term\", na_constant_term)\n\n    def difference_to_modes(self, u: Tensor) -&gt; Tensor:\n        \"\"\"Difference between the latent variable $U$ and all the modes (one mode per population).\n\n        Args:\n            u: Latent variables tensor of size $(B, M)$.\n\n        Returns:\n            Tensor of size $(B, P, M)$ representing differences to all modes.\n        \"\"\"\n        diff = u[:, None, :] - self.modes\n\n        diff[:, self.rho_mask] = torch.clamp(\n            diff[:, self.rho_mask].abs() - self.uniform_law_radius, min=0\n        )  # Handling NA values\n\n        return diff\n\n    def log_prob_per_marker(self, u: Tensor) -&gt; Tensor:\n        \"\"\"Log probability per marker and per population.\n\n        Args:\n            u: Latent variables tensor of size $(B, M)$.\n\n        Returns:\n            Log probabilities tensor of size $(B, P, M)$.\n        \"\"\"\n        diff = self.difference_to_modes(u)  # size B x P x M\n\n        return self.normal.log_prob(diff) + self.rho_mask * torch.log(self.gamma)\n\n    def log_prob(self, u: Tensor) -&gt; Tensor:\n        \"\"\"Log probability per population.\n\n        Args:\n            u: Latent variables tensor of size $(B, M)$.\n\n        Returns:\n            Log probabilities tensor of size $(B, P)$.\n        \"\"\"\n        diff = self.difference_to_modes(u)  # size B x P x M\n\n        return self.prior_h.log_prob(diff) + self.na_constant_term\n\n    def sample(self, z: Tensor) -&gt; Tensor:\n        \"\"\"Sampling latent cell-marker expressions.\n\n        Args:\n            z: Tensor of population indices.\n\n        Returns:\n            Latent expressions, i.e. a tensor of size $(len(Z), M)$.\n        \"\"\"\n        (n_samples,) = z.shape\n\n        e = self.rho[z] + self.rho_mask[z] * self.uniform.sample(\n            (n_samples, self.n_markers)\n        )\n        h = self.prior_h.sample((n_samples,))\n\n        return e + h\n</code></pre>"},{"location":"api/prior/#scyan.module.PriorDistribution.prior_h","title":"<code>prior_h: distributions.Distribution</code>  <code>property</code>","text":"<p>The distribution of \\(H\\)</p>"},{"location":"api/prior/#scyan.module.PriorDistribution.__init__","title":"<code>__init__(rho, is_continuum_marker, prior_std, n_markers)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>rho</code> <code>Tensor</code> <p>Tensor $ ho$ representing the knowledge table (size \\(P\\) x \\(M\\))</p> required <code>is_continuum_marker</code> <code>Tensor</code> <p>tensor of size \\(M\\) whose values tell if the marker is a continuum of expressions.</p> required <code>prior_std</code> <code>float</code> <p>Standard deviation \\(\\sigma\\) for \\(H\\).</p> required <code>n_markers</code> <code>int</code> <p>Number of markers in the table.</p> required Source code in <code>scyan/module/distribution.py</code> <pre><code>def __init__(\n    self,\n    rho: Tensor,\n    is_continuum_marker: Tensor,\n    prior_std: float,\n    n_markers: int,\n):\n    \"\"\"\n    Args:\n        rho: Tensor $\\rho$ representing the knowledge table (size $P$ x $M$)\n        is_continuum_marker: tensor of size $M$ whose values tell if the marker is a continuum of expressions.\n        prior_std: Standard deviation $\\sigma$ for $H$.\n        n_markers: Number of markers in the table.\n    \"\"\"\n    super().__init__()\n    self.n_markers = n_markers\n    self.is_continuum_marker = is_continuum_marker\n\n    self.register_buffer(\"rho\", rho)\n    self.register_buffer(\"loc\", torch.zeros((n_markers)))\n    self.set_rho_mask()\n\n    self.prior_std = prior_std\n    self.uniform = distributions.Uniform(-1, 1)\n</code></pre>"},{"location":"api/prior/#scyan.module.PriorDistribution.difference_to_modes","title":"<code>difference_to_modes(u)</code>","text":"<p>Difference between the latent variable \\(U\\) and all the modes (one mode per population).</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>Tensor</code> <p>Latent variables tensor of size \\((B, M)\\).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of size \\((B, P, M)\\) representing differences to all modes.</p> Source code in <code>scyan/module/distribution.py</code> <pre><code>def difference_to_modes(self, u: Tensor) -&gt; Tensor:\n    \"\"\"Difference between the latent variable $U$ and all the modes (one mode per population).\n\n    Args:\n        u: Latent variables tensor of size $(B, M)$.\n\n    Returns:\n        Tensor of size $(B, P, M)$ representing differences to all modes.\n    \"\"\"\n    diff = u[:, None, :] - self.modes\n\n    diff[:, self.rho_mask] = torch.clamp(\n        diff[:, self.rho_mask].abs() - self.uniform_law_radius, min=0\n    )  # Handling NA values\n\n    return diff\n</code></pre>"},{"location":"api/prior/#scyan.module.PriorDistribution.log_prob","title":"<code>log_prob(u)</code>","text":"<p>Log probability per population.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>Tensor</code> <p>Latent variables tensor of size \\((B, M)\\).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Log probabilities tensor of size \\((B, P)\\).</p> Source code in <code>scyan/module/distribution.py</code> <pre><code>def log_prob(self, u: Tensor) -&gt; Tensor:\n    \"\"\"Log probability per population.\n\n    Args:\n        u: Latent variables tensor of size $(B, M)$.\n\n    Returns:\n        Log probabilities tensor of size $(B, P)$.\n    \"\"\"\n    diff = self.difference_to_modes(u)  # size B x P x M\n\n    return self.prior_h.log_prob(diff) + self.na_constant_term\n</code></pre>"},{"location":"api/prior/#scyan.module.PriorDistribution.log_prob_per_marker","title":"<code>log_prob_per_marker(u)</code>","text":"<p>Log probability per marker and per population.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>Tensor</code> <p>Latent variables tensor of size \\((B, M)\\).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Log probabilities tensor of size \\((B, P, M)\\).</p> Source code in <code>scyan/module/distribution.py</code> <pre><code>def log_prob_per_marker(self, u: Tensor) -&gt; Tensor:\n    \"\"\"Log probability per marker and per population.\n\n    Args:\n        u: Latent variables tensor of size $(B, M)$.\n\n    Returns:\n        Log probabilities tensor of size $(B, P, M)$.\n    \"\"\"\n    diff = self.difference_to_modes(u)  # size B x P x M\n\n    return self.normal.log_prob(diff) + self.rho_mask * torch.log(self.gamma)\n</code></pre>"},{"location":"api/prior/#scyan.module.PriorDistribution.sample","title":"<code>sample(z)</code>","text":"<p>Sampling latent cell-marker expressions.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>Tensor</code> <p>Tensor of population indices.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Latent expressions, i.e. a tensor of size \\((len(Z), M)\\).</p> Source code in <code>scyan/module/distribution.py</code> <pre><code>def sample(self, z: Tensor) -&gt; Tensor:\n    \"\"\"Sampling latent cell-marker expressions.\n\n    Args:\n        z: Tensor of population indices.\n\n    Returns:\n        Latent expressions, i.e. a tensor of size $(len(Z), M)$.\n    \"\"\"\n    (n_samples,) = z.shape\n\n    e = self.rho[z] + self.rho_mask[z] * self.uniform.sample(\n        (n_samples, self.n_markers)\n    )\n    h = self.prior_h.sample((n_samples,))\n\n    return e + h\n</code></pre>"},{"location":"api/real_nvp/","title":"scyan.module.RealNVP","text":"<p>             Bases: <code>LightningModule</code></p> <p>Normalizing flow module (more specifically the RealNVP transformation \\(f_{\\phi}\\)).</p> <p>Attributes:</p> Name Type Description <code>module</code> <code>Sequential</code> <p>Sequence of coupling layers.</p> Source code in <code>scyan/module/real_nvp.py</code> <pre><code>class RealNVP(pl.LightningModule):\n    \"\"\"Normalizing flow module (more specifically the RealNVP transformation $f_{\\phi}$).\n\n    Attributes:\n        module (nn.Sequential): Sequence of [coupling layers][scyan.module.CouplingLayer].\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        output_size: int,\n        n_hidden_layers: int,\n        n_layers: int,\n    ):\n        \"\"\"\n        Args:\n            input_size: Input size, i.e. number of markers + covariates\n            hidden_size: MLP (`s` and `t`) hidden size.\n            output_size: Output size, i.e. number of markers.\n            n_hidden_layers: Number of hidden layers for the MLP (`s` and `t`).\n            n_layers: Number of coupling layers.\n        \"\"\"\n        super().__init__()\n        self.module_list = nn.ModuleList(\n            [\n                CouplingLayer(\n                    input_size,\n                    hidden_size,\n                    output_size,\n                    n_hidden_layers,\n                    self._mask(output_size, i),\n                )\n                for i in range(n_layers)\n            ]\n        )\n        self.module = nn.Sequential(*self.module_list)\n\n    def _mask(self, output_size: int, shift: int):\n        return (((torch.arange(output_size) - shift) % 3) &gt; 0).to(int)\n\n    def forward(self, x: Tensor, covariates: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        \"\"\"Forward implementation, i.e. $f_{\\phi}$.\n\n        Args:\n            x: Inputs of size $(B, M)$.\n            covariates: Covariates of size $(B, M_c)$\n\n        Returns:\n            Tuple of (outputs, covariates, lod_det_jacobian sum)\n        \"\"\"\n        return self.module((x, covariates, None))\n\n    def inverse(self, u: Tensor, covariates: Tensor) -&gt; Tensor:\n        \"\"\"Go through the RealNVP in reverse direction, i.e. $f_{\\phi}^{-1}$.\n\n        Args:\n            u: Latent expressions of size $(B, M)$.\n            covariates: Covariates of size $(B, M_c)$\n\n        Returns:\n            Outputs of size $(B, M)$.\n        \"\"\"\n        for module in reversed(self.module_list):\n            u = module.inverse(u, covariates)\n        return u\n</code></pre>"},{"location":"api/real_nvp/#scyan.module.RealNVP.__init__","title":"<code>__init__(input_size, hidden_size, output_size, n_hidden_layers, n_layers)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>Input size, i.e. number of markers + covariates</p> required <code>hidden_size</code> <code>int</code> <p>MLP (<code>s</code> and <code>t</code>) hidden size.</p> required <code>output_size</code> <code>int</code> <p>Output size, i.e. number of markers.</p> required <code>n_hidden_layers</code> <code>int</code> <p>Number of hidden layers for the MLP (<code>s</code> and <code>t</code>).</p> required <code>n_layers</code> <code>int</code> <p>Number of coupling layers.</p> required Source code in <code>scyan/module/real_nvp.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    hidden_size: int,\n    output_size: int,\n    n_hidden_layers: int,\n    n_layers: int,\n):\n    \"\"\"\n    Args:\n        input_size: Input size, i.e. number of markers + covariates\n        hidden_size: MLP (`s` and `t`) hidden size.\n        output_size: Output size, i.e. number of markers.\n        n_hidden_layers: Number of hidden layers for the MLP (`s` and `t`).\n        n_layers: Number of coupling layers.\n    \"\"\"\n    super().__init__()\n    self.module_list = nn.ModuleList(\n        [\n            CouplingLayer(\n                input_size,\n                hidden_size,\n                output_size,\n                n_hidden_layers,\n                self._mask(output_size, i),\n            )\n            for i in range(n_layers)\n        ]\n    )\n    self.module = nn.Sequential(*self.module_list)\n</code></pre>"},{"location":"api/real_nvp/#scyan.module.RealNVP.forward","title":"<code>forward(x, covariates)</code>","text":"<p>Forward implementation, i.e. \\(f_{\\phi}\\).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Inputs of size \\((B, M)\\).</p> required <code>covariates</code> <code>Tensor</code> <p>Covariates of size \\((B, M_c)\\)</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>Tuple of (outputs, covariates, lod_det_jacobian sum)</p> Source code in <code>scyan/module/real_nvp.py</code> <pre><code>def forward(self, x: Tensor, covariates: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Forward implementation, i.e. $f_{\\phi}$.\n\n    Args:\n        x: Inputs of size $(B, M)$.\n        covariates: Covariates of size $(B, M_c)$\n\n    Returns:\n        Tuple of (outputs, covariates, lod_det_jacobian sum)\n    \"\"\"\n    return self.module((x, covariates, None))\n</code></pre>"},{"location":"api/real_nvp/#scyan.module.RealNVP.inverse","title":"<code>inverse(u, covariates)</code>","text":"<p>Go through the RealNVP in reverse direction, i.e. \\(f_{\\phi}^{-1}\\).</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>Tensor</code> <p>Latent expressions of size \\((B, M)\\).</p> required <code>covariates</code> <code>Tensor</code> <p>Covariates of size \\((B, M_c)\\)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Outputs of size \\((B, M)\\).</p> Source code in <code>scyan/module/real_nvp.py</code> <pre><code>def inverse(self, u: Tensor, covariates: Tensor) -&gt; Tensor:\n    \"\"\"Go through the RealNVP in reverse direction, i.e. $f_{\\phi}^{-1}$.\n\n    Args:\n        u: Latent expressions of size $(B, M)$.\n        covariates: Covariates of size $(B, M_c)$\n\n    Returns:\n        Outputs of size $(B, M)$.\n    \"\"\"\n    for module in reversed(self.module_list):\n        u = module.inverse(u, covariates)\n    return u\n</code></pre>"},{"location":"api/tools/","title":"Tools","text":""},{"location":"api/tools/#scyan.tools.umap","title":"<code>scyan.tools.umap(adata, markers=None, obsm=None, n_cells=200000, min_dist=0.5, obsm_key='X_umap', filter=None, **umap_kwargs)</code>","text":"<p>Run a UMAP on a specific set of markers (or all markers by default). It can be useful to show differences that are due to some markers of interest, instead of using the whole panel.</p> <p>Info</p> <p>This function returns a UMAP reducer. You can reuse it with <code>reducer.transform(...)</code> or save it with scyan.data.add.</p> <p>Note</p> <p>To actually plot the UMAP, use scyan.plot.umap.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required <code>markers</code> <code>Optional[List[str]]</code> <p>List marker names. By default, use all the panel markers, i.e., <code>adata.var_names</code>.</p> <code>None</code> <code>obsm</code> <code>Optional[str]</code> <p>Name of the obsm to consider to train the UMAP. By default, uses <code>adata.X</code>.</p> <code>None</code> <code>n_cells</code> <code>Optional[int]</code> <p>Number of cells to be considered for the UMAP (to accelerate it when \\(N\\) is very high). If <code>None</code>, consider all cells.</p> <code>200000</code> <code>min_dist</code> <code>float</code> <p>Min dist UMAP parameter.</p> <code>0.5</code> <code>obsm_key</code> <code>str</code> <p>Key for <code>adata.obsm</code> to add the embedding.</p> <code>'X_umap'</code> <code>filter</code> <code>Optional[Tuple]</code> <p>Optional tuple <code>(key, value)</code> used to train the UMAP on a set of cells that satisfies a constraint. <code>key</code> is the key of <code>adata.obs</code> to consider, and <code>value</code> the value the cells need to have.</p> <code>None</code> <code>**umap_kwargs</code> <code>int</code> <p>Optional kwargs to provide to the <code>UMAP</code> initialization.</p> <code>{}</code> <p>Returns:</p> Type Description <code>UMAP</code> <p>UMAP reducer.</p> Source code in <code>scyan/tools/representation.py</code> <pre><code>def umap(\n    adata: AnnData,\n    markers: Optional[List[str]] = None,\n    obsm: Optional[str] = None,\n    n_cells: Optional[int] = 200_000,\n    min_dist: float = 0.5,\n    obsm_key: str = \"X_umap\",\n    filter: Optional[Tuple] = None,\n    **umap_kwargs: int,\n) -&gt; UMAP:\n    \"\"\"Run a [UMAP](https://umap-learn.readthedocs.io/en/latest/) on a specific set of markers (or all markers by default). It can be useful to show differences that are due to some markers of interest, instead of using the whole panel.\n\n    !!! info\n\n        This function returns a UMAP reducer. You can reuse it with `reducer.transform(...)` or save it with [scyan.data.add][].\n\n    !!! note\n\n        To actually plot the UMAP, use [scyan.plot.umap][].\n\n    Args:\n        adata: An `AnnData` object.\n        markers: List marker names. By default, use all the panel markers, i.e., `adata.var_names`.\n        obsm: Name of the obsm to consider to train the UMAP. By default, uses `adata.X`.\n        n_cells: Number of cells to be considered for the UMAP (to accelerate it when $N$ is very high). If `None`, consider all cells.\n        min_dist: Min dist UMAP parameter.\n        obsm_key: Key for `adata.obsm` to add the embedding.\n        filter: Optional tuple `(key, value)` used to train the UMAP on a set of cells that satisfies a constraint. `key` is the key of `adata.obs` to consider, and `value` the value the cells need to have.\n        **umap_kwargs: Optional kwargs to provide to the `UMAP` initialization.\n\n    Returns:\n        UMAP reducer.\n    \"\"\"\n    reducer = UMAP(min_dist=min_dist, **umap_kwargs)\n\n    if markers is None:\n        markers = adata.var_names\n\n    adata.obsm[obsm_key] = np.zeros((adata.n_obs, 2))\n    indices = _get_subset_indices(adata.n_obs, n_cells)\n    adata_view = adata[indices, markers]\n    X = adata_view.X if obsm is None else adata_view.obsm[obsm]\n\n    _check_is_processed(X)\n\n    log.info(\"Fitting UMAP...\")\n    if filter is None:\n        embedding = reducer.fit_transform(X)\n    else:\n        key, value = filter\n        reducer.fit(X[adata[indices].obs[key] == value])\n        log.info(\"Transforming...\")\n        embedding = reducer.transform(X)\n\n    adata.obsm[obsm_key][indices] = embedding\n\n    return reducer\n</code></pre>"},{"location":"api/tools/#scyan.tools.leiden","title":"<code>scyan.tools.leiden(adata, resolution=1, key_added='leiden', n_neighbors=15)</code>","text":"<p>Leiden clustering</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>AnnData object.</p> required <code>resolution</code> <code>float</code> <p>Resolution of the clustering.</p> <code>1</code> <code>key_added</code> <code>str</code> <p>Name of the key of adata.obs where clusters will be saved.</p> <code>'leiden'</code> <code>n_neighbors</code> <code>int</code> <p>Number of neighbors.</p> <code>15</code> Source code in <code>scyan/tools/representation.py</code> <pre><code>def leiden(\n    adata: AnnData,\n    resolution: float = 1,\n    key_added: str = \"leiden\",\n    n_neighbors: int = 15,\n) -&gt; None:\n    \"\"\"Leiden clustering\n\n    Args:\n        adata: AnnData object.\n        resolution: Resolution of the clustering.\n        key_added: Name of the key of adata.obs where clusters will be saved.\n        n_neighbors: Number of neighbors.\n    \"\"\"\n    try:\n        import leidenalg\n    except:\n        raise ImportError(\n            \"\"\"To run leiden, you need to have 'leidenalg' installed. You can install the population discovery extra with \"pip install 'scyan[discovery]'\", or directly install leidenalg with \"conda install -c conda-forge leidenalg\".\"\"\"\n        )\n\n    import igraph as ig\n    from sklearn.neighbors import kneighbors_graph\n\n    if not \"knn_graph\" in adata.obsp:\n        adata.obsp[\"knn_graph\"] = kneighbors_graph(\n            adata.X, n_neighbors=n_neighbors, metric=\"euclidean\", include_self=False\n        )\n\n    # TODO (improvement): add weights according to euclidean distance\n    graph = ig.Graph.Weighted_Adjacency(adata.obsp[\"knn_graph\"], mode=\"DIRECTED\")\n\n    partition = leidenalg.find_partition(\n        graph,\n        leidenalg.RBConfigurationVertexPartition,\n        resolution_parameter=resolution,\n    )\n    adata.obs[key_added] = pd.Categorical([str(x) for x in partition.membership])\n</code></pre>"},{"location":"api/tools/#scyan.tools.subcluster","title":"<code>scyan.tools.subcluster(adata, population, markers=None, key='scyan_pop', resolution=0.2, size_ratio_th=0.02, min_cells_th=200, n_cells=100000)</code>","text":"<p>Create sub-clusters among a given populations, and filters small clusters according to (i) a minimum number of cells and (ii) a minimum ratio of cells.</p> <p>Info</p> <p>After having run this method, you can analyze the results with scyan.plot.umap and scyan.plot.pops_expressions.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required <code>population</code> <code>str</code> <p>Name of the population to target (one of <code>adata.obs[key]</code>).</p> required <code>markers</code> <code>Optional[List[str]]</code> <p>Optional list of markers used to create subclusters. By default, uses the complete panel.</p> <code>None</code> <code>key</code> <code>str</code> <p>Key to look for population in <code>adata.obs</code>. By default, uses the model predictions, but you can also choose a population level (if any), or other observations.</p> <code>'scyan_pop'</code> <code>resolution</code> <code>float</code> <p>Resolution used for leiden clustering. Higher resolution leads to more clusters.</p> <code>0.2</code> <code>size_ratio_th</code> <code>float</code> <p>(Only used if <code>population</code> is <code>None</code>): Minimum ratio of cells to be considered as a significant cluster (compared to the parent cluster).</p> <code>0.02</code> <code>min_cells_th</code> <code>int</code> <p>(Only used if <code>population</code> is <code>None</code>): Minimum number of cells to be considered as a significant cluster.</p> <code>200</code> <code>n_cells</code> <code>int</code> <p>Number of cells to be considered for the subclustering (to accelerate it when \\(N\\) is very high). If <code>None</code>, consider all cells.</p> <code>100000</code> Source code in <code>scyan/tools/representation.py</code> <pre><code>def subcluster(\n    adata: AnnData,\n    population: str,\n    markers: Optional[List[str]] = None,\n    key: str = \"scyan_pop\",\n    resolution: float = 0.2,\n    size_ratio_th: float = 0.02,\n    min_cells_th: int = 200,\n    n_cells: int = 100_000,\n) -&gt; None:\n    \"\"\"Create sub-clusters among a given populations, and filters small clusters according to (i) a minimum number of cells and (ii) a minimum ratio of cells.\n    !!! info\n        After having run this method, you can analyze the results with [scyan.plot.umap][] and [scyan.plot.pops_expressions][].\n\n    Args:\n        adata: An `AnnData` object.\n        population: Name of the population to target (one of `adata.obs[key]`).\n        markers: Optional list of markers used to create subclusters. By default, uses the complete panel.\n        key: Key to look for population in `adata.obs`. By default, uses the model predictions, but you can also choose a population level (if any), or other observations.\n        resolution: Resolution used for leiden clustering. Higher resolution leads to more clusters.\n        size_ratio_th: (Only used if `population` is `None`): Minimum ratio of cells to be considered as a significant cluster (compared to the parent cluster).\n        min_cells_th: (Only used if `population` is `None`): Minimum number of cells to be considered as a significant cluster.\n        n_cells: Number of cells to be considered for the subclustering (to accelerate it when $N$ is very high). If `None`, consider all cells.\n    \"\"\"\n    leiden_key = f\"leiden_{resolution}_{population}\"\n    subcluster_key = f\"scyan_subcluster_{population}\"\n    condition = adata.obs[key] == population\n    markers = list(adata.var_names if markers is None else markers)\n\n    if leiden_key in adata.obs and adata.uns.get(leiden_key, []) == markers:\n        log.info(\n            \"Found leiden labels with the same resolution. Skipping leiden clustering.\"\n        )\n        indices = np.where(~adata.obs[leiden_key].isna())[0]\n        adata_sub = adata[indices, markers].copy()\n    else:\n        has_umap = _has_umap(adata)\n        if has_umap.all() or condition.sum() &lt;= n_cells:\n            indices = _get_subset_indices(condition.sum(), n_cells)\n            indices = np.where(condition)[0][indices]\n        else:\n            indices = _get_subset_indices((condition &amp; has_umap).sum(), n_cells)\n            indices = np.where(condition &amp; has_umap)[0][indices]\n\n            k = len(indices)\n            if k &lt; n_cells:\n                indices2 = _get_subset_indices((condition &amp; ~has_umap).sum(), n_cells - k)\n                indices2 = np.where(condition &amp; ~has_umap)[0][indices2]\n                indices = np.concatenate([indices, indices2])\n\n        adata_sub = adata[indices, markers].copy()\n\n        leiden(adata_sub, resolution, leiden_key)\n\n    series = pd.Series(index=np.arange(adata.n_obs), dtype=str)\n    series[indices] = adata_sub.obs[leiden_key].values\n    adata.obs[leiden_key] = series.values\n    adata.obs[leiden_key] = adata.obs[leiden_key].astype(\"category\")\n\n    counts = adata_sub.obs[leiden_key].value_counts()\n    remove = counts &lt; max(counts.sum() * size_ratio_th, min_cells_th)\n\n    assert (\n        not remove.all()\n    ), \"All subclusters where filtered. Consider updating size_ratio_th and/or min_cells_th.\"\n\n    adata_sub.obs.loc[\n        np.isin(adata_sub.obs[leiden_key], remove[remove].index), leiden_key\n    ] = np.nan\n\n    series = pd.Series(index=np.arange(adata.n_obs), dtype=str)\n    series[indices] = adata_sub.obs[leiden_key].values\n    adata.obs[subcluster_key] = series.values\n    adata.obs[subcluster_key] = adata.obs[subcluster_key].astype(\"category\")\n\n    adata.uns[leiden_key] = markers\n    log.info(\n        f\"Subclusters created, you can now use:\\n   - scyan.plot.umap(adata, color='{subcluster_key}') to show the clusters\\n   - scyan.plot.pops_expressions(model, key='{subcluster_key}') to plot their expressions\"\n    )\n</code></pre>"},{"location":"api/tools/#scyan.tools.palette_level","title":"<code>scyan.tools.palette_level(table, population_index=0, level_index=1, hue_shift=0.4, alpha_l=0.25, step_l=0.15, alpha_s=0.3, step_s=0.4)</code>","text":"<p>Computes a color palette that in grouped by the hierarchical main populations. It improves the UMAP readability when many populations are defined.</p> <p>Info</p> <p>Once such a color palette is defined, you can use it for plotting. For instance, try <code>scyan.plot.umap(adata, color=\"scyan_pop\", palette=palette)</code>, where <code>palette</code> is the one you created with this function.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>DataFrame</code> <p>Knowledge table provided to Scyan. It must be a multi-index DataFrame.</p> required <code>population_index</code> <code>Union[int, str]</code> <p>Index or name of the level in <code>table.index</code> storing the low-level/children population names.</p> <code>0</code> <code>level_index</code> <code>Union[int, str]</code> <p>Index or name of the level in <code>table.index</code> storing the main population names.</p> <code>1</code> <code>hue_shift</code> <code>float</code> <p>Shift the hue values. The value must be a float in <code>[0, 1]</code>.</p> <code>0.4</code> <code>alpha_l</code> <code>float</code> <p>Lower it to have a larger lightness range of colors.</p> <code>0.25</code> <code>step_l</code> <code>float</code> <p>Increase it to have more distinct colors (in term of lightness).</p> <code>0.15</code> <code>alpha_s</code> <code>float</code> <p>Lower it to have a larger saturation range of colors.</p> <code>0.3</code> <code>step_s</code> <code>float</code> <p>Increase it to have more distinct colors (in term of saturation).</p> <code>0.4</code> <p>Returns:</p> Type Description <code>Dict[str, Tuple[float]]</code> <p>A dictionnary whose keys are population names and values are RGB colors.</p> Source code in <code>scyan/tools/colors.py</code> <pre><code>def palette_level(\n    table: pd.DataFrame,\n    population_index: Union[int, str] = 0,\n    level_index: Union[int, str] = 1,\n    hue_shift: float = 0.4,\n    alpha_l: float = 0.25,\n    step_l: float = 0.15,\n    alpha_s: float = 0.3,\n    step_s: float = 0.4,\n) -&gt; Dict[str, Tuple[float]]:\n    \"\"\"Computes a color palette that in grouped by the hierarchical main populations. It improves the UMAP readability when many populations are defined.\n\n    !!! info\n        Once such a color palette is defined, you can use it for plotting. For instance, try `scyan.plot.umap(adata, color=\"scyan_pop\", palette=palette)`, where `palette` is the one you created with this function.\n\n    Args:\n        table: Knowledge table provided to Scyan. It must be a multi-index DataFrame.\n        population_index: Index or name of the level in `table.index` storing the low-level/children population names.\n        level_index: Index or name of the level in `table.index` storing the main population names.\n        hue_shift: Shift the hue values. The value must be a float in `[0, 1]`.\n        alpha_l: Lower it to have a larger lightness range of colors.\n        step_l: Increase it to have more distinct colors (in term of lightness).\n        alpha_s: Lower it to have a larger saturation range of colors.\n        step_s: Increase it to have more distinct colors (in term of saturation).\n\n    Returns:\n        A dictionnary whose keys are population names and values are RGB colors.\n    \"\"\"\n    assert isinstance(\n        table.index, pd.MultiIndex\n    ), f\"The provided table has no multi-index. To work with hierarchical populations, consider reading https://mics-lab.github.io/scyan/tutorials/usage/#working-with-hierarchical-populations\"\n\n    pops = table.index.get_level_values(population_index).values\n    level = table.index.get_level_values(level_index)\n    level_counts = level.value_counts()\n\n    group_palette = GroupPalette(alpha_l, step_l, alpha_s, step_s)\n    color_groups = group_palette(level_counts.values, hue_shift)\n\n    block_indices = [level_counts.index.get_loc(pop) for pop in level]\n    s = pd.Series(level)\n    inner_block_indices = s.groupby(s).cumcount().values\n\n    return {\n        pop: list(color_groups[block_index][inner_index])\n        for pop, block_index, inner_index in zip(pops, block_indices, inner_block_indices)\n    }\n</code></pre>"},{"location":"api/tools/#scyan.tools.cell_type_ratios","title":"<code>scyan.tools.cell_type_ratios(adata, groupby=None, normalize=True, key='scyan_pop', among=None)</code>","text":"<p>Computes the ratio of cells per population. This ratio can be provided for each patient (or for any kind of 'group').</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required <code>groupby</code> <code>Union[str, List[str], None]</code> <p>Key(s) of <code>adata.obs</code> used to create groups (e.g. the patient ID).</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If <code>False</code>, returns counts instead of ratios. If <code>\"%\"</code>, use percentage instead of ratios in <code>[0, 1]</code>;</p> <code>True</code> <code>key</code> <code>str</code> <p>Key of <code>adata.obs</code> containing the population names (or the values to count).</p> <code>'scyan_pop'</code> <code>among</code> <code>str</code> <p>Key of <code>adata.obs</code> containing the parent population name. Typically, if using hierarchical populations, you can provide <code>'scyan_pop_level'</code> with your level name. E.g., if the parent of population of \"T CD4 RM\" is called \"T cells\" in <code>adata.obs[among]</code>, then this function computes the 'T CD4 RM ratio among T cells'.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame of ratios or counts (one row per group, one column per population). If <code>normalize=False</code>, then each row sums to 1 (for <code>among=None</code>).</p> Source code in <code>scyan/tools/biomarkers.py</code> <pre><code>def cell_type_ratios(\n    adata: AnnData,\n    groupby: Union[str, List[str], None] = None,\n    normalize: bool = True,\n    key: str = \"scyan_pop\",\n    among: str = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Computes the ratio of cells per population. This ratio can be provided for each patient (or for any kind of 'group').\n\n    Args:\n        adata: An `AnnData` object.\n        groupby: Key(s) of `adata.obs` used to create groups (e.g. the patient ID).\n        normalize: If `False`, returns counts instead of ratios. If `\"%\"`, use percentage instead of ratios in `[0, 1]`;\n        key: Key of `adata.obs` containing the population names (or the values to count).\n        among: Key of `adata.obs` containing the parent population name. Typically, if using hierarchical populations, you can provide `'scyan_pop_level'` with your level name. E.g., if the parent of population of \"T CD4 RM\" is called \"T cells\" in `adata.obs[among]`, then this function computes the 'T CD4 RM ratio among T cells'.\n\n    Returns:\n        A DataFrame of ratios or counts (one row per group, one column per population). If `normalize=False`, then each row sums to 1 (for `among=None`).\n    \"\"\"\n    assert (\n        among is None or normalize\n    ), \"If 'among' is `None`, then normalize can't be `False`\"\n\n    column_suffix = (\n        (\"percentage\" if normalize == \"%\" else \"ratio\") if normalize else \"count\"\n    )\n\n    counts = _get_counts(adata, groupby, key, normalize)\n\n    if among is None:\n        counts.columns = [f\"{name} {column_suffix}\" for name in counts.columns]\n        return counts.mul(100) if normalize == \"%\" else counts\n\n    parents_count = _get_counts(adata, groupby, among, normalize)\n\n    df_parent = adata.obs.groupby(among)[key].apply(lambda s: s.value_counts()).unstack()\n    assert (\n        (df_parent &gt; 0).sum(0) &lt;= 1\n    ).all(), f\"Each population from adata.obs['{key}'] should have only one parent population in adata.obs['{among}']\"\n    to_parent_dict = dict(df_parent.idxmax())\n\n    counts /= parents_count[[to_parent_dict[pop] for pop in counts.columns]].values\n    counts.columns = [\n        f\"{pop} {column_suffix} among {to_parent_dict[pop]}\" for pop in counts.columns\n    ]\n    return counts.mul(100) if normalize == \"%\" else counts\n</code></pre>"},{"location":"api/tools/#scyan.tools.mean_intensities","title":"<code>scyan.tools.mean_intensities(adata, groupby=None, layer=None, key='scyan_pop', unstack_join=' mean intensity on ', obsm=None, obsm_names=None)</code>","text":"<p>Compute the Mean Metal Intensity (MMI) or Mean Fluorescence Intensity (MFI) per population. If needed, mean intensities can be computed per group (e.g., per patient) by providing the <code>groupby</code> argument.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required <code>groupby</code> <code>Union[str, List[str], None]</code> <p>Key(s) of <code>adata.obs</code> used to create groups. For instance, <code>\"id\"</code> computes MMI per population for each ID. You can also provide something like <code>[\"group\", \"id\"]</code> to get MMI per group, and per patient inside each group.</p> <code>None</code> <code>layer</code> <code>Optional[str]</code> <p>In which <code>adata.layers</code> we get expression intensities. By default, it uses <code>adata.X</code>.</p> <code>None</code> <code>key</code> <code>str</code> <p>Key of <code>adata.obs</code> containing the population names.</p> <code>'scyan_pop'</code> <code>unstack_join</code> <code>Optional[str]</code> <p>If <code>None</code>, keep the information grouped. Else, flattens the biomarkers into one series (or one row per group if <code>groupby</code> is a list) and uses <code>unstack_join</code> to join the names of the multi-level columns. For instance, <code>' expression on '</code> can be a good choice.</p> <code>' mean intensity on '</code> <code>obsm</code> <code>Optional[str]</code> <p>In which <code>adata.obsm</code> we get expression intensities. By default, it uses <code>adata.X</code>. If not <code>None</code> then <code>obsm_names</code> is required too.</p> <code>None</code> <code>obsm_names</code> <code>Optional[List[str]]</code> <p>Ordered list of names in <code>adata.obsm[obsm]</code> if <code>obsm</code> was provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame of MFI. If <code>groupby</code> was a list, it is a multi-index dataframe.</p> Source code in <code>scyan/tools/biomarkers.py</code> <pre><code>def mean_intensities(\n    adata: AnnData,\n    groupby: Union[str, List[str], None] = None,\n    layer: Optional[str] = None,\n    key: str = \"scyan_pop\",\n    unstack_join: Optional[str] = \" mean intensity on \",\n    obsm: Optional[str] = None,\n    obsm_names: Optional[List[str]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute the Mean Metal Intensity (MMI) or Mean Fluorescence Intensity (MFI) per population. If needed, mean intensities can be computed per group (e.g., per patient) by providing the `groupby` argument.\n\n    Args:\n        adata: An `AnnData` object.\n        groupby: Key(s) of `adata.obs` used to create groups. For instance, `\"id\"` computes MMI per population for each ID. You can also provide something like `[\"group\", \"id\"]` to get MMI per group, and per patient inside each group.\n        layer: In which `adata.layers` we get expression intensities. By default, it uses `adata.X`.\n        key: Key of `adata.obs` containing the population names.\n        unstack_join: If `None`, keep the information grouped. Else, flattens the biomarkers into one series (or one row per group if `groupby` is a list) and uses `unstack_join` to join the names of the multi-level columns. For instance, `' expression on '` can be a good choice.\n        obsm: In which `adata.obsm` we get expression intensities. By default, it uses `adata.X`. If not `None` then `obsm_names` is required too.\n        obsm_names: Ordered list of names in `adata.obsm[obsm]` if `obsm` was provided.\n\n    Returns:\n        A DataFrame of MFI. If `groupby` was a list, it is a multi-index dataframe.\n    \"\"\"\n    if groupby is None:\n        groupby = [key]\n    elif isinstance(groupby, str):\n        groupby = [groupby, key]\n    else:\n        groupby = list(groupby) + [key]\n\n    if obsm is not None:\n        assert (\n            layer is None\n        ), \"You must choose between 'obsm' and 'layer', do not use both.\"\n\n        df = pd.DataFrame(data=adata.obsm[obsm], columns=obsm_names)\n    else:\n        df = adata.to_df(layer)\n\n    for group in groupby:\n        df[group] = adata.obs[group].values\n\n    res = df.groupby(groupby).mean().dropna(how=\"all\")\n\n    if res.values.min() &lt; 0:\n        log.warning(\n            \"The minimum expression value is negative. Are you sure you are using unscaled values? If not, you can use 'scyan.preprocess.unscale' and save the unscaled result in a 'adata.layers' of your choice (then use this layer argument in the current function). If you know what you are doing, or if you use flow cytometry data, you can ignore this warning.\"\n        )\n\n    if unstack_join is None:\n        return res\n\n    res = res.unstack(level=-1)\n    if isinstance(res, pd.Series):\n        res.index = [unstack_join.join(row).strip() for row in res.index.values]\n    else:\n        res.columns = [unstack_join.join(col).strip() for col in res.columns.values]\n    return res\n</code></pre>"},{"location":"api/tools/#scyan.tools.PolygonGatingUMAP","title":"<code>scyan.tools.PolygonGatingUMAP</code>","text":"<p>Class used to select cells on a UMAP using polygons.</p> <p>Note</p> <p>If used on a Jupyter Notebook, you should first run <code>%matplotlib tk</code>. After the selection, you can run <code>%matplotlib inline</code> to retrieve the default behavior.</p> <pre><code># Usage example (`%matplotlib tk` is required for the cell selection on jupyter notebooks)\n&gt;&gt;&gt; %matplotlib tk\n&gt;&gt;&gt; selector = scyan.tools.PolygonGatingUMAP(adata)\n&gt;&gt;&gt; selector.select()         # select the cells\n\n&gt;&gt;&gt; sub_adata = selector.extract_adata() # on a notebook, this has to be on a new jupyter cell\n</code></pre> Source code in <code>scyan/tools/gating.py</code> <pre><code>class PolygonGatingUMAP:\n    \"\"\"Class used to select cells on a UMAP using polygons.\n\n    !!! note\n\n        If used on a Jupyter Notebook, you should first run `%matplotlib tk`. After the selection, you can run `%matplotlib inline` to retrieve the default behavior.\n\n    ```py\n    # Usage example (`%matplotlib tk` is required for the cell selection on jupyter notebooks)\n    &gt;&gt;&gt; %matplotlib tk\n    &gt;&gt;&gt; selector = scyan.tools.PolygonGatingUMAP(adata)\n    &gt;&gt;&gt; selector.select()         # select the cells\n\n    &gt;&gt;&gt; sub_adata = selector.extract_adata() # on a notebook, this has to be on a new jupyter cell\n    ```\n    \"\"\"\n\n    def __init__(self, adata: AnnData) -&gt; None:\n        \"\"\"\n        Args:\n            adata: An `AnnData` object.\n        \"\"\"\n        self.adata = adata\n        self.has_umap = _has_umap(adata)\n        self.x_umap = self.adata.obsm[\"X_umap\"]\n\n    def select(self, s: float = 0.05) -&gt; None:\n        \"\"\"Open a UMAP plot on which you can draw a polygon to select cells.\n\n        Args:\n            s: Size of the cells on the plot.\n        \"\"\"\n        _, ax = plt.subplots()\n\n        pts = ax.scatter(\n            self.x_umap[self.has_umap, 0],\n            self.x_umap[self.has_umap, 1],\n            marker=\".\",\n            rasterized=True,\n            s=s,\n        )\n\n        self.selector = _SelectFromCollection(ax, pts, self.x_umap[self.has_umap])\n\n        log.info(\n            f\"Enclose cells within a polygon. Helper:\\n    - Click on the plot to add a polygon vertex\\n    - Press the 'esc' key to start a new polygon\\n    - Try holding the 'ctrl' key to move a single vertex\\n    - Once the polygon is finished and overlaid in red, you can close the window\"\n        )\n        plt.show()\n\n    def save_selection(self, key_added: str = \"scyan_selected\"):\n        \"\"\"Save the selected cells in `adata.obs[key_added]`.\n\n        Args:\n            key_added: Column name used to save the selected cells in `adata.obs`.\n        \"\"\"\n        self.adata.obs[key_added] = \"unselected\"\n        col_index = self.adata.obs.columns.get_loc(key_added)\n        self.adata.obs.iloc[\n            np.where(self.has_umap)[0][self.selector.ind], col_index\n        ] = \"selected\"\n        self.adata.obs[key_added] = self.adata.obs[key_added].astype(\"category\")\n\n        self.selector.disconnect()\n        log.info(\n            f\"Selected {len(self.selector.ind)} cells and saved the selection in adata.obs['{key_added}']\"\n        )\n\n    def extract_adata(self) -&gt; AnnData:\n        \"\"\"Returns an anndata objects whose cells where inside the polygon\"\"\"\n        log.info(f\"Selected {len(self.selector.ind)} cells\")\n        self.selector.disconnect()\n\n        return self.adata[np.where(self.has_umap)[0][self.selector.ind]]\n</code></pre>"},{"location":"api/tools/#scyan.tools.PolygonGatingUMAP.__init__","title":"<code>__init__(adata)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required Source code in <code>scyan/tools/gating.py</code> <pre><code>def __init__(self, adata: AnnData) -&gt; None:\n    \"\"\"\n    Args:\n        adata: An `AnnData` object.\n    \"\"\"\n    self.adata = adata\n    self.has_umap = _has_umap(adata)\n    self.x_umap = self.adata.obsm[\"X_umap\"]\n</code></pre>"},{"location":"api/tools/#scyan.tools.PolygonGatingUMAP.select","title":"<code>select(s=0.05)</code>","text":"<p>Open a UMAP plot on which you can draw a polygon to select cells.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>float</code> <p>Size of the cells on the plot.</p> <code>0.05</code> Source code in <code>scyan/tools/gating.py</code> <pre><code>def select(self, s: float = 0.05) -&gt; None:\n    \"\"\"Open a UMAP plot on which you can draw a polygon to select cells.\n\n    Args:\n        s: Size of the cells on the plot.\n    \"\"\"\n    _, ax = plt.subplots()\n\n    pts = ax.scatter(\n        self.x_umap[self.has_umap, 0],\n        self.x_umap[self.has_umap, 1],\n        marker=\".\",\n        rasterized=True,\n        s=s,\n    )\n\n    self.selector = _SelectFromCollection(ax, pts, self.x_umap[self.has_umap])\n\n    log.info(\n        f\"Enclose cells within a polygon. Helper:\\n    - Click on the plot to add a polygon vertex\\n    - Press the 'esc' key to start a new polygon\\n    - Try holding the 'ctrl' key to move a single vertex\\n    - Once the polygon is finished and overlaid in red, you can close the window\"\n    )\n    plt.show()\n</code></pre>"},{"location":"api/tools/#scyan.tools.PolygonGatingUMAP.save_selection","title":"<code>save_selection(key_added='scyan_selected')</code>","text":"<p>Save the selected cells in <code>adata.obs[key_added]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>key_added</code> <code>str</code> <p>Column name used to save the selected cells in <code>adata.obs</code>.</p> <code>'scyan_selected'</code> Source code in <code>scyan/tools/gating.py</code> <pre><code>def save_selection(self, key_added: str = \"scyan_selected\"):\n    \"\"\"Save the selected cells in `adata.obs[key_added]`.\n\n    Args:\n        key_added: Column name used to save the selected cells in `adata.obs`.\n    \"\"\"\n    self.adata.obs[key_added] = \"unselected\"\n    col_index = self.adata.obs.columns.get_loc(key_added)\n    self.adata.obs.iloc[\n        np.where(self.has_umap)[0][self.selector.ind], col_index\n    ] = \"selected\"\n    self.adata.obs[key_added] = self.adata.obs[key_added].astype(\"category\")\n\n    self.selector.disconnect()\n    log.info(\n        f\"Selected {len(self.selector.ind)} cells and saved the selection in adata.obs['{key_added}']\"\n    )\n</code></pre>"},{"location":"api/tools/#scyan.tools.PolygonGatingUMAP.extract_adata","title":"<code>extract_adata()</code>","text":"<p>Returns an anndata objects whose cells where inside the polygon</p> Source code in <code>scyan/tools/gating.py</code> <pre><code>def extract_adata(self) -&gt; AnnData:\n    \"\"\"Returns an anndata objects whose cells where inside the polygon\"\"\"\n    log.info(f\"Selected {len(self.selector.ind)} cells\")\n    self.selector.disconnect()\n\n    return self.adata[np.where(self.has_umap)[0][self.selector.ind]]\n</code></pre>"},{"location":"api/tools/#scyan.tools.PolygonGatingScatter","title":"<code>scyan.tools.PolygonGatingScatter</code>","text":"<p>Class used to select cells on a scatterplot using polygons.</p> <p>Note</p> <p>If used on a Jupyter Notebook, you should first run <code>%matplotlib tk</code>. After the selection, you can run <code>%matplotlib inline</code> to retrieve the default behavior.</p> <pre><code># Usage example (`%matplotlib tk` is required for the cell selection on jupyter notebooks)\n&gt;&gt;&gt; %matplotlib tk\n&gt;&gt;&gt; selector = scyan.tools.PolygonGatingScatter(adata)\n&gt;&gt;&gt; selector.select()         # select the cells\n\n&gt;&gt;&gt; sub_adata = selector.extract_adata() # on a notebook, this has to be on a new jupyter cell\n</code></pre> Source code in <code>scyan/tools/gating.py</code> <pre><code>class PolygonGatingScatter:\n    \"\"\"Class used to select cells on a scatterplot using polygons.\n\n    !!! note\n\n        If used on a Jupyter Notebook, you should first run `%matplotlib tk`. After the selection, you can run `%matplotlib inline` to retrieve the default behavior.\n\n    ```py\n    # Usage example (`%matplotlib tk` is required for the cell selection on jupyter notebooks)\n    &gt;&gt;&gt; %matplotlib tk\n    &gt;&gt;&gt; selector = scyan.tools.PolygonGatingScatter(adata)\n    &gt;&gt;&gt; selector.select()         # select the cells\n\n    &gt;&gt;&gt; sub_adata = selector.extract_adata() # on a notebook, this has to be on a new jupyter cell\n    ```\n    \"\"\"\n\n    def __init__(self, adata: AnnData) -&gt; None:\n        \"\"\"\n        Args:\n            adata: An `AnnData` object.\n        \"\"\"\n        self.adata = adata\n\n    def select(\n        self, x: str, y: str, s: float = 0.05, max_cells_display: int = 100_000\n    ) -&gt; None:\n        \"\"\"Open a scatter plot on which you can draw a polygon to select cells.\n\n        Args:\n            x: Column name of adata.obs used for the x-axis\n            y: Column name of adata.obs used for the y-axis\n            s: Size of the cells on the plot.\n        \"\"\"\n        _, ax = plt.subplots()\n\n        indices = np.arange(self.adata.n_obs)\n        if max_cells_display is not None and max_cells_display &lt; self.adata.n_obs:\n            indices = np.random.choice(\n                np.arange(self.adata.n_obs), size=max_cells_display, replace=False\n            )\n\n        x = self.adata.obs_vector(x)\n        y = self.adata.obs_vector(y)\n        xy = np.stack([x, y], axis=1)\n\n        pts = ax.scatter(\n            xy[indices, 0],\n            xy[indices, 1],\n            marker=\".\",\n            rasterized=True,\n            s=s,\n        )\n\n        self.selector = _SelectFromCollection(ax, pts, xy)\n\n        log.info(\n            f\"Enclose cells within a polygon. Helper:\\n    - Click on the plot to add a polygon vertex\\n    - Press the 'esc' key to start a new polygon\\n    - Try holding the 'ctrl' key to move a single vertex\\n    - Once the polygon is finished and overlaid in red, you can close the window\"\n        )\n        plt.show()\n\n    def save_selection(self, key_added: str = \"scyan_selected\"):\n        \"\"\"Save the selected cells in `adata.obs[key_added]`.\n\n        Args:\n            key_added: Column name used to save the selected cells in `adata.obs`.\n        \"\"\"\n        self.adata.obs[key_added] = \"unselected\"\n        col_index = self.adata.obs.columns.get_loc(key_added)\n        self.adata.obs.iloc[self.selector.ind, col_index] = \"selected\"\n        self.adata.obs[key_added] = self.adata.obs[key_added].astype(\"category\")\n\n        self.selector.disconnect()\n        log.info(\n            f\"Selected {len(self.selector.ind)} cells and saved the selection in adata.obs['{key_added}']\"\n        )\n\n    def extract_adata(self) -&gt; AnnData:\n        \"\"\"Returns an anndata objects whose cells where inside the polygon\"\"\"\n        log.info(f\"Selected {len(self.selector.ind)} cells\")\n        self.selector.disconnect()\n\n        return self.adata[self.selector.ind]\n</code></pre>"},{"location":"api/tools/#scyan.tools.PolygonGatingScatter.__init__","title":"<code>__init__(adata)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An <code>AnnData</code> object.</p> required Source code in <code>scyan/tools/gating.py</code> <pre><code>def __init__(self, adata: AnnData) -&gt; None:\n    \"\"\"\n    Args:\n        adata: An `AnnData` object.\n    \"\"\"\n    self.adata = adata\n</code></pre>"},{"location":"api/tools/#scyan.tools.PolygonGatingScatter.select","title":"<code>select(x, y, s=0.05, max_cells_display=100000)</code>","text":"<p>Open a scatter plot on which you can draw a polygon to select cells.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>Column name of adata.obs used for the x-axis</p> required <code>y</code> <code>str</code> <p>Column name of adata.obs used for the y-axis</p> required <code>s</code> <code>float</code> <p>Size of the cells on the plot.</p> <code>0.05</code> Source code in <code>scyan/tools/gating.py</code> <pre><code>def select(\n    self, x: str, y: str, s: float = 0.05, max_cells_display: int = 100_000\n) -&gt; None:\n    \"\"\"Open a scatter plot on which you can draw a polygon to select cells.\n\n    Args:\n        x: Column name of adata.obs used for the x-axis\n        y: Column name of adata.obs used for the y-axis\n        s: Size of the cells on the plot.\n    \"\"\"\n    _, ax = plt.subplots()\n\n    indices = np.arange(self.adata.n_obs)\n    if max_cells_display is not None and max_cells_display &lt; self.adata.n_obs:\n        indices = np.random.choice(\n            np.arange(self.adata.n_obs), size=max_cells_display, replace=False\n        )\n\n    x = self.adata.obs_vector(x)\n    y = self.adata.obs_vector(y)\n    xy = np.stack([x, y], axis=1)\n\n    pts = ax.scatter(\n        xy[indices, 0],\n        xy[indices, 1],\n        marker=\".\",\n        rasterized=True,\n        s=s,\n    )\n\n    self.selector = _SelectFromCollection(ax, pts, xy)\n\n    log.info(\n        f\"Enclose cells within a polygon. Helper:\\n    - Click on the plot to add a polygon vertex\\n    - Press the 'esc' key to start a new polygon\\n    - Try holding the 'ctrl' key to move a single vertex\\n    - Once the polygon is finished and overlaid in red, you can close the window\"\n    )\n    plt.show()\n</code></pre>"},{"location":"api/tools/#scyan.tools.PolygonGatingScatter.save_selection","title":"<code>save_selection(key_added='scyan_selected')</code>","text":"<p>Save the selected cells in <code>adata.obs[key_added]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>key_added</code> <code>str</code> <p>Column name used to save the selected cells in <code>adata.obs</code>.</p> <code>'scyan_selected'</code> Source code in <code>scyan/tools/gating.py</code> <pre><code>def save_selection(self, key_added: str = \"scyan_selected\"):\n    \"\"\"Save the selected cells in `adata.obs[key_added]`.\n\n    Args:\n        key_added: Column name used to save the selected cells in `adata.obs`.\n    \"\"\"\n    self.adata.obs[key_added] = \"unselected\"\n    col_index = self.adata.obs.columns.get_loc(key_added)\n    self.adata.obs.iloc[self.selector.ind, col_index] = \"selected\"\n    self.adata.obs[key_added] = self.adata.obs[key_added].astype(\"category\")\n\n    self.selector.disconnect()\n    log.info(\n        f\"Selected {len(self.selector.ind)} cells and saved the selection in adata.obs['{key_added}']\"\n    )\n</code></pre>"},{"location":"api/tools/#scyan.tools.PolygonGatingScatter.extract_adata","title":"<code>extract_adata()</code>","text":"<p>Returns an anndata objects whose cells where inside the polygon</p> Source code in <code>scyan/tools/gating.py</code> <pre><code>def extract_adata(self) -&gt; AnnData:\n    \"\"\"Returns an anndata objects whose cells where inside the polygon\"\"\"\n    log.info(f\"Selected {len(self.selector.ind)} cells\")\n    self.selector.disconnect()\n\n    return self.adata[self.selector.ind]\n</code></pre>"},{"location":"tutorials/batch_effect_correction/","title":"Batch effect correction","text":"In\u00a0[1]: Copied! <pre>import scyan\n</pre> import scyan <pre>Global seed set to 0\n</pre> In\u00a0[2]: Copied! <pre>adata, table = scyan.data.load(\"poised\") # Automatic data loading\n</pre> adata, table = scyan.data.load(\"poised\") # Automatic data loading In\u00a0[3]: Copied! <pre>import numpy as np\n\nnoise = np.random.exponential(\n    scale=1, size=(7, adata.n_vars, adata.n_vars)\n)\n\nadata.X = scyan.preprocess.unscale(adata)\nadata.X = scyan.preprocess.inverse_transform(adata)\n\nfor i, batch in enumerate(adata.obs.batch.cat.categories):\n    spillover = np.eye(adata.n_vars) + 0.01 * noise[i]\n    adata.X[adata.obs.batch == batch] = (\n        adata.X[adata.obs.batch == batch] @ spillover\n    )\n\nscyan.preprocess.asinh_transform(adata)\nscyan.preprocess.scale(adata)\n</pre> import numpy as np  noise = np.random.exponential(     scale=1, size=(7, adata.n_vars, adata.n_vars) )  adata.X = scyan.preprocess.unscale(adata) adata.X = scyan.preprocess.inverse_transform(adata)  for i, batch in enumerate(adata.obs.batch.cat.categories):     spillover = np.eye(adata.n_vars) + 0.01 * noise[i]     adata.X[adata.obs.batch == batch] = (         adata.X[adata.obs.batch == batch] @ spillover     )  scyan.preprocess.asinh_transform(adata) scyan.preprocess.scale(adata) <pre>[INFO] (scyan.preprocess) Performing inverse asinh transform\n[INFO] (scyan.preprocess) Data will be standardised, and translated so that 0 goes to -1. This is advised only when using CyTOF data (if this is not your case, consider running 'auto_logicle_transform' instead of 'asinh_transform').\n</pre> In\u00a0[4]: Copied! <pre>scyan.tools.umap(adata, markers=table.columns)\n</pre> scyan.tools.umap(adata, markers=table.columns) <pre>[INFO] (scyan.tools.representation) Fitting UMAP...\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n</pre> Out[4]: <pre>UMAP(min_dist=0.5, tqdm_kwds={'bar_format': '{desc}: {percentage:3.0f}%| {bar} {n_fmt}/{total_fmt} [{elapsed}]', 'desc': 'Epochs completed', 'disable': True})</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.UMAP<pre>UMAP(min_dist=0.5, tqdm_kwds={'bar_format': '{desc}: {percentage:3.0f}%| {bar} {n_fmt}/{total_fmt} [{elapsed}]', 'desc': 'Epochs completed', 'disable': True})</pre> In\u00a0[5]: Copied! <pre>scyan.plot.umap(adata, color=\"batch\", title=\"UMAP before batch correction\")\n</pre> scyan.plot.umap(adata, color=\"batch\", title=\"UMAP before batch correction\") In\u00a0[6]: Copied! <pre># Only 'batch_key' is required for batch correction\nmodel = scyan.Scyan(adata, table, batch_key=\"batch\", prior_std=0.35)\n\n# For better batch effect correction, you could increase the default 'patience' and decrease 'min_delta' arguments of model.fit()\n# NB: if you have less than 1 million cells, you can also increase 'max_epochs'\nmodel.fit(patience=10, min_delta=0.1)\n</pre> # Only 'batch_key' is required for batch correction model = scyan.Scyan(adata, table, batch_key=\"batch\", prior_std=0.35)  # For better batch effect correction, you could increase the default 'patience' and decrease 'min_delta' arguments of model.fit() # NB: if you have less than 1 million cells, you can also increase 'max_epochs' model.fit(patience=10, min_delta=0.1) <pre>[INFO] (scyan.model) Initialized Scyan model with N=4178320 cells, P=26 populations and M=19 markers.\n   \u251c\u2500\u2500 Covariates: batch\n   \u251c\u2500\u2500 No continuum-marker provided\n   \u2514\u2500\u2500 Batch correction mode: True\n[INFO] (scyan.model) Training scyan with the following hyperparameters:\n\"batch_key\":       batch\n\"batch_size\":      8192\n\"hidden_size\":     16\n\"lr\":              0.0005\n\"max_samples\":     200000\n\"modulo_temp\":     3\n\"n_hidden_layers\": 6\n\"n_layers\":        7\n\"prior_std\":       0.35\n\"temperature\":     0.5\n\nGPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/Users/quentinblampey/Library/Caches/pypoetry/virtualenvs/scyan-5lsXrWE1-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n  rank_zero_warn(\n\n  | Name   | Type        | Params\n---------------------------------------\n0 | module | ScyanModule | 33.4 K\n---------------------------------------\n33.4 K    Trainable params\n0         Non-trainable params\n33.4 K    Total params\n0.134     Total estimated model params size (MB)\n</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=100` reached.\n[INFO] (scyan.model) Successfully ended traning.\n</pre> Out[6]: <pre>Scyan model with N=4178320 cells, P=26 populations and M=19 markers.\n   \u251c\u2500\u2500 Covariates: batch\n   \u251c\u2500\u2500 No continuum-marker provided\n   \u2514\u2500\u2500 Batch correction mode: True</pre> <p>After model training, we can align the distribution on a reference batch (here, <code>B10</code>).</p> <p>Note that corrected expressions are still preprocess and scaled. You can inserve these transformations with <code>scyan.preprocess.unscale</code> and <code>scyan.preprocess.inverse_transform</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Get corrected marker expressions (warning: the values are still preprocessed and standardised)\nadata.obsm[\"scyan_corrected\"] = model.batch_effect_correction().numpy(force=True)\n</pre> # Get corrected marker expressions (warning: the values are still preprocessed and standardised) adata.obsm[\"scyan_corrected\"] = model.batch_effect_correction().numpy(force=True) In\u00a0[\u00a0]: Copied! <pre>model.predict()\nmodel.refine_fit()\n</pre> model.predict() model.refine_fit() <p>Then run everything again</p> In\u00a0[11]: Copied! <pre>adata.obsm[\"scyan_corrected\"] = model.batch_effect_correction(batch_ref=\"B10\").numpy(force=True)\nscyan.tools.umap(adata, obsm=\"scyan_corrected\");\nscyan.plot.umap(adata, color=\"batch\", title=\"UMAP after refined batch correction\")\n</pre> adata.obsm[\"scyan_corrected\"] = model.batch_effect_correction(batch_ref=\"B10\").numpy(force=True) scyan.tools.umap(adata, obsm=\"scyan_corrected\"); scyan.plot.umap(adata, color=\"batch\", title=\"UMAP after refined batch correction\") <pre>DataLoader:   0%|          | 0/511 [00:00&lt;?, ?it/s]</pre> <pre>DataLoader:   0%|          | 0/511 [00:00&lt;?, ?it/s]</pre> <pre>[INFO] (scyan.tools.representation) Fitting UMAP...\n</pre> In\u00a0[12]: Copied! <pre>print(adata.obsm[\"scyan_corrected\"])\nprint(f\"Marker names: {', '.join(model.var_names)}\")\n</pre> print(adata.obsm[\"scyan_corrected\"]) print(f\"Marker names: {', '.join(model.var_names)}\") <pre>[[-0.9539349  -0.93171173 -0.92845386 ... -0.9675956  -0.951983\n  -0.81466496]\n [ 3.1832964   1.8580393  -0.49163747 ... -0.48776484 -0.5667361\n   0.6440381 ]\n [ 3.1670866   1.3268812  -0.4432921  ... -0.5298085  -0.65013283\n   0.57265604]\n ...\n [ 3.005826    3.4372778   1.9195399  ...  1.6127968   5.003887\n   5.5465055 ]\n [ 1.5146042   2.7854807   0.25790054 ...  0.6421641   1.5063114\n   2.5468485 ]\n [ 1.5101717   2.7549195   0.4220283  ...  0.5319461   1.8654416\n   2.3529358 ]]\nMarker names: CD19, CD20, CD3, CD4, CD8, TCRgd, CD16, CD56, CD25, CD127, CD45RA, CCR7, HLA.DR, CD14, CD11c, CD123, CD27, CD69, CD40L\n</pre> <p>You can also update <code>adata.X</code> and work on the unscaled data. Note that it only updates the markers that are in the table, so consider using them all if needed (see above).</p> In\u00a0[\u00a0]: Copied! <pre>adata[:, model.var_names].X = adata.obsm[\"scyan_corrected\"]\nadata.X = scyan.preprocess.unscale(adata)\n</pre> adata[:, model.var_names].X = adata.obsm[\"scyan_corrected\"] adata.X = scyan.preprocess.unscale(adata)"},{"location":"tutorials/batch_effect_correction/#batch-effect-correction","title":"Batch effect correction\u00b6","text":"<p>Runtime: about 3 min after loading the dataset.</p> <p>The batch effect correction is illustrated on the POISED dataset, which contains 7 batches.</p> <p>Note</p> <p>         Scyan performs batch effect correction as the same time as training for cell-type annotations. Since these two tasks are linked, we advise the user to have already great annotations before to try batch effect correction. More precise annotations makes a better batch-effect correction.      </p>"},{"location":"tutorials/batch_effect_correction/#amplify-batch-effect-skip-this-step-on-your-data","title":"Amplify batch effect (skip this step on your data)\u00b6","text":"<p>To illustrate batch effect correction on a more complex dataset, we amplify the batch effect. Skip this step, since you don't want to add extra noise.</p>"},{"location":"tutorials/batch_effect_correction/#correct-batch-effect","title":"Correct batch effect\u00b6","text":"<p>To correct the batch effect, you must have a column in <code>adata.obs</code> dedicated to the batch. Then, provide a <code>batch_key</code> to <code>scyan.Scyan</code>, i.e. the name of the column dedicated to the batch. You can also choose a <code>batch_ref</code>, i.e. one of all the batches you want to use, or let Scyan choose it for you (by default, the batch with the highest number of cells).</p> <p>The batch knowledge is automatically added as one of the model <code>categorical_covariates</code>. You can add more categorical or continuous covariates if needed (see scyan.Scyan API).</p> <p>If you use your own dataset, use <code>Scyan</code> default parameters first.</p>"},{"location":"tutorials/batch_effect_correction/#refine-fit-handling-na-markers","title":"Refine fit / handling NA markers\u00b6","text":"<p>Sometimes, you will have a panel that includes some markers for which you have no prior knowledge at all. To handle batch effect correction for such markers, you can run <code>model.refine_fit()</code> (after having run <code>model.fit()</code>).</p> <p>Note</p> <p>         Such markers have to be added in the table (if not done yet). Just add a column of NAs for each of these markers.     </p>"},{"location":"tutorials/batch_effect_correction/#use-batch-corrected-expressions-for-further-analysis","title":"Use batch corrected expressions for further analysis\u00b6","text":"<p>The batch effect corrected expressions are stored in <code>adata.obsm[\"scyan_corrected\"]</code>, and the corresponding marker names are in <code>model.var_names</code>. You can use them for further analysis.</p>"},{"location":"tutorials/debarcoding/","title":"Debarcoding","text":"In\u00a0[1]: Copied! <pre>import scyan\n</pre> import scyan <pre>Global seed set to 0\n</pre> In\u00a0[2]: Copied! <pre>adata, table = scyan.data.load(\"debarcoding\")\n\nmodel = scyan.Scyan(adata, table)\n</pre> adata, table = scyan.data.load(\"debarcoding\")  model = scyan.Scyan(adata, table) <pre>[INFO] (scyan.model) Initialized Scyan model with N=100000 cells, P=22 populations and M=6 markers.\n   \u251c\u2500\u2500 No covariate provided\n   \u251c\u2500\u2500 No continuum-marker provided\n   \u2514\u2500\u2500 Batch correction mode: False\n</pre> <p>The <code>table</code> contains the barcodes of each patient (or sample):</p> In\u00a0[5]: Copied! <pre>table.head()\n</pre> table.head() Out[5]: BC1 BC2 BC3 BC4 BC5 BC6 Sample ID Pos 1 1 1 1 1 1 A1 1 1 1 -1 -1 -1 A2 1 1 -1 1 -1 -1 A3 1 1 -1 -1 1 -1 A4 1 1 -1 -1 -1 1 In\u00a0[3]: Copied! <pre>model.fit()\n</pre> model.fit() <pre>INFO:scyan.model:Training scyan with the following hyperparameters:\n\"batch_key\":       None\n\"batch_size\":      16384\n\"hidden_size\":     16\n\"lr\":              0.001\n\"max_samples\":     200000\n\"modulo_temp\":     2\n\"n_hidden_layers\": 7\n\"n_layers\":        7\n\"prior_std\":       0.25\n\"temperature\":     0.5\n\nGPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/Users/quentinblampey/Library/Caches/pypoetry/virtualenvs/scyan-5lsXrWE1-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:200: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n  rank_zero_warn(\n\n  | Name   | Type        | Params\n---------------------------------------\n0 | module | ScyanModule | 29.7 K\n---------------------------------------\n29.7 K    Trainable params\n0         Non-trainable params\n29.7 K    Total params\n0.119     Total estimated model params size (MB)\n/Users/quentinblampey/Library/Caches/pypoetry/virtualenvs/scyan-5lsXrWE1-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>INFO:scyan.model:Successfully ended traning.\n</pre> Out[3]: <pre>Scyan model with N=100000 cells, P=22 populations and M=6 markers.\n   \u251c\u2500\u2500 No covariate provided.\n   \u2514\u2500\u2500 Batch correction mode: False</pre> In\u00a0[4]: Copied! <pre>model.predict(log_prob_th=-10);  # Predictions are saved in adata.obs.scyan_pop by default\n</pre> model.predict(log_prob_th=-10);  # Predictions are saved in adata.obs.scyan_pop by default <pre>DataLoader:   0%|          | 0/7 [00:00&lt;?, ?it/s]</pre> In\u00a0[5]: Copied! <pre>scyan.plot.umap(adata, color=\"scyan_pop\")\n</pre> scyan.plot.umap(adata, color=\"scyan_pop\") In\u00a0[6]: Copied! <pre>scyan.plot.scatter(adata, [\"A1\", \"A2\", \"B2\", \"C4\", \"D5\"], markers=[\"BC1\", \"BC2\", \"BC3\", \"BC4\"])\n</pre> scyan.plot.scatter(adata, [\"A1\", \"A2\", \"B2\", \"C4\", \"D5\"], markers=[\"BC1\", \"BC2\", \"BC3\", \"BC4\"]) <p>The latent space corresponds to the barcodes!</p> In\u00a0[7]: Copied! <pre>scyan.plot.pops_expressions(model)\n</pre> scyan.plot.pops_expressions(model) <pre>DataLoader:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <p>One sample latent expression is its barcode.</p> In\u00a0[8]: Copied! <pre>scyan.plot.pop_expressions(model, \"D4\")\n</pre> scyan.plot.pop_expressions(model, \"D4\") <pre>DataLoader:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre>"},{"location":"tutorials/debarcoding/#debarcoding","title":"Debarcoding\u00b6","text":"<p>Runtime: about 20s.</p>"},{"location":"tutorials/debarcoding/#model-initialization-and-fitting","title":"Model initialization and fitting\u00b6","text":""},{"location":"tutorials/debarcoding/#results-visualization-and-interpretability","title":"Results visualization and interpretability\u00b6","text":""},{"location":"tutorials/population_discovery/","title":"Population discovery","text":"In\u00a0[1]: Copied! <pre>import scyan\n</pre> import scyan <pre>Global seed set to 0\n</pre> In\u00a0[2]: Copied! <pre>adata, table = scyan.data.load(\"poised\") # Load data (automatic)\n</pre> adata, table = scyan.data.load(\"poised\") # Load data (automatic) <p>For a real use case, we remove some knowledge from the table, and we show that Scyan helps improving the table.</p> <p>The modifications are the following:</p> <ul> <li>We only keep the <code>classical monocytes</code> among all the monocytes populations. We will show later on that we can retrieve the non-classical and intermediate monocytes.</li> <li>We only keep <code>TCD4 EM</code> cells instead of the two subpopulations <code>TCD4 EM CD27-</code> and <code>TCD4 EM CD27+</code>. We will show that we will also be able to go deeper inside the annotation of <code>TCD4 EM</code> cells.</li> </ul> In\u00a0[3]: Copied! <pre>import numpy as np\n\n# Keep only classical monocytes\ntable = table.drop(['ncMonocytes', 'iMonocytes', 'cMonocytes CD14mid'], axis=0, level=0)\n\n# Keep only T CD4 EM and remove information about CD27\ntable = table.rename(index={\"TCD4 EM CD27-\": \"TCD4 EM\"}, level=0)\ntable.loc[\"TCD4 EM\", \"CD27\"] = np.nan # No information on CD27 anymore\ntable = table.drop(\"TCD4 EM CD27+\", axis=0, level=0)\n</pre> import numpy as np  # Keep only classical monocytes table = table.drop(['ncMonocytes', 'iMonocytes', 'cMonocytes CD14mid'], axis=0, level=0)  # Keep only T CD4 EM and remove information about CD27 table = table.rename(index={\"TCD4 EM CD27-\": \"TCD4 EM\"}, level=0) table.loc[\"TCD4 EM\", \"CD27\"] = np.nan # No information on CD27 anymore table = table.drop(\"TCD4 EM CD27+\", axis=0, level=0) In\u00a0[4]: Copied! <pre># When training for the first time on a new dataset, we advise to start with the default arguments (i.e., remove 'prior_std' and 'lr')\nmodel = scyan.Scyan(adata, table, prior_std=0.25, lr=0.0001)\nmodel.fit()\n</pre> # When training for the first time on a new dataset, we advise to start with the default arguments (i.e., remove 'prior_std' and 'lr') model = scyan.Scyan(adata, table, prior_std=0.25, lr=0.0001) model.fit() <pre>GPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/Users/quentinblampey/Library/Caches/pypoetry/virtualenvs/scyan-5lsXrWE1-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:200: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n  rank_zero_warn(\n\n  | Name   | Type        | Params\n---------------------------------------\n0 | module | ScyanModule | 31.9 K\n---------------------------------------\n31.9 K    Trainable params\n0         Non-trainable params\n31.9 K    Total params\n0.127     Total estimated model params size (MB)\n</pre> <pre>Training: 0it [00:00, ?it/s]</pre> Out[4]: <pre>Scyan model with N=4178320 cells, P=22 populations and M=19 markers.\n   \u251c\u2500\u2500 No covariate provided.\n   \u2514\u2500\u2500 Batch correction mode: False</pre> In\u00a0[6]: Copied! <pre># A higher log_prob_th (i.e., closer to 0) will display more \"unknown\" cells.\n# It helps finding cells which are not really corresponding to known ones\nmodel.predict(log_prob_th=-20);\n</pre> # A higher log_prob_th (i.e., closer to 0) will display more \"unknown\" cells. # It helps finding cells which are not really corresponding to known ones model.predict(log_prob_th=-20); <pre>DataLoader:   0%|          | 0/511 [00:00&lt;?, ?it/s]</pre> <p>We see some unknown cells just above the Monocytes and in the left of the DCs. They are displayed in gray (<code>NA</code> in the legend).</p> In\u00a0[51]: Copied! <pre>scyan.plot.umap(adata, color=\"scyan_pop_level\")\n</pre> scyan.plot.umap(adata, color=\"scyan_pop_level\") In\u00a0[19]: Copied! <pre>%matplotlib tk\n</pre> %matplotlib tk In\u00a0[20]: Copied! <pre>selector = scyan.tools.PolygonGatingUMAP(adata)\nselector.select()\n</pre> selector = scyan.tools.PolygonGatingUMAP(adata) selector.select() <pre>Enclose cells within a polygon. Helper:\n    - Click on the plot to add a polygon vertex\n    - Press the 'esc' key to start a new polygon\n    - Try holding the 'ctrl' key to move a single vertex\n    - Once the polygon is finished and overlaid in red, you can close the window\n</pre> In\u00a0[21]: Copied! <pre>selector.save_selection()\n</pre> selector.save_selection() <pre>Selected 1264 cells and saved the selection in adata.obs['scyan_selected']\n</pre> <p>We go back to <code>matplotlib inline</code></p> In\u00a0[22]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline In\u00a0[25]: Copied! <pre># Show the latent expressions of the selected cells\nscyan.plot.pop_expressions(model, \"selected\", key=\"scyan_selected\")\n</pre> # Show the latent expressions of the selected cells scyan.plot.pop_expressions(model, \"selected\", key=\"scyan_selected\") <pre>DataLoader:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <p>But mainly, we can show why Scyan couldn't predict a population for these cells.</p> <p>The below figure shows the decomposition of Scyan confidence per population and per marker. For the cells of interest, we provide a confidence level for the prediction of each population (first column). Then, each population probability is decomposed into a sum of marker impact on the probability (one row). Dark colours indicate that the corresponding marker expression decreased the population probability of the corresponding row. For instance, the expression of CD14 (which is low for this population, according to above figure) decreased the confidence for predicting cMonocytes.</p> In\u00a0[26]: Copied! <pre>scyan.plot.probs_per_marker(model, \"selected\", key=\"scyan_selected\")\n</pre> scyan.plot.probs_per_marker(model, \"selected\", key=\"scyan_selected\") <pre>DataLoader:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <p>According to these graphs, we can conclude that the \"unknown\" cells were non-classical/intermediate monocytes.</p> <p>The next step would be to add them in the knowledge table and run Scyan again!</p> In\u00a0[7]: Copied! <pre>scyan.plot.umap(adata, color=\"scyan_pop\", palette=scyan.tools.palette_level(table))\n</pre> scyan.plot.umap(adata, color=\"scyan_pop\", palette=scyan.tools.palette_level(table)) <p>Now, we are going to split them into multiple subclustering using Leiden. Then, we will characterize the multiple clusters found (see below).</p> In\u00a0[52]: Copied! <pre># Increase the resolution to get more clusters\nscyan.tools.subcluster(adata, \"TCD4 EM\", resolution=0.1, markers=table.columns)\n</pre> # Increase the resolution to get more clusters scyan.tools.subcluster(adata, \"TCD4 EM\", resolution=0.1, markers=table.columns) <pre>OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n</pre> <p>It found 2 subclusters insides T CD4 EM cells.</p> In\u00a0[53]: Copied! <pre>scyan.plot.umap(adata, \"scyan_subcluster_TCD4 EM\")\n</pre> scyan.plot.umap(adata, \"scyan_subcluster_TCD4 EM\") <p>Below, we try to characterise the different subclusters (0 and 1) using the latent expressions.</p> <p>We remind that, a latent expression of -1 means negative, while 1 means positive.</p> <p>We can see that the subcluster 0 is CD27+, and the subcluster 1 is CD27-. The next step would be to replace <code>TCD4 EM</code> by <code>TCD4 EM CD27-</code> and <code>TCD4 EM CD27+</code> in the knowledge table and run Scyan again!</p> In\u00a0[58]: Copied! <pre>scyan.plot.pops_expressions(model, key=\"scyan_subcluster_TCD4 EM\", figsize=(8, 2))\n</pre> scyan.plot.pops_expressions(model, key=\"scyan_subcluster_TCD4 EM\", figsize=(8, 2)) <pre>DataLoader:   0%|          | 0/13 [00:00&lt;?, ?it/s]</pre>"},{"location":"tutorials/population_discovery/#population-discovery","title":"Population discovery\u00b6","text":""},{"location":"tutorials/population_discovery/#model-fitting-on-incomplete-knowledge","title":"Model fitting on incomplete knowledge\u00b6","text":""},{"location":"tutorials/population_discovery/#characterizing-unknown-populations","title":"Characterizing unknown populations\u00b6","text":"<p>Your knowledge table may be incomplete, and some populations may be predicted as \"Unknown\" by Scyan. We are going to select these unknown cells and characterise them.</p>"},{"location":"tutorials/population_discovery/#selecting-the-desired-unknown-cells","title":"Selecting the desired unknown cells\u00b6","text":"<p>This can simply be done by selecting the cells of interest on the UMAP. Here, We are selecting the gray cells (i.e., the unknown cells) that are just above the monocytes.</p> <p>To select cells interactively, we need to change the matplotlib background. For instance, we can use <code>matplotlib tk</code>:</p>"},{"location":"tutorials/population_discovery/#characterize-the-unknown-cells","title":"Characterize the \"unknown\" cells\u00b6","text":""},{"location":"tutorials/population_discovery/#define-more-precise-populations","title":"Define more precise populations\u00b6","text":"<p>You may want to go deeper in the annotations and annotate subpopulations among the one you defined. This is the purpose of this section.</p> <p>NB: you'll need an additional package: <code>leidenalg</code>. You can either install it directly (e.g., with <code>pip</code>), or install the \"discovery\" extra from scyan: <code>pip install \"scyan[discovery]\"</code>.</p> <p>On the UMAP below, we see that it seems possible to go deeper into the TCD4 EM cells, as it seems to be split in two.</p>"},{"location":"tutorials/preprocessing/","title":"Preprocessing","text":"In\u00a0[1]: Copied! <pre>import scyan\n</pre> import scyan <pre>Global seed set to 0\n</pre> In\u00a0[7]: Copied! <pre># If you have a FCS file\nadata = scyan.read_fcs(\"&lt;path-to-fcs&gt;.fcs\")\n\n# If you have a CSV file\nadata = scyan.read_csv(\"&lt;path-to-csv&gt;.csv\")\n\nprint(f\"Created anndata object with {adata.n_obs} cells and {adata.n_vars} markers.\\n\\n-&gt; The markers names are: {', '.join(adata.var_names)}\\n-&gt; The non-marker names are: {', '.join(adata.obs.columns)}\")\n</pre> # If you have a FCS file adata = scyan.read_fcs(\".fcs\")  # If you have a CSV file adata = scyan.read_csv(\".csv\")  print(f\"Created anndata object with {adata.n_obs} cells and {adata.n_vars} markers.\\n\\n-&gt; The markers names are: {', '.join(adata.var_names)}\\n-&gt; The non-marker names are: {', '.join(adata.obs.columns)}\") <pre>Created anndata object with 216331 cells and 42 markers.\n\n-&gt; The markers names are: epcam, CD4, CD38, CD1a, CD24, CD123, CD47, CD39, CD31, CD169, CCR7, CD44, CD141, CD1c, CD9, HLADQ, CD11b, CD103, CD3/16/9/20, CD366, PD1, CD21, CD127, GP38, CD14, CD45, CD206, CTLA4, CD207, CD223, PDL1, CD69, CD25, Siglec10, HLADR, FOLR2, CADM1, CD45RA, CD5, Via dye, CD88, CD8\n-&gt; The non-marker names are: Time, SSC-H, SSC-A, FSC-H, FSC-A, SSC-B-H, SSC-B-A, AF-A\n</pre> In\u00a0[4]: Copied! <pre>is_cytof = True\n\nif is_cytof: # we recommend asinh for CyTOF data\n    scyan.preprocess.asinh_transform(adata)\nelse: # we recommend auto_logicle for flow or spectral flow\n    scyan.preprocess.auto_logicle_transform(adata)\n\nscyan.preprocess.scale(adata)\n</pre> is_cytof = True  if is_cytof: # we recommend asinh for CyTOF data     scyan.preprocess.asinh_transform(adata) else: # we recommend auto_logicle for flow or spectral flow     scyan.preprocess.auto_logicle_transform(adata)  scyan.preprocess.scale(adata) In\u00a0[5]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[6]: Copied! <pre>table = pd.read_csv(\"&lt;path-to-csv&gt;.csv\", index_col=0)\n</pre> table = pd.read_csv(\".csv\", index_col=0) In\u00a0[7]: Copied! <pre>table.head() # Display the first 5 rows of the table\n</pre> table.head() # Display the first 5 rows of the table Out[7]: CD19 CD4 CD8 CD34 CD20 CD45 CD123 CD11c CD7 CD16 CD38 CD3 HLA-DR CD64 Populations Basophils -1 NaN -1.0 -1 -1.0 NaN 1 -1 -1.0 -1.0 NaN -1 -1.0 -1.0 CD4 T cells -1 1.0 -1.0 -1 -1.0 NaN -1 -1 NaN -1.0 NaN 1 -1.0 -1.0 CD8 T cells -1 -1.0 1.0 -1 -1.0 NaN -1 -1 1.0 -1.0 NaN 1 -1.0 -1.0 CD16- NK cells -1 NaN NaN -1 -1.0 NaN -1 -1 1.0 -1.0 NaN -1 -1.0 -1.0 CD16+ NK cells -1 NaN NaN -1 NaN NaN -1 -1 1.0 1.0 NaN -1 -1.0 -1.0 <p>You can see our advices when creating this table.</p> In\u00a0[\u00a0]: Copied! <pre># Option 1: Use all markers to compute the UMAP\nscyan.tools.umap(adata)\n\n# Option 2: Use only the cell-type markers (recommended), or your choose your own list of markers\nscyan.tools.umap(adata, markers=table.columns)\n</pre> # Option 1: Use all markers to compute the UMAP scyan.tools.umap(adata)  # Option 2: Use only the cell-type markers (recommended), or your choose your own list of markers scyan.tools.umap(adata, markers=table.columns) In\u00a0[9]: Copied! <pre>scyan.data.add(\"your-project-name\", adata, table)\n</pre> scyan.data.add(\"your-project-name\", adata, table) <pre>INFO:scyan.data.datasets:Creating new dataset folder at /.../your_project_name\nINFO:scyan.data.datasets:Created file /.../your_project_name/default.h5ad\nINFO:scyan.data.datasets:Created file /.../your_project_name/default.csv\n</pre> <p>From now on, you can now simply load your processed data with scyan.data.load:</p> In\u00a0[10]: Copied! <pre>adata, table = scyan.data.load(\"your-project-name\")\n</pre> adata, table = scyan.data.load(\"your-project-name\")"},{"location":"tutorials/preprocessing/#preprocessing","title":"Preprocessing\u00b6","text":"<p>This tutorial helps you preprocessing your raw data so that you can run <code>Scyan</code> afterwards.</p> <p>You'll learn how to:</p> <ul> <li>Create an <code>adata</code> object based on a FCS (or CSV) file and preprocess it.</li> <li>Create the knowledge table required for the annotation.</li> <li>(Optional) Compute a UMAP and save your dataset for later use.</li> </ul> <p>Before continuing, make sure you have already installed scyan.</p>"},{"location":"tutorials/preprocessing/#1-creation-of-an-anndata-object-for-your-cytometry-data","title":"1. Creation of an <code>AnnData</code> object for your cytometry data\u00b6","text":"<p>Consider reading the anndata documentation if you have never heard about <code>anndata</code> before (it's a nice library for handling single-cell data).</p> <p>Note</p> <p>         Make sure you only take the population of interest. E.g., if you are interested into immune cells, consider providing only the live cells that are CD45+. If not possible, continue the tutorial, but consider running Scyan for filtering these cells before annotating the populations.     </p>"},{"location":"tutorials/preprocessing/#a-loading-a-fcs-or-csv-file","title":"a) Loading a <code>FCS</code> or <code>CSV</code> file\u00b6","text":"<p>You probably have <code>.fcs</code> or <code>.csv</code> files that you want to load. For this, you can use <code>scyan.read_fcs</code> or <code>scyan.read_csv</code>.</p>"},{"location":"tutorials/preprocessing/#b-sanity-check","title":"b) Sanity check\u00b6","text":"<p>Make sure that the listed markers (i.e., <code>adata.var_names</code>) contains only protein markers, and that every other variable is inside <code>adata.obs</code>. If this is not the case, consider reading <code>scyan.read_fcs</code> or <code>scyan.read_csv</code> for more advanced usage (e.g., you can update <code>marker_regex=\"^cd|^hla|epcam|^ccr\"</code> to target all your markers).</p>"},{"location":"tutorials/preprocessing/#c-concatenate-your-data-optional","title":"c) Concatenate your data (optional)\u00b6","text":"<p>If you have multiple <code>FCS</code>, consider concatenating your data. We advise to add a observation column such as \"batch\" or \"patient_id\" to keep the information about the batch / patient ID.</p> Click to show an example <p>This short script will concatenate all the FCS inside a specific folder, and save each file name into <code>adata.obs[\"file\"]</code> so that we don't loose information. You can add additional information, e.g. in <code>adata.obs[\"batch\"]</code> if you have different batches.</p> <pre><code>import anndata\nfrom pathlib import Path\n\nfolder_path = Path(\".\") # Replace \".\" by the path to your folder containing FCS files\nfcs_paths = [path for path in folder_path.iterdir() if path.suffix == \".fcs\"]\n\ndef read_one(path):\n    adata = scyan.read_fcs(path)\n    adata.obs[\"file\"] = path.stem\n    adata.obs[\"batch\"] = \"NA\" # If you have batches, add here the batch of the corresponding path\n    return adata\n\nadata = anndata.concat([read_one(p) for p in fcs_paths], index_unique=\"-\")\n</code></pre>"},{"location":"tutorials/preprocessing/#d-preprocessing","title":"d) Preprocessing\u00b6","text":"<p>Choose either the <code>asinh</code> or <code>logicle</code> transformation below, and scale your data.</p>"},{"location":"tutorials/preprocessing/#2-creation-of-the-knowledge-table","title":"2. Creation of the knowledge table\u00b6","text":"<p>Note</p> <p>         Some existing tables can be found here. It could help you making your table.     </p> <p>The knowledge table contains well-known marker expressions per population. For instance, if you want <code>Scyan</code> to annotate CD4 T cells, you have to tell which markers CD4 T cells are supposed to express or not. Depending on your panel, it may be CD4+, CD8-, CD45+, CD3+, etc. Values inside the table can be:</p> <ul> <li><code>-1</code> for negative expressions.</li> <li><code>1</code> for positive expressions.</li> <li><code>NA</code> when you don't know or if it is not applicable (if you use a CSV, you can also let the field empty, it will be read as <code>NaN</code> by <code>pandas</code>).</li> <li>Some float values such as <code>0</code> or <code>-0.5</code> for mid and low expressions respectively (use it only when necessary).</li> </ul> <p>Each row corresponds to one population, and each column corresponds to one marker (i.e., one of <code>adata.var_names</code>).</p> <p>You can either directly create a <code>csv</code>, or use Excel and export the table as <code>csv</code>. Then, you can then import the <code>csv</code> to make a pandas <code>DataFrame</code>.</p>"},{"location":"tutorials/preprocessing/#example","title":"Example\u00b6","text":""},{"location":"tutorials/preprocessing/#sanity-check","title":"Sanity check\u00b6","text":"<p>Make sure <code>table.index</code> contains population names, and that <code>table.columns</code> contains existing marker names (i.e., included in <code>adata.var_names</code>).</p> <p>NB: the table index can be a <code>MultiIndex</code> to list hierarchical populations, and the first level should correspond to the most precise populations (see how to work with hierarchical populations).</p>"},{"location":"tutorials/preprocessing/#3-optional-compute-a-umap","title":"3. (Optional) Compute a UMAP\u00b6","text":"<p>You can compute the UMAP coordinates using <code>scyan.tools.umap</code>. The API will guide you for the usage of this tool: especially, you can choose to compute the UMAP on a specific set of markers, or choose a subset of cells on which computing the UMAP (for acceleration).</p> <p>Note that it only computes the coordinates, then you'll have to use <code>scyan.plot.umap</code> to display it.</p>"},{"location":"tutorials/preprocessing/#4-optional-save-your-data-for-later-use","title":"4. (Optional) Save your data for later use\u00b6","text":"<p>You can use scyan.data.add to save your data.</p>"},{"location":"tutorials/preprocessing/#next-steps","title":"Next steps\u00b6","text":"<p>Congratulations! You can now follow our tutorial on model training and visualization.</p>"},{"location":"tutorials/usage/","title":"Scyan usage and visualization","text":"In\u00a0[26]: Copied! <pre>import scyan\n</pre> import scyan In\u00a0[27]: Copied! <pre>adata, table = scyan.data.load(\"poised\") # Automatic data loading\n</pre> adata, table = scyan.data.load(\"poised\") # Automatic data loading <p>Note</p> <p>         Scyan is highly customizable. Before creating a new model as below, consider reading <code>scyan.Scyan.__init__</code> to understand the optional initialization arguments. For instance, you can add covariates (<code>continuous_covariates</code> and <code>categorical_covariates</code>) or continuum markers (i.e., markers whose expression is a continuum / has no clear seperation).     </p> In\u00a0[\u00a0]: Copied! <pre># When training for the first time on a new dataset, we advise to start with the default arguments (i.e., remove 'prior_std' and 'lr')\nmodel = scyan.Scyan(adata, table, prior_std=0.25, lr=0.0001)\nmodel.fit()\n</pre> # When training for the first time on a new dataset, we advise to start with the default arguments (i.e., remove 'prior_std' and 'lr') model = scyan.Scyan(adata, table, prior_std=0.25, lr=0.0001) model.fit() In\u00a0[\u00a0]: Copied! <pre>model.predict()\n\n# Here, for simplicity, we don't consider 'DC others', and we merge 'cMonocytes CD14mid' into 'cMonocytes'\nadata.obs[\"scyan_pop\"] = adata.obs.scyan_pop.replace(\"DC others\", \"Unknown\").replace(\"cMonocytes CD14mid\", \"cMonocytes\")\n</pre> model.predict()  # Here, for simplicity, we don't consider 'DC others', and we merge 'cMonocytes CD14mid' into 'cMonocytes' adata.obs[\"scyan_pop\"] = adata.obs.scyan_pop.replace(\"DC others\", \"Unknown\").replace(\"cMonocytes CD14mid\", \"cMonocytes\") In\u00a0[30]: Copied! <pre>scyan.plot.umap(adata, color=\"scyan_pop\", title=\"Scyan predictions\", palette=adata.uns[\"palette\"])\n</pre> scyan.plot.umap(adata, color=\"scyan_pop\", title=\"Scyan predictions\", palette=adata.uns[\"palette\"]) <p>If desired, the following plot can help better choose <code>log_prob_th</code> (the argument provided to <code>model.predict()</code> to decide below which threshold a cell should be left unclassified). By default, this parameter is <code>-50</code>, but the best value can depend on the dataset. Choose a value before that the ratio of predicted cells decreases dramatically:</p> In\u00a0[31]: Copied! <pre>scyan.plot.log_prob_threshold(adata)\n</pre> scyan.plot.log_prob_threshold(adata) In\u00a0[8]: Copied! <pre>population = \"ncMonocytes\"\n</pre> population = \"ncMonocytes\" In\u00a0[9]: Copied! <pre>scyan.plot.pop_expressions(model, population)\n</pre> scyan.plot.pop_expressions(model, population) <pre>DataLoader:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <p>Interpret why Scyan predicted ncMonocyte and not another population. Dark values correspond to a marker that reduces the confidence in predicting a population.</p> <p>E.g., the dark value on (mDC1, CD16) means that the expressions of CD16 of the targeted cells (that is positive, according to the above figure) reduced the model confidence to predict mDC1, which is indeed expected.</p> In\u00a0[10]: Copied! <pre>scyan.plot.probs_per_marker(model, population)\n</pre> scyan.plot.probs_per_marker(model, population) <pre>DataLoader:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <p>Show population percentages for all patients. It <code>groupby</code> is not provided, then it just shows the global percentages. Note that <code>groupby</code> can also be a list: it will make multi-level groups, e.g. you can set <code>groupby=['batch', 'patient']</code>.</p> In\u00a0[47]: Copied! <pre>scyan.plot.pop_percentage(adata, groupby=\"patient\")\n</pre> scyan.plot.pop_percentage(adata, groupby=\"patient\") <p>You can also show dynamics if you have multiple time points (or conditions, like below).</p> In\u00a0[44]: Copied! <pre># The column 'condition' contains two possible values: PeaStim or Unstim\nadata.obs[\"condition\"] = adata.obs.file.map(lambda x: x.split(\"_\")[1])\n\nscyan.plot.pop_dynamics(adata, time_key=\"condition\", groupby=\"patient\", key=\"scyan_pop_level\")\n</pre> # The column 'condition' contains two possible values: PeaStim or Unstim adata.obs[\"condition\"] = adata.obs.file.map(lambda x: x.split(\"_\")[1])  scyan.plot.pop_dynamics(adata, time_key=\"condition\", groupby=\"patient\", key=\"scyan_pop_level\") <pre>[INFO] (scyan.plot.ratios) Converting adata.obs['condition'] to categorical\n</pre> In\u00a0[48]: Copied! <pre>scyan.plot.pops_expressions(model)\n</pre> scyan.plot.pops_expressions(model) <pre>DataLoader:   0%|          | 0/25 [00:00&lt;?, ?it/s]</pre> In\u00a0[7]: Copied! <pre>populations = ['NKT', 'NK', 'TCD8 Naive']\nscyan.plot.scatter(adata, populations, n_markers=3)\n</pre> populations = ['NKT', 'NK', 'TCD8 Naive'] scyan.plot.scatter(adata, populations, n_markers=3) In\u00a0[40]: Copied! <pre>table\n</pre> table Out[40]: CD19 CD20 ... CD69 CD40L Population level TCD4 PeaReactive T CD4 cells -1 -1.0 ... 1.0 1.0 TCD4 EM CD27- T CD4 cells -1 -1.0 ... -1.0 -1.0 ... ... ... ... ... ... ... mDC2 DCs -1 NaN ... NaN NaN DC others DCs -1 NaN ... NaN NaN <p>26 rows \u00d7 19 columns</p> <p>Hierarchical population can be described by a tree (if you have not installed pygraphviz yet, run <code>pip install pygraphviz</code>). This is an example with one intermediate level, but you can add more.</p> In\u00a0[5]: Copied! <pre>scyan.plot.pops_hierarchy(model, figsize=(15, 6))\n</pre> scyan.plot.pops_hierarchy(model, figsize=(15, 6)) In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[\u00a0]: Copied! <pre>pd.read_csv(&lt;table-path.csv&gt;, index_col=[0, 1]) # if you have one population and one level column\npd.read_csv(&lt;table-path.csv&gt;, index_col=[0, 1, 2]) # if you have two levels\n</pre> pd.read_csv(, index_col=[0, 1]) # if you have one population and one level column pd.read_csv(, index_col=[0, 1, 2]) # if you have two levels In\u00a0[6]: Copied! <pre>scyan.plot.umap(adata, color=\"scyan_pop_level\")\n</pre> scyan.plot.umap(adata, color=\"scyan_pop_level\") <p>You can also use <code>scyan.plot.pop_level</code> to display subpopulations from one group at this level. By default, it uses <code>level='level'</code>, but you can change it.</p> In\u00a0[7]: Copied! <pre>scyan.plot.pop_level(model, \"T CD4 cells\") # Plot all populations that belongs to the group \"T CD4 cells\"\n</pre> scyan.plot.pop_level(model, \"T CD4 cells\") # Plot all populations that belongs to the group \"T CD4 cells\" In\u00a0[9]: Copied! <pre>palette = scyan.tools.palette_level(table)\npalette[\"Unknown\"] = \"#bbb\" # Only if you renamed the np.nan cells as \"Unknown\"\n\nscyan.plot.umap(adata, color=\"scyan_pop\", title=\"Scyan predictions\", palette=palette)\n</pre> palette = scyan.tools.palette_level(table) palette[\"Unknown\"] = \"#bbb\" # Only if you renamed the np.nan cells as \"Unknown\"  scyan.plot.umap(adata, color=\"scyan_pop\", title=\"Scyan predictions\", palette=palette) In\u00a0[10]: Copied! <pre>print(\"Available level names:\", model.level_names)\nprint(\"Populations at level 'level':\", model.pops('level'))\nprint(\"Children populations of T CD4 cells:\", model.pops(children_of='T CD4 cells'))\nprint(\"Parent population of B naive cells:\", model.pops(parent_of=\"B Naive\"))\n</pre> print(\"Available level names:\", model.level_names) print(\"Populations at level 'level':\", model.pops('level')) print(\"Children populations of T CD4 cells:\", model.pops(children_of='T CD4 cells')) print(\"Parent population of B naive cells:\", model.pops(parent_of=\"B Naive\")) <pre>Available level names: ['level']\nPopulations at level 'level': ['T CD8 cells', 'T CD4 cells', 'Monocytes', 'NK', 'DCs', 'Tregs', 'gdTCR', 'B cells', 'NKT']\nChildren populations of T CD4 cells: ['TCD4 CM', 'TCD4 Naive', 'TCD4 PeaReactive', 'TCD4 EM CD27+', 'TCD4 EM CD27-']\nParent population of B naive cells: B cells\n</pre> In\u00a0[12]: Copied! <pre># Display at most 5 columns and 5 rows when showing a DataFrame (you can remove it)\nimport pandas as pd\npd.set_option('display.max_rows', 5)\npd.set_option('display.max_columns', 5)\n</pre> # Display at most 5 columns and 5 rows when showing a DataFrame (you can remove it) import pandas as pd pd.set_option('display.max_rows', 5) pd.set_option('display.max_columns', 5) In\u00a0[13]: Copied! <pre>scyan.tools.cell_type_ratios(adata, \"file\")\n</pre> scyan.tools.cell_type_ratios(adata, \"file\") Out[13]: TCD4 PeaReactive percentage TCD4 EM CD27- percentage ... mDC2 percentage Unknown percentage file P015_PeaStim_label_expression 0.000774 0.018336 ... 0.001133 0.002347 P015_UnStim_label_expression 0.000106 0.023992 ... 0.006534 0.002052 ... ... ... ... ... ... P120_PeaStim_label_expression 0.003211 0.034703 ... 0.000584 0.003547 P120_UnStim_label_expression 0.001780 0.023246 ... 0.001780 0.002911 <p>30 rows \u00d7 25 columns</p> <p>We can also group the files by another column, here <code>\"category\"</code> (it contains either \"Peanut stimulated\" or \"Unstimulated\").</p> In\u00a0[14]: Copied! <pre>scyan.tools.cell_type_ratios(adata, [\"category\", \"file\"])\n</pre> scyan.tools.cell_type_ratios(adata, [\"category\", \"file\"]) Out[14]: TCD4 PeaReactive percentage TCD4 EM CD27- percentage ... mDC2 percentage Unknown percentage category file Peanut stimulated P015_PeaStim_label_expression 0.000774 0.018336 ... 0.001133 0.002347 P042_PeaStim_label_expression 0.002132 0.036484 ... 0.000984 0.002317 ... ... ... ... ... ... ... Unstimulated P117_UnStim_label_expression 0.001814 0.097315 ... 0.001313 0.001455 P120_UnStim_label_expression 0.001780 0.023246 ... 0.001780 0.002911 <p>30 rows \u00d7 25 columns</p> <p>Other options:</p> <ul> <li>Set <code>normalize=False</code> if you want population counts instead of ratios.</li> <li>Set <code>among=\"scyan_pop_level\"</code> to get percentages compared to the parent population.</li> </ul> <p>Note that <code>adata.X</code> contains scaled values, so we first unscale it.</p> In\u00a0[15]: Copied! <pre>adata.layers[\"unscaled\"] = scyan.preprocess.unscale(adata)\n</pre> adata.layers[\"unscaled\"] = scyan.preprocess.unscale(adata) <p>Then we get our MMI/MFI (i.e., mean intensities):</p> In\u00a0[16]: Copied! <pre>df_mmi = scyan.tools.mean_intensities(adata, \"file\", layer=\"unscaled\") # Again, it can be grouped by \"category\" if needed\n\nprint(f\"Found {len(df_mmi.columns)} MMI per patient\")\ndf_mmi\n</pre> df_mmi = scyan.tools.mean_intensities(adata, \"file\", layer=\"unscaled\") # Again, it can be grouped by \"category\" if needed  print(f\"Found {len(df_mmi.columns)} MMI per patient\") df_mmi <pre>Found 975 MMI per patient\n</pre> Out[16]: CD16 mean intensity on TCD4 PeaReactive CD16 mean intensity on TCD4 EM CD27- ... CLA mean intensity on mDC2 CLA mean intensity on Unknown file P015_PeaStim_label_expression 0.025255 0.026074 ... 0.620651 0.821010 P015_UnStim_label_expression 0.034365 0.019730 ... 0.455839 1.238357 ... ... ... ... ... ... P120_PeaStim_label_expression 0.056948 0.045655 ... 0.762157 0.577603 P120_UnStim_label_expression 0.040490 0.026502 ... 0.469605 0.861299 <p>30 rows \u00d7 975 columns</p> <p>Or, more simply, it's possible not to group the values per patient. In this case, we get a series instead of a DataFrame.</p> In\u00a0[17]: Copied! <pre>scyan.tools.mean_intensities(adata, None, layer=\"unscaled\")\n</pre> scyan.tools.mean_intensities(adata, None, layer=\"unscaled\") Out[17]: <pre>CD16 mean intensity on TCD4 PeaReactive    0.053543\nCD16 mean intensity on TCD4 EM CD27-       0.062606\n                                             ...   \nCLA mean intensity on mDC2                 0.549531\nCLA mean intensity on Unknown              0.940028\nLength: 975, dtype: float32</pre>"},{"location":"tutorials/usage/#scyan-usage-and-visualization","title":"Scyan usage and visualization\u00b6","text":"<p>Runtime: about 2min after loading the dataset.</p> <p>This notebook is run on the POISED dataset.</p>"},{"location":"tutorials/usage/#model-initialization-and-fitting","title":"Model initialization and fitting\u00b6","text":"<p>Scyan runs on preprocessed data (<code>asinh</code> or <code>auto_logicle</code>) and scaled. If not done yet, consider reading our preprocessing tutorial. The data below is ready for use.</p>"},{"location":"tutorials/usage/#run-predictions","title":"Run predictions\u00b6","text":"<p>Predictions are saved in <code>adata.obs.scyan_pop</code> by default.</p> <p>Note</p> <p>         The cells that are unknown are annotated as <code>np.nan</code> (i.e. cells chose maximum probability is lower than a given threshold, see <code>scyan.Scyan.predict</code>).     </p>"},{"location":"tutorials/usage/#visualization-and-interpretability","title":"Visualization and interpretability\u00b6","text":""},{"location":"tutorials/usage/#model-interpretability-for-one-population","title":"Model interpretability for one population\u00b6","text":"<p>Let's now consider one specific population: non-classical monocytes.</p>"},{"location":"tutorials/usage/#population-percentage-and-dynamics","title":"Population percentage and dynamics\u00b6","text":""},{"location":"tutorials/usage/#latent-expressions-for-all-populations","title":"Latent expressions for all populations\u00b6","text":"<p>Expressions close to $-1$ represent negative expressions, while expressions close to $1$ represent positive expressions. Thus, $0$ can be considered as a mid expression.</p>"},{"location":"tutorials/usage/#pairwise-scatter-plots","title":"Pairwise scatter plots\u00b6","text":"<p>The populations below can be shown on 3 scatter plots (the markers are automatically chosen to make the best population separation).</p>"},{"location":"tutorials/usage/#working-with-hierarchical-populations","title":"Working with hierarchical populations\u00b6","text":"<p>For some projects, you may try to annotate many dozens of different populations. It becomes difficult for the human eye to differentiate so many colors on a UMAP. For this reason, you can create multi-level population names (each level going further into the details of the populations), and display only populations at a certain level or go through subpopulations of one specific group of cells.</p> <p>See below an example of hierarchical populations (e.g., both TCD4 PeaReactive and TCD4 EM CD27- are grouped inside one main population called T CD4 cells):</p>"},{"location":"tutorials/usage/#making-a-hierarchical-table","title":"Making a hierarchical table\u00b6","text":"<p>To create a hierarchical classification of your populations, add (to your knowledge table) one or multiple columns whose names start with <code>level</code> (e.g., <code>level1</code>, <code>level_main_population</code>, or just <code>level</code>). We recommend writing these columns after the first column. The easiest way to create these columns is to manually add them to your knowledge table's <code>csv</code> file; see an example here.</p> <p>NB: the first column corresponds to the most detailed population names (i.e. leaves on the tree above), and it contains the population names that Scyan will use for its annotation. The name of this first column should not start with <code>level</code>, contrary to the columns used to make the population hierarchy.</p> <p>If you save your data with <code>scyan.data.add</code>, you won't need any other formatting because <code>scyan</code> will detect the levels using their names and return a multi-index dataframe as displayed above. But if you want to read the table directly, you can read it as a multi-index DataFrame with the following command line:</p>"},{"location":"tutorials/usage/#plot-the-different-population-levels-on-a-umap","title":"Plot the different population levels on a UMAP\u00b6","text":"<p>You can use the key associated with a specific level to display all populations at this level.</p> <p>E.g., if you want to display the level called <code>level2</code>, use <code>color=\"scyan_pop_level2\"</code>.</p>"},{"location":"tutorials/usage/#making-a-block-color-palette","title":"Making a block color palette\u00b6","text":"<p>The population tree can be used to defined colors by block, i.e. similar colors for populations of the same groups. For instance, here, all five subpopulations of T CD4 cells are in shades of green.</p>"},{"location":"tutorials/usage/#compatibility-with-the-other-toolsplots","title":"Compatibility with the other tools/plots\u00b6","text":"<p>Most plots and tools can also be used on a specific level. Actually, when making prediction, scyan also added keys in your <code>adata.obs</code> corresponding to each level. Thus, when available, provide <code>key=\"scyan_pop_&lt;level&gt;\"</code> to any plot or tool to work at this specific level (where <code>&lt;level&gt;</code> is one of <code>model.level_names</code>).</p> <p>Note also that <code>model.pops(...)</code> can help you navigating through the different populations at different levels of the tree.</p>"},{"location":"tutorials/usage/#biomarkers-extraction","title":"Biomarkers extraction\u00b6","text":"<p>After training, you probably want to extract biomarkers per patient. For this, we offer two types of biomarkers: (i) population counts/ratios, and (ii) mean intensity per population for all markers.</p> <p>You probably have an observation that corresponds to your sample/patient ID. For this public dataset, this observation is <code>adata.obs[\"file\"]</code>, and there is one clinical condition which is stored in <code>adata.obs[\"category\"]</code>.</p>"},{"location":"tutorials/usage/#ratios-of-populations","title":"Ratios of populations\u00b6","text":""},{"location":"tutorials/usage/#mmi-per-population-and-per-marker","title":"MMI per population and per marker\u00b6","text":""}]}